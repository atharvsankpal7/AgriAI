{"version":3,"file":"static/js/982.19361e83.chunk.js","mappings":"+bAAO,MAAMA,EAAU,SC0BhB,IACIC,EACAC,EAIAC,EAEAC,EACAC,EACAC,EACAC,EACAC,EACAC,EAbAC,GAAO,EAGPC,EAAwCC,KACxCC,EAA0CD,KAC1CE,EAAwCF,KAExCG,EAAkCH,KC9BvC,MAAOI,EACXC,WAAAA,CAAmBC,GAAA,KAAAA,KAAAA,CAAY,CAC/B,IAAKC,OAAOC,eACV,MAAO,eACT,ECFGC,GFoCC,SAAmBA,GAA0D,IAA5CC,EAAAC,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA6B,CAAEb,MAAM,GAC1E,GAAIA,EACF,MAAM,IAAIe,MAAM,kCAADC,OACsBL,EAAMpB,KAAI,kDAGjD,GAAIA,EACF,MAAM,IAAIwB,MAAM,+BAADC,OAAiCL,EAAMpB,KAAI,mCAAAyB,OAAoCzB,EAAI,OAEpGS,EAAOY,EAAQZ,KACfT,EAAOoB,EAAMpB,KACbC,EAAQmB,EAAMnB,MACdS,EAAUU,EAAMV,QAChBE,EAAWQ,EAAMR,SACjBC,EAAUO,EAAMP,QAChBX,EAAWkB,EAAMlB,SACjBY,EAAOM,EAAMN,KACbX,EAAOiB,EAAMjB,KACbC,EAAiBgB,EAAMhB,eACvBC,EAA6Be,EAAMf,2BACnCC,EAAkBc,EAAMd,gBACxBC,EAAea,EAAMb,aACrBC,EAAiBY,EAAMZ,cACzB,CE3DiBY,CCEX,WAA8E,IAAzD,iBAAEM,GAAgBJ,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAqC,CAAC,EACjF,MAAMK,EACJD,EAAgB,qNAOlB,IAAIE,EAAQC,EAAUC,EAAWC,EACjC,IAEEH,EAAS3B,MAET4B,EAAWnB,QAEXoB,EAAYlB,SAEZmB,EAAWlB,O,CACX,MAAOmB,GACP,MAAM,IAAIR,MAAM,iEAADC,OAEVO,EAAcC,QACjB,MAAAR,OAAKE,G,CAIT,MAAO,CACL3B,KAAM,MACNC,MAAO2B,EACPlB,QAASmB,EACTjB,SAAUkB,EACVjB,QAASkB,EACT7B,SAEsB,qBAAbA,SAA2BA,SAChC,MAEEc,WAAAA,GACE,MAAM,IAAIQ,MAAM,qFAADC,OACwEE,GAEzF,GAGNb,KACkB,qBAATA,KAAuBA,KAC5B,MACEE,WAAAA,GACE,MAAM,IAAIQ,MAAM,iFAADC,OACoEE,GAErF,GAGNxB,KAEkB,qBAATA,KAAuBA,KAC5B,MAEEa,WAAAA,GACE,MAAM,IAAIQ,MAAM,iFAADC,OACoEE,GAErF,GAGNvB,eAE4B,qBAAnBA,eAAiCA,eACtC,MAEEY,WAAAA,GACE,MAAM,IAAIQ,MAAM,uFAADC,OAC0EE,GAE3F,GAGNtB,2BAA4B6B,MAE1BC,EACAC,KAAuB,IAEpBA,EACHnB,KAAM,IAAIF,EAAcoB,KAE1B7B,gBAAkB+B,IAAyB,EAC3C9B,aAAcA,KACZ,MAAM,IAAIiB,MACR,iJACD,EAEHhB,eAAiB8B,IAAe,EAEpC,CDjGgC7B,GAAmB,CAAEA,MAAM,IEDrD,MAAO8B,UAAoBf,OAE3B,MAAOgB,UAAiBD,EAS5BvB,WAAAA,CACEyB,EACAT,EACAC,EACAS,GAEAC,MAAM,GAADlB,OAAIe,EAASI,YAAYH,EAAQT,EAAOC,KAC7CY,KAAKJ,OAASA,EACdI,KAAKH,QAAUA,EAEf,MAAMI,EAAOd,EACba,KAAKb,MAAQc,EACbD,KAAKE,KAAW,OAAJD,QAAI,IAAJA,OAAI,EAAJA,EAAa,KACzBD,KAAKG,MAAY,OAAJF,QAAI,IAAJA,OAAI,EAAJA,EAAc,MAC3BD,KAAKI,KAAW,OAAJH,QAAI,IAAJA,OAAI,EAAJA,EAAa,IAC3B,CAEQ,kBAAOF,CAAYH,EAA4BT,EAAYC,GACjE,MAAMiB,EACC,OAALlB,QAAK,IAALA,GAAAA,EAAOC,QACoB,kBAAlBD,EAAMC,QACXD,EAAMC,QACNkB,KAAKC,UAAUpB,EAAMC,SACvBD,EAAQmB,KAAKC,UAAUpB,GACvBC,EAEJ,OAAIQ,GAAUS,EACL,GAAPzB,OAAUgB,EAAM,KAAAhB,OAAIyB,GAElBT,EACK,GAAPhB,OAAUgB,EAAM,0BAEdS,GAGG,0BACT,CAEA,eAAOG,CACLZ,EACAa,EACArB,EACAS,GAEA,IAAKD,EACH,OAAO,IAAIc,EAAmB,CAAEC,MAAOC,GAAYH,KAGrD,MAAMtB,EAA8C,OAArCsB,QAAqC,IAArCA,OAAqC,EAArCA,EAA+C,MAE9D,OAAe,MAAXb,EACK,IAAIiB,EAAgBjB,EAAQT,EAAOC,EAASS,GAGtC,MAAXD,EACK,IAAIkB,EAAoBlB,EAAQT,EAAOC,EAASS,GAG1C,MAAXD,EACK,IAAImB,EAAsBnB,EAAQT,EAAOC,EAASS,GAG5C,MAAXD,EACK,IAAIoB,EAAcpB,EAAQT,EAAOC,EAASS,GAGpC,MAAXD,EACK,IAAIqB,EAAcrB,EAAQT,EAAOC,EAASS,GAGpC,MAAXD,EACK,IAAIsB,EAAyBtB,EAAQT,EAAOC,EAASS,GAG/C,MAAXD,EACK,IAAIuB,EAAevB,EAAQT,EAAOC,EAASS,GAGhDD,GAAU,IACL,IAAIwB,EAAoBxB,EAAQT,EAAOC,EAASS,GAGlD,IAAIF,EAASC,EAAQT,EAAOC,EAASS,EAC9C,EAGI,MAAOwB,UAA0B1B,EAGrCxB,WAAAA,GAAkD,IAAtC,QAAEiB,GAAOX,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA2B,CAAC,EAC/CqB,WAAMhC,OAAWA,EAAWsB,GAAW,4BAAwBtB,GAH/C,KAAA8B,YAAoB9B,CAItC,EAGI,MAAO4C,UAA2Bf,EAGtCxB,WAAAA,CAAAmD,GAA+E,IAAnE,QAAElC,EAAO,MAAEuB,GAAwDW,EAC7ExB,WAAMhC,OAAWA,EAAWsB,GAAW,yBAAqBtB,GAH5C,KAAA8B,YAAoB9B,EAMhC6C,IAAOX,KAAKW,MAAQA,EAC1B,EAGI,MAAOY,UAAkCb,EAC7CvC,WAAAA,GAAkD,IAAtC,QAAEiB,GAAOX,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA2B,CAAC,EAC/CqB,MAAM,CAAEV,QAAgB,OAAPA,QAAO,IAAPA,EAAAA,EAAW,sBAC9B,EAGI,MAAOyB,UAAwBlB,EAArCxB,WAAAA,G,oBACoB,KAAAyB,OAAc,GAClC,EAEM,MAAOkB,UAA4BnB,EAAzCxB,WAAAA,G,oBACoB,KAAAyB,OAAc,GAClC,EAEM,MAAOmB,UAA8BpB,EAA3CxB,WAAAA,G,oBACoB,KAAAyB,OAAc,GAClC,EAEM,MAAOoB,UAAsBrB,EAAnCxB,WAAAA,G,oBACoB,KAAAyB,OAAc,GAClC,EAEM,MAAOqB,UAAsBtB,EAAnCxB,WAAAA,G,oBACoB,KAAAyB,OAAc,GAClC,EAEM,MAAOsB,UAAiCvB,EAA9CxB,WAAAA,G,oBACoB,KAAAyB,OAAc,GAClC,EAEM,MAAOuB,UAAuBxB,EAApCxB,WAAAA,G,oBACoB,KAAAyB,OAAc,GAClC,EAEM,MAAOwB,UAA4BzB,GC7InC,MAAO6B,EAGXrD,WAAAA,CACUsD,EACRC,GADQ,KAAAD,SAAAA,EAGRzB,KAAK0B,WAAaA,CACpB,CAEA,sBAAOC,CAAsBC,EAAoBF,GAC/C,IAAIG,GAAW,EACf,MAAMC,EAAU,IAAIC,EAoEpB,OAAO,IAAIP,GA5CXnC,kBACE,GAAIwC,EACF,MAAM,IAAIlD,MAAM,4EAElBkD,GAAW,EACX,IAAIG,GAAO,EACX,IACE,UAAW,MAAMC,KA7BrB5C,kBACE,IAAKuC,EAASxD,KAEZ,MADAsD,EAAWQ,QACL,IAAIxC,EAAY,qDAGxB,MAAMyC,EAAc,IAAIC,EAElBC,EAAOC,EAAmCV,EAASxD,MACzD,UAAW,MAAMmE,KAASF,EACxB,IAAK,MAAMG,KAAQL,EAAYM,OAAOF,GAAQ,CAC5C,MAAMN,EAAMH,EAAQW,OAAOD,GACvBP,UAAWA,E,CAInB,IAAK,MAAMO,KAAQL,EAAYO,QAAS,CACtC,MAAMT,EAAMH,EAAQW,OAAOD,GACvBP,UAAWA,E,CAEnB,CAS4BU,GACtB,IAAIX,EAEJ,GAAIC,EAAIhC,KAAK2C,WAAW,UACtBZ,GAAO,OAIT,GAAkB,OAAdC,EAAIY,MAAgB,CACtB,IAAI5C,EAEJ,IACEA,EAAOK,KAAKwC,MAAMb,EAAIhC,K,CACtB,MAAO8C,GAGP,MAFAC,QAAQ7D,MAAM,qCAAsC8C,EAAIhC,MACxD+C,QAAQ7D,MAAM,cAAe8C,EAAIgB,KAC3BF,C,CAGR,GAAI9C,GAAQA,EAAKd,MACf,MAAM,IAAIQ,OAAS7B,EAAWmC,EAAKd,WAAOrB,OAAWA,SAGjDmC,C,CAGV+B,GAAO,C,CACP,MAAOe,GAEP,GAAIA,aAAapE,OAAoB,eAAXoE,EAAEG,KAAuB,OACnD,MAAMH,C,CACN,QAEKf,GAAMN,EAAWQ,O,CAE1B,GAE4BR,EAC9B,CAMA,yBAAOyB,CAAyBC,EAAgC1B,GAC9D,IAAIG,GAAW,EAuCf,OAAO,IAAIL,GAtBXnC,kBACE,GAAIwC,EACF,MAAM,IAAIlD,MAAM,4EAElBkD,GAAW,EACX,IAAIG,GAAO,EACX,IACE,UAAW,MAAMQ,KAtBrBnD,kBACE,MAAM8C,EAAc,IAAIC,EAElBC,EAAOC,EAAmCc,GAChD,UAAW,MAAMb,KAASF,EACxB,IAAK,MAAMG,KAAQL,EAAYM,OAAOF,SAC9BC,EAIV,IAAK,MAAMA,KAAQL,EAAYO,cACvBF,CAEV,CAS6Ba,GACnBrB,GACAQ,UAAYlC,KAAKwC,MAAMN,IAE7BR,GAAO,C,CACP,MAAOe,GAEP,GAAIA,aAAapE,OAAoB,eAAXoE,EAAEG,KAAuB,OACnD,MAAMH,C,CACN,QAEKf,GAAMN,EAAWQ,O,CAE1B,GAE4BR,EAC9B,CAEA,CAACrD,OAAOiF,iBACN,OAAOtD,KAAKyB,UACd,CAMA8B,GAAAA,GACE,MAAMC,EAA6C,GAC7CC,EAA8C,GAC9ChC,EAAWzB,KAAKyB,WAEhBiC,EAAeC,IACZ,CACLC,KAAMA,KACJ,GAAqB,IAAjBD,EAAMjF,OAAc,CACtB,MAAMmF,EAASpC,EAASmC,OACxBJ,EAAKM,KAAKD,GACVJ,EAAMK,KAAKD,E,CAEb,OAAOF,EAAMI,OAAQ,IAK3B,MAAO,CACL,IAAIvC,GAAO,IAAMkC,EAAYF,IAAOxD,KAAK0B,YACzC,IAAIF,GAAO,IAAMkC,EAAYD,IAAQzD,KAAK0B,YAE9C,CAOAsC,gBAAAA,GACE,MAAMC,EAAOjE,KACb,IAAIqC,EACJ,MAAM6B,EAAU,IAAIC,YAEpB,OAAO,IAAI5G,EAAe,CACxB,WAAM6G,GACJ/B,EAAO4B,EAAK5F,OAAOiF,gBACrB,EACA,UAAMe,CAAKC,GACT,IACE,MAAM,MAAE7E,EAAK,KAAEuC,SAAeK,EAAKuB,OACnC,GAAI5B,EAAM,OAAOsC,EAAKC,QAEtB,MAAMC,EAAQN,EAAQO,OAAOnE,KAAKC,UAAUd,GAAS,MAErD6E,EAAKI,QAAQF,E,CACb,MAAOG,GACPL,EAAKnF,MAAMwF,E,CAEf,EACA,YAAMC,GAAM,IAAAC,EAAAC,QACO,QAAjBD,GAAMC,EAAAzC,GAAK0C,cAAM,IAAAF,OAAA,EAAXA,EAAAG,KAAAF,GACR,GAEJ,EAGF,MAAM/C,EAKJ5D,WAAAA,GACE6B,KAAK6C,MAAQ,KACb7C,KAAKC,KAAO,GACZD,KAAKiF,OAAS,EAChB,CAEAxC,MAAAA,CAAOD,GAKL,GAJIA,EAAK0C,SAAS,QAChB1C,EAAOA,EAAK2C,UAAU,EAAG3C,EAAK9D,OAAS,KAGpC8D,EAAM,CAET,IAAKxC,KAAK6C,QAAU7C,KAAKC,KAAKvB,OAAQ,OAAO,KAE7C,MAAMuD,EAAuB,CAC3BY,MAAO7C,KAAK6C,MACZ5C,KAAMD,KAAKC,KAAKmF,KAAK,MACrBnC,IAAKjD,KAAKiF,QAOZ,OAJAjF,KAAK6C,MAAQ,KACb7C,KAAKC,KAAO,GACZD,KAAKiF,OAAS,GAEPhD,C,CAKT,GAFAjC,KAAKiF,OAAOnB,KAAKtB,GAEbA,EAAKI,WAAW,KAClB,OAAO,KAGT,IAAKyC,EAAWC,EAAG7F,GAyHvB,SAAmB8F,EAAaC,GAC9B,MAAMC,EAAQF,EAAIG,QAAQF,GAC1B,IAAe,IAAXC,EACF,MAAO,CAACF,EAAIJ,UAAU,EAAGM,GAAQD,EAAWD,EAAIJ,UAAUM,EAAQD,EAAU9G,SAG9E,MAAO,CAAC6G,EAAK,GAAI,GACnB,CAhIgCI,CAAUnD,EAAM,KAY5C,OAVI/C,EAAMmD,WAAW,OACnBnD,EAAQA,EAAM0F,UAAU,IAGR,UAAdE,EACFrF,KAAK6C,MAAQpD,EACU,SAAd4F,GACTrF,KAAKC,KAAK6D,KAAKrE,GAGV,IACT,EASF,MAAM2C,EASJjE,WAAAA,GACE6B,KAAK4F,OAAS,GACd5F,KAAK6F,YAAa,CACpB,CAEApD,MAAAA,CAAOF,GACL,IAAIuD,EAAO9F,KAAK+F,WAAWxD,GAW3B,GATIvC,KAAK6F,aACPC,EAAO,KAAOA,EACd9F,KAAK6F,YAAa,GAEhBC,EAAKZ,SAAS,QAChBlF,KAAK6F,YAAa,EAClBC,EAAOA,EAAKE,MAAM,GAAI,KAGnBF,EACH,MAAO,GAGT,MAAMG,EAAkB7D,EAAY8D,cAAcC,IAAIL,EAAKA,EAAKpH,OAAS,IAAM,IAC/E,IAAI0H,EAAQN,EAAKO,MAAMjE,EAAYkE,gBAEnC,OAAqB,IAAjBF,EAAM1H,QAAiBuH,GAKvBjG,KAAK4F,OAAOlH,OAAS,IACvB0H,EAAQ,CAACpG,KAAK4F,OAAOR,KAAK,IAAMgB,EAAM,MAAOA,EAAMJ,MAAM,IACzDhG,KAAK4F,OAAS,IAGXK,IACHjG,KAAK4F,OAAS,CAACQ,EAAMG,OAAS,KAGzBH,IAbLpG,KAAK4F,OAAO9B,KAAKsC,EAAM,IAChB,GAaX,CAEAL,UAAAA,CAAWvB,GACT,GAAa,MAATA,EAAe,MAAO,GAC1B,GAAqB,kBAAVA,EAAoB,OAAOA,EAGtC,GAAsB,qBAAXgC,OAAwB,CACjC,GAAIhC,aAAiBgC,OACnB,OAAOhC,EAAMiC,WAEf,GAAIjC,aAAiBkC,WACnB,OAAOF,OAAOG,KAAKnC,GAAOiC,WAG5B,MAAM,IAAI/G,EAAY,wCAADd,OACqB4F,EAAMrG,YAAY+E,KAAI,qI,CAKlE,GAA2B,qBAAhB0D,YAA6B,CAC2B,IAAAC,EAAjE,GAAIrC,aAAiBkC,YAAclC,aAAiBsC,YAElD,OADgB,QAAhBD,EAAA7G,KAAK+G,mBAAW,IAAAF,IAAhB7G,KAAK+G,YAAgB,IAAIH,YAAY,SAC9B5G,KAAK+G,YAAYtE,OAAO+B,GAGjC,MAAM,IAAI9E,EAAY,oDAADd,OAEhB4F,EAAcrG,YAAY+E,KAC7B,kD,CAIJ,MAAM,IAAIxD,EAAY,iGAGxB,CAEAgD,KAAAA,GACE,IAAK1C,KAAK4F,OAAOlH,SAAWsB,KAAK6F,WAC/B,MAAO,GAGT,MAAMO,EAAQ,CAACpG,KAAK4F,OAAOR,KAAK,KAGhC,OAFApF,KAAK4F,OAAS,GACd5F,KAAK6F,YAAa,EACXO,CACT,EAkBI,SAAU9D,EAA+B0E,GAC7C,GAAIA,EAAO3I,OAAOiF,eAAgB,OAAO0D,EAEzC,MAAMC,EAASD,EAAOE,YACtB,MAAO,CACL,UAAMtD,GACJ,IACE,MAAMC,QAAeoD,EAAOE,OAE5B,OADU,OAANtD,QAAM,IAANA,GAAAA,EAAQ7B,MAAMiF,EAAOG,cAClBvD,C,CACP,MAAOd,GAEP,MADAkE,EAAOG,cACDrE,C,CAEV,EACA,YAAMgC,GACJ,MAAMsC,EAAgBJ,EAAOrC,SAG7B,OAFAqC,EAAOG,oBACDC,EACC,CAAErF,MAAM,EAAMvC,WAAO3B,EAC9B,EACA,CAACO,OAAOiF,iBACN,OAAOtD,IACT,EAEJ,CAzISoC,EAAA8D,cAAgB,IAAIoB,IAAI,CAAC,KAAM,KAAM,KAAQ,KAAQ,OAAQ,OAAQ,OAAQ,OAAQ,SAAU,WAC/FlF,EAAAkE,eAAiB,mDCpNnB,MAAMiB,EAAkB9H,GACpB,MAATA,GACiB,kBAAVA,GACc,kBAAdA,EAAMD,KACS,oBAAfC,EAAM+H,KAaFC,EAAchI,GAChB,MAATA,GACiB,kBAAVA,GACe,kBAAfA,EAAMiI,MACS,kBAAfjI,EAAMW,MACS,oBAAfX,EAAMqG,MACU,oBAAhBrG,EAAMuG,OACgB,oBAAtBvG,EAAMkI,YAEFC,EAAgBnI,GApBFA,IAChB,MAATA,GACiB,kBAAVA,GACe,kBAAfA,EAAMyD,MACiB,kBAAvBzD,EAAMoI,cACbJ,EAAWhI,GAgBJqI,CAAWrI,IAAU8H,EAAe9H,IAAU9B,EAAe8B,GAc/DJ,eAAe0I,EACpBtI,EACAyD,GACyC,IAAA8E,EAAA,IAAzCxJ,EAAAC,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAuC,CAAC,EAKxC,GAFAgB,QAAcA,EAEV8H,EAAe9H,GAAQ,KAAAwI,EACzB,MAAMT,QAAa/H,EAAM+H,OAGzB,OAFAtE,IAAAA,EAAyD,QAArD+E,EAAK,IAAIC,IAAIzI,EAAMD,KAAK2I,SAAS9B,MAAM,SAASE,aAAK,IAAA0B,EAAAA,EAAI,gBAEtD,IAAI3K,EAAK,CAACkK,GAActE,EAAM1E,E,CAGvC,MAAM4J,QAcR/I,eAAwBI,GACtB,IAAI4I,EAAyB,GAC7B,GACmB,kBAAV5I,GACPqH,YAAYwB,OAAO7I,IACnBA,aAAiBqH,YAEjBuB,EAAMvE,KAAKrE,QACN,GAAIgI,EAAWhI,GACpB4I,EAAMvE,WAAWrE,EAAMkI,mBAClB,CAMA,IAAAY,EANA,IACLC,EAAwB/I,GAMxB,MAAM,IAAId,MAAM,yBAADC,cACmBa,EAAK,mBAAAb,OAAuB,OAALa,QAAK,IAALA,GAAkB,QAAb8I,EAAL9I,EAAOtB,mBAAW,IAAAoK,OAAA,EAAlBA,EACnDrF,KAAI,aAAAtE,OAOd,SAAuBa,GACrB,MAAMgJ,EAAQC,OAAOC,oBAAoBlJ,GACzC,MAAO,IAAPb,OAAW6J,EAAMG,KAAKC,GAAC,IAAAjK,OAASiK,EAAC,OAAKzD,KAAK,MAAK,IAClD,CAV0B0D,CAAcrJ,KANpC,UAAW,MAAM8C,KAAS9C,EACxB4I,EAAMvE,KAAKvB,E,CASf,OAAO8F,CACT,CAtCqBU,CAAStJ,GAI5B,GAFAyD,IAAAA,EAAuB,QAAnB8E,EA2CN,SAAiBvI,GAAU,IAAAuJ,EACzB,OACEC,EAAyBxJ,EAAMyD,OAC/B+F,EAAyBxJ,EAAMyJ,YAEK,QADpCF,EACAC,EAAyBxJ,EAAM0J,aAAK,IAAAH,OAAA,EAApCA,EAAsC3C,MAAM,SAASE,MAEzD,CAlDW6C,CAAQ3J,UAAM,IAAAuI,EAAAA,EAAI,iBAEtBxJ,EAAQ4B,KAAM,KAAAiJ,EACjB,MAAMjJ,EAAuB,QAAnBiJ,EAAIjB,EAAK,UAAU,IAAAiB,OAAA,EAAfA,EAAiBjJ,KACX,kBAATA,IACT5B,EAAU,IAAKA,EAAS4B,Q,CAI5B,OAAO,IAAI9C,EAAK8K,EAAMlF,EAAM1E,EAC9B,CA0CA,MAAMyK,EAA4BK,GACf,kBAANA,EAAuBA,EACZ,qBAAX9C,QAA0B8C,aAAa9C,OAAe+C,OAAOD,QAAxE,EAIId,EAA2B/I,GACtB,MAATA,GAAkC,kBAAVA,GAA6D,oBAAhCA,EAAMpB,OAAOiF,eAEvDkG,EAAmBpL,GAC9BA,GAAwB,kBAATA,GAAqBA,EAAKA,MAAqC,kBAA7BA,EAAKC,OAAOC,aAelDmL,EAA8BpK,UAGzC,MAAMC,QAAaoK,EAAWnK,EAAKnB,MACnC,OAAOZ,EAA2B8B,EAAMC,EAAK,EAGlCmK,EAAarK,UACxB,MAAMC,EAAO,IAAIjC,EAEjB,aADMsM,QAAQC,IAAIlB,OAAOmB,QAAQzL,GAAQ,CAAC,GAAGwK,KAAItH,IAAA,IAAEwI,EAAKrK,GAAM6B,EAAA,OAAKyI,EAAazK,EAAMwK,EAAKrK,EAAM,KAC1FH,CAAI,EAcPyK,EAAe1K,MAAOC,EAAgBwK,EAAarK,KACvD,QAAc3B,IAAV2B,EAAJ,CACA,GAAa,MAATA,EACF,MAAM,IAAIuK,UAAU,sBAADpL,OACKkL,EAAG,iEAK7B,GAAqB,kBAAVrK,GAAuC,kBAAVA,GAAuC,mBAAVA,EACnEH,EAAK2K,OAAOH,EAAKP,OAAO9J,SACnB,GAAImI,EAAanI,GAAQ,CAC9B,MAAMyK,QAAanC,EAAOtI,GAC1BH,EAAK2K,OAAOH,EAAKI,E,MACZ,GAAIC,MAAMC,QAAQ3K,SACjBkK,QAAQC,IAAInK,EAAMmJ,KAAKyB,GAAUN,EAAazK,EAAMwK,EAAM,KAAMO,UACjE,IAAqB,kBAAV5K,EAKhB,MAAM,IAAIuK,UAAU,wGAADpL,OACuFa,EAAK,mBALzGkK,QAAQC,IACZlB,OAAOmB,QAAQpK,GAAOmJ,KAAI0B,IAAA,IAAEpH,EAAMqH,GAAKD,EAAA,OAAKP,EAAazK,EAAM,GAAFV,OAAKkL,EAAG,KAAAlL,OAAIsE,EAAI,KAAKqH,EAAK,I,CAjB5D,C,8pBCrLjClL,eAAemL,EAAwB/B,GACrC,MAAM,SAAE7G,GAAa6G,EACrB,GAAIA,EAAMjK,QAAQwI,OAMhB,OALAyD,GAAM,WAAY7I,EAAShC,OAAQgC,EAASpC,IAAKoC,EAAS/B,QAAS+B,EAASxD,MAKxEqK,EAAMjK,QAAQkM,cACTjC,EAAMjK,QAAQkM,cAAc/I,gBAAgBC,EAAU6G,EAAM/G,YAG9DF,EAAOG,gBAAgBC,EAAU6G,EAAM/G,YAIhD,GAAwB,MAApBE,EAAShC,OACX,OAAO,KAGT,GAAI6I,EAAMjK,QAAQmM,iBAChB,OAAO/I,EAGT,MAAMgJ,EAAchJ,EAAS/B,QAAQgL,IAAI,gBACzC,GAAe,OAAXD,QAAW,IAAXA,GAAAA,EAAaE,SAAS,oBAAqB,CAC7C,MAAMC,QAAanJ,EAASmJ,OAI5B,OAFAN,GAAM,WAAY7I,EAAShC,OAAQgC,EAASpC,IAAKoC,EAAS/B,QAASkL,GAE5DA,C,CAGT,MAAMjF,QAAalE,EAASkE,OAI5B,OAHA2E,GAAM,WAAY7I,EAAShC,OAAQgC,EAASpC,IAAKoC,EAAS/B,QAASiG,GAG5DA,CACT,CAMM,MAAOkF,UAAsBrB,QAGjCxL,WAAAA,CACU8M,GACoF,IAApFC,EAAAzM,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAgE+L,EAExE1K,OAAOqL,IAILA,EAAQ,KAAY,IAPd,KAAAF,gBAAAA,EACA,KAAAC,cAAAA,CAQV,CAEAE,WAAAA,CAAeC,GACb,OAAO,IAAIL,EAAWhL,KAAKiL,iBAAiB5L,SAAiBgM,QAAgBrL,KAAKkL,cAAczC,KAClG,CAeA6C,UAAAA,GACE,OAAOtL,KAAKiL,gBAAgBM,MAAM1C,GAAMA,EAAEjH,UAC5C,CAcA,kBAAM4J,GACJ,MAAOvL,EAAM2B,SAAkB+H,QAAQC,IAAI,CAAC5J,KAAK8C,QAAS9C,KAAKsL,eAC/D,MAAO,CAAErL,OAAM2B,WACjB,CAEQkB,KAAAA,GAIN,OAHK9C,KAAKyL,gBACRzL,KAAKyL,cAAgBzL,KAAKiL,gBAAgBM,KAAKvL,KAAKkL,gBAE/ClL,KAAKyL,aACd,CAESF,IAAAA,CACPG,EACAC,GAEA,OAAO3L,KAAK8C,QAAQyI,KAAKG,EAAaC,EACxC,CAESC,MACPD,GAEA,OAAO3L,KAAK8C,QAAQ8I,MAAMD,EAC5B,CAESE,QAAQC,GACf,OAAO9L,KAAK8C,QAAQ+I,QAAQC,EAC9B,EAGI,MAAgBC,EASpB5N,WAAAA,CAAAmD,GAYC,IAZW,QACV0K,EAAO,WACPC,EAAa,EAAC,QACdC,EAAU,IAAM,UAChBC,EACA/O,MAAOgP,GAOR9K,EACCtB,KAAKgM,QAAUA,EACfhM,KAAKiM,WAAaI,GAAwB,aAAcJ,GACxDjM,KAAKkM,QAAUG,GAAwB,UAAWH,GAClDlM,KAAKmM,UAAYA,EAEjBnM,KAAK5C,MAAsB,OAAdgP,QAAc,IAAdA,EAAAA,EAAkBhP,CACjC,CAEUkP,WAAAA,CAAY/M,GACpB,MAAO,CAAC,CACV,CAUUgN,cAAAA,CAAehN,GACvB,MAAO,CACLiN,OAAQ,mBACR,eAAgB,mBAChB,aAAcxM,KAAKyM,kBAChBC,QACA1M,KAAKsM,YAAY/M,GAExB,CAOUoN,eAAAA,CAAgB9M,EAAkB+M,GAAyB,CAE3DC,qBAAAA,GACR,MAAO,wBAAPjO,OAA+BkO,KACjC,CAEAjC,GAAAA,CAAc1B,EAAc5J,GAC1B,OAAOS,KAAK+M,cAAc,MAAO5D,EAAM5J,EACzC,CAEAyN,IAAAA,CAAe7D,EAAc5J,GAC3B,OAAOS,KAAK+M,cAAc,OAAQ5D,EAAM5J,EAC1C,CAEA0N,KAAAA,CAAgB9D,EAAc5J,GAC5B,OAAOS,KAAK+M,cAAc,QAAS5D,EAAM5J,EAC3C,CAEA2N,GAAAA,CAAc/D,EAAc5J,GAC1B,OAAOS,KAAK+M,cAAc,MAAO5D,EAAM5J,EACzC,CAEA4N,OAAiBhE,EAAc5J,GAC7B,OAAOS,KAAK+M,cAAc,SAAU5D,EAAM5J,EAC5C,CAEQwN,aAAAA,CACNK,EACAjE,EACA5J,GAEA,OAAOS,KAAKqN,QAAQ1D,QAAQwB,QAAQ5L,GAAMgM,MAAMhM,IAAI,CAAQ6N,SAAQjE,UAAS5J,MAC/E,CAEA+N,UAAAA,CACEnE,EACAoE,EACAhO,GAEA,OAAOS,KAAKwN,eAAeD,EAAM,CAAEH,OAAQ,MAAOjE,UAAS5J,GAC7D,CAEQkO,sBAAAA,CAAuBrP,GAC7B,GAAoB,kBAATA,EAAmB,CAC5B,GAAsB,qBAAXoI,OACT,OAAOA,OAAOkH,WAAWtP,EAAM,QAAQqI,WAGzC,GAA2B,qBAAhBtC,YAA6B,CAGtC,OAFgB,IAAIA,aACIM,OAAOrG,GAChBM,OAAO+H,U,EAI1B,OAAO,IACT,CAEAkH,YAAAA,CAAkBnP,GAAiC,IAAAoP,EAAAtD,EAAAuD,EAAAC,EAAAC,EAAAC,EACjD,MAAM,OAAEZ,EAAM,KAAEjE,EAAI,MAAE8E,EAAK,QAAWpO,EAAU,CAAC,GAAMrB,EAEjDJ,EACJoL,EAAgBhL,EAAQJ,MAAQI,EAAQJ,KAAKA,KAC3CI,EAAQJ,KAAOkC,KAAKC,UAAU/B,EAAQJ,KAAM,KAAM,GAClD,KACE8P,EAAgBlO,KAAKyN,uBAAuBrP,GAE5CoB,EAAMQ,KAAKmO,SAAShF,EAAO8E,GAC7B,YAAazP,GAAS6N,GAAwB,UAAW7N,EAAQ0N,SACrE,MAAMA,EAAyB,QAAlB0B,EAAGpP,EAAQ0N,eAAO,IAAA0B,EAAAA,EAAI5N,KAAKkM,QAClCC,EAA+C,QAAtC7B,EAAoB,QAApBuD,EAAGrP,EAAQ2N,iBAAS,IAAA0B,EAAAA,EAAI7N,KAAKmM,iBAAS,IAAA7B,EAAAA,EAAI7M,EAAgB+B,GACnE4O,EAAkBlC,EAAU,IAEgB,kBAAvB,OAAjBC,QAAiB,IAAjBA,GAA0B,QAAT2B,EAAjB3B,EAAmB3N,eAAO,IAAAsP,OAAA,EAA1BA,EAA4B5B,UACpCkC,GAAqD,QAAtCL,EAAK5B,EAAkB3N,QAAQ0N,eAAO,IAAA6B,EAAAA,EAAI,KAMxD5B,EAAkB3N,QAAQ0N,QAAUkC,GAGnCpO,KAAKqO,mBAAgC,QAAXjB,IACvB5O,EAAQ8P,iBAAgB9P,EAAQ8P,eAAiBtO,KAAK6M,yBAC3DhN,EAAQG,KAAKqO,mBAAqB7P,EAAQ8P,gBAe5C,MAAO,CAAEC,IAVgB,CACvBnB,YACIhP,GAAQ,CAAEA,KAAMA,GACpByB,QALiBG,KAAKwO,aAAa,CAAEhQ,UAASqB,UAASqO,qBAMnD/B,GAAa,CAAEsC,MAAOtC,GAG1BuC,OAAsB,QAAhBV,EAAExP,EAAQkQ,cAAM,IAAAV,EAAAA,EAAI,MAGdxO,MAAK0M,UACrB,CAEQsC,YAAAA,CAAYG,GAQnB,IARoB,QACnBnQ,EAAO,QACPqB,EAAO,cACPqO,GAKDS,EACC,MAAMC,EAAqC,CAAC,EACxCV,IACFU,EAAW,kBAAoBV,GAcjC,OAVAW,GAAgBD,EADO5O,KAAKuM,eAAe/N,IAE3CqQ,GAAgBD,EAAY/O,GAGxB2J,EAAgBhL,EAAQJ,OAAuB,SAAd0Q,UAC5BF,EAAW,gBAGpB5O,KAAK2M,gBAAgBiC,EAAY/O,GAE1B+O,CACT,CAKU,oBAAMG,CAAevQ,GAA8C,CAQnE,oBAAMwQ,CACd3B,EAAoB4B,GAC2C,IAA/D,IAAEzP,EAAG,QAAEhB,GAAwDyQ,CAC/C,CAERC,YAAAA,CAAarP,GACrB,OACGA,EACCxB,OAAOoD,YAAY5B,EACnB6I,OAAOyG,YAAYhF,MAAMxD,KAAK9G,GAA+B+I,KAAKwG,GAAW,IAAIA,MACjF,IAAKvP,GAHI,CAAC,CAKhB,CAEUwP,eAAAA,CACRzP,EACAT,EACAC,EACAS,GAEA,OAAOF,EAASa,SAASZ,EAAQT,EAAOC,EAASS,EACnD,CAEAwN,OAAAA,CACE7O,GACsC,IAAtC8Q,EAAA7Q,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAkC,KAElC,OAAO,IAAIuM,EAAWhL,KAAKuP,YAAY/Q,EAAS8Q,GAClD,CAEQ,iBAAMC,CACZC,EACAC,GAA+B,IAAAC,EAE/B,MAAMlR,QAAgBgR,EACQ,IAAAG,EAAN,MAApBF,IACFA,EAAqC,QAArBE,EAAGnR,EAAQyN,kBAAU,IAAA0D,EAAAA,EAAI3P,KAAKiM,kBAG1CjM,KAAK+O,eAAevQ,GAE1B,MAAM,IAAE+P,EAAG,IAAE/O,EAAG,QAAE0M,GAAYlM,KAAK2N,aAAanP,GAMhD,SAJMwB,KAAKgP,eAAeT,EAAK,CAAE/O,MAAKhB,YAEtCiM,GAAM,UAAWjL,EAAKhB,EAAS+P,EAAI1O,SAEjB,QAAlB6P,EAAIlR,EAAQkQ,cAAM,IAAAgB,GAAdA,EAAgBE,QAClB,MAAM,IAAIvO,EAGZ,MAAMK,EAAa,IAAImO,gBACjBjO,QAAiB5B,KAAK8P,iBAAiBtQ,EAAK+O,EAAKrC,EAASxK,GAAYkK,MAAMhL,IAElF,GAAIgB,aAAoBjD,MAAO,KAAAoR,EAC7B,GAAkB,QAAlBA,EAAIvR,EAAQkQ,cAAM,IAAAqB,GAAdA,EAAgBH,QAClB,MAAM,IAAIvO,EAEZ,GAAIoO,EACF,OAAOzP,KAAKgQ,aAAaxR,EAASiR,GAEpC,GAAsB,eAAlB7N,EAASsB,KACX,MAAM,IAAI3B,EAEZ,MAAM,IAAIb,EAAmB,CAAEC,MAAOiB,G,CAGxC,MAAMqO,EAAkBC,GAAsBtO,EAAS/B,SAEvD,IAAK+B,EAASuO,GAAI,CAChB,GAAIV,GAAoBzP,KAAKoQ,YAAYxO,GAAW,CAClD,MAAMyO,EAAe,aAAHzR,OAAgB6Q,EAAgB,uBAElD,OADAhF,GAAM,oBAAD7L,OAAqByR,EAAY,KAAKzO,EAAShC,OAAQJ,EAAKyQ,GAC1DjQ,KAAKgQ,aAAaxR,EAASiR,EAAkBQ,E,CAGtD,MAAMK,QAAgB1O,EAASkE,OAAO8F,OAAO7I,GAAMnC,GAAYmC,GAAG3D,UAC5DmR,EAAUC,GAASF,GACnBG,EAAaF,OAAUzS,EAAYwS,EAGzC7F,GAAM,oBAAD7L,OAFgB6Q,EAAmB,gCAAH,yBAEC,KAAK7N,EAAShC,OAAQJ,EAAKyQ,EAAiBQ,GAGlF,MADYzQ,KAAKqP,gBAAgBzN,EAAShC,OAAQ2Q,EAASE,EAAYR,E,CAIzE,MAAO,CAAErO,WAAUpD,UAASkD,aAC9B,CAEA8L,cAAAA,CACED,EACA/O,GAEA,MAAM6O,EAAUrN,KAAKuP,YAAY/Q,EAAS,MAC1C,OAAO,IAAIkS,GAA6B1Q,KAAMqN,EAASE,EACzD,CAEAY,QAAAA,CAAchF,EAAc8E,GAC1B,MAAMzO,EACJmR,GAAcxH,GACZ,IAAIjB,IAAIiB,GACR,IAAIjB,IAAIlI,KAAKgM,SAAWhM,KAAKgM,QAAQ9G,SAAS,MAAQiE,EAAKvG,WAAW,KAAOuG,EAAKnD,MAAM,GAAKmD,IAE3FyH,EAAe5Q,KAAK4Q,eAS1B,OARKC,GAAWD,KACd3C,EAAQ,IAAK2C,KAAiB3C,IAGX,kBAAVA,GAAsBA,IAAU9D,MAAMC,QAAQ6D,KACvDzO,EAAIsR,OAAS9Q,KAAK+Q,eAAe9C,IAG5BzO,EAAIiH,UACb,CAEUsK,cAAAA,CAAe9C,GACvB,OAAOvF,OAAOmB,QAAQoE,GACnB+C,QAAOC,IAAA,IAAE3L,EAAG7F,GAAMwR,EAAA,MAAsB,qBAAVxR,CAAqB,IACnDmJ,KAAIsI,IAAiB,IAAfpH,EAAKrK,GAAMyR,EAChB,GAAqB,kBAAVzR,GAAuC,kBAAVA,GAAuC,mBAAVA,EACnE,MAAO,GAAPb,OAAUuS,mBAAmBrH,GAAI,KAAAlL,OAAIuS,mBAAmB1R,IAE1D,GAAc,OAAVA,EACF,MAAO,GAAPb,OAAUuS,mBAAmBrH,GAAI,KAEnC,MAAM,IAAIpK,EAAY,yBAADd,cACaa,EAAK,qQACtC,IAEF2F,KAAK,IACV,CAEA,sBAAM0K,CACJtQ,EACA4R,EACAC,EACA3P,GAEA,MAAM,OAAEgN,KAAWlQ,GAAY4S,GAAQ,CAAC,EACpC1C,GAAQA,EAAO4C,iBAAiB,SAAS,IAAM5P,EAAWQ,UAE9D,MAAMgK,EAAUqF,YAAW,IAAM7P,EAAWQ,SAASmP,GAErD,OACErR,KAAKwR,mBAEFpU,MAAM4H,UAAKlH,EAAW0B,EAAK,CAAEkP,OAAQhN,EAAWgN,UAAkBlQ,IAClEqN,SAAQ,KACP4F,aAAavF,EAAQ,GAG7B,CAEUsF,gBAAAA,GACR,MAAO,CAAEpU,MAAO4C,KAAK5C,MACvB,CAEQgT,WAAAA,CAAYxO,GAElB,MAAM8P,EAAoB9P,EAAS/B,QAAQgL,IAAI,kBAG/C,MAA0B,SAAtB6G,GACsB,UAAtBA,IAGoB,MAApB9P,EAAShC,SAGW,MAApBgC,EAAShC,SAGW,MAApBgC,EAAShC,QAGTgC,EAAShC,QAAU,MAGzB,CAEQ,kBAAMoQ,CACZxR,EACAiR,EACAQ,GAEA,IAAI0B,EAGJ,MAAMC,EAAwC,OAAf3B,QAAe,IAAfA,OAAe,EAAfA,EAAkB,kBACjD,GAAI2B,EAAwB,CAC1B,MAAMC,EAAYC,WAAWF,GACxBG,OAAOC,MAAMH,KAChBF,EAAgBE,E,CAKpB,MAAMI,EAAkC,OAAfhC,QAAe,IAAfA,OAAe,EAAfA,EAAkB,eAC3C,GAAIgC,IAAqBN,EAAe,CACtC,MAAMO,EAAiBJ,WAAWG,GAIhCN,EAHGI,OAAOC,MAAME,GAGAC,KAAKrP,MAAMmP,GAAoBE,KAAKC,MAFnB,IAAjBF,C,CAQpB,KAAMP,GAAiB,GAAKA,GAAiBA,EAAgB,KAAY,KAAAU,EACvE,MAAMpG,EAA+B,QAArBoG,EAAG7T,EAAQyN,kBAAU,IAAAoG,EAAAA,EAAIrS,KAAKiM,WAC9C0F,EAAgB3R,KAAKsS,mCAAmC7C,EAAkBxD,E,CAI5E,aAFMsG,GAAMZ,GAEL3R,KAAKuP,YAAY/Q,EAASiR,EAAmB,EACtD,CAEQ6C,kCAAAA,CAAmC7C,EAA0BxD,GACnE,MAGMuG,EAAavG,EAAawD,EAQhC,OALqBgD,KAAKC,IANA,GAMwBD,KAAKE,IAAI,EAAGH,GALxC,IAQP,EAAoB,IAAhBC,KAAKG,UAEO,GACjC,CAEQnG,YAAAA,GACN,MAAO,GAAP7N,OAAUoB,KAAK7B,YAAY+E,KAAI,QAAAtE,OAAO1B,EACxC,EAKI,MAAgB2V,EAOpB1U,WAAAA,CAAY2U,EAAmBlR,EAAoBxD,EAAeI,GANlEuU,EAAAC,IAAA,aAOEC,EAAAjT,KAAI+S,EAAWD,EAAM,KACrB9S,KAAKxB,QAAUA,EACfwB,KAAK4B,SAAWA,EAChB5B,KAAK5B,KAAOA,CACd,CAUA8U,WAAAA,GAEE,QADclT,KAAKmT,oBACRzU,QACmB,MAAvBsB,KAAKoT,cACd,CAEA,iBAAMC,GACJ,MAAMC,EAAWtT,KAAKoT,eACtB,IAAKE,EACH,MAAM,IAAI5T,EACR,yFAGJ,MAAM6T,EAAc,IAAKvT,KAAKxB,SAC9B,GAAI,WAAY8U,GAAyC,kBAAtBC,EAAYtF,MAC7CsF,EAAYtF,MAAQ,IAAKsF,EAAYtF,SAAUqF,EAASE,aACnD,GAAI,QAASF,EAAU,CAC5B,MAAME,EAAS,IAAI9K,OAAOmB,QAAQ0J,EAAYtF,OAAS,CAAC,MAAOqF,EAAS9T,IAAIiU,aAAa5J,WACzF,IAAK,MAAOC,EAAKrK,KAAU+T,EACzBF,EAAS9T,IAAIiU,aAAaT,IAAIlJ,EAAKrK,GAErC8T,EAAYtF,WAAQnQ,EACpByV,EAAYpK,KAAOmK,EAAS9T,IAAIiH,U,CAElC,aAAaiN,EAAA1T,KAAI+S,EAAA,KAASvF,eAAexN,KAAK7B,YAAoBoV,EACpE,CAEA,eAAOI,GAEL,IAAIC,EAA2B5T,KAE/B,UADM4T,EACCA,EAAKV,eACVU,QAAaA,EAAKP,oBACZO,CAEV,CAEA,QAAOb,EAAA,IAAAc,QAACxV,OAAOiF,kBACb,UAAW,MAAMsQ,KAAQ5T,KAAK2T,YAC5B,IAAK,MAAMG,KAAQF,EAAKT,0BAChBW,CAGZ,EAYI,MAAOpD,WAIH1F,EAGR7M,WAAAA,CACE2U,EACAzF,EACAE,GAEAzN,MACEuN,GACAhO,SAAiB,IAAIkO,EAAKuF,EAAQrK,EAAM7G,eAAgB4I,EAAqB/B,GAAQA,EAAMjK,UAE/F,CASA,OAAQH,OAAOiF,iBACb,MAAMsQ,QAAa5T,MACnB,UAAW,MAAM8T,KAAQF,QACjBE,CAEV,EAGK,MAAM5D,GACXrQ,GAEO,IAAIkU,MACTrL,OAAOyG,YAELtP,EAAQgK,WAEV,CACEgB,GAAAA,CAAImJ,EAAQ9Q,GACV,MAAM4G,EAAM5G,EAAKuD,WACjB,OAAOuN,EAAOlK,EAAImK,gBAAkBD,EAAOlK,EAC7C,IAiCAoK,GAA+C,CACnD9G,QAAQ,EACRjE,MAAM,EACN8E,OAAO,EACP7P,MAAM,EACNyB,SAAS,EAEToM,YAAY,EACZjF,QAAQ,EACRkF,SAAS,EACTC,WAAW,EACXuC,QAAQ,EACRJ,gBAAgB,EAEhB3D,kBAAkB,EAClBD,eAAe,GAGJyJ,GAAoBC,GAEd,kBAARA,GACC,OAARA,IACCvD,GAAWuD,IACZ1L,OAAO2L,KAAKD,GAAKE,OAAOC,GAAMC,GAAON,GAAoBK,KA+BvDE,GAAwBA,KAC5B,GAAoB,qBAATC,MAAsC,MAAdA,KAAKC,MACtC,MAAO,CACL,mBAAoB,KACpB,8BAA+BzX,EAC/B,iBAAkB0X,GAAkBF,KAAKC,MAAME,IAC/C,mBAAoBC,GAAcJ,KAAKC,MAAMI,MAC7C,sBAAuB,OACvB,8BAA+BL,KAAKM,SAGxC,GAA2B,qBAAhBC,YACT,MAAO,CACL,mBAAoB,KACpB,8BAA+B/X,EAC/B,iBAAkB,UAClB,mBAAoB,SAAF0B,OAAWqW,aAC7B,sBAAuB,OACvB,8BAA+BC,QAAQF,SAI3C,GAAqF,qBAAjFtM,OAAOyM,UAAU1O,SAASzB,KAAwB,qBAAZkQ,QAA0BA,QAAU,GAC5E,MAAO,CACL,mBAAoB,KACpB,8BAA+BhY,EAC/B,iBAAkB0X,GAAkBM,QAAQE,UAC5C,mBAAoBN,GAAcI,QAAQH,MAC1C,sBAAuB,OACvB,8BAA+BG,QAAQF,SAI3C,MAAMK,EA+BR,WACE,GAAyB,qBAAdC,YAA8BA,UACvC,OAAO,KAIT,MAAMC,EAAkB,CACtB,CAAEzL,IAAK,OAAiB0L,QAAS,wCACjC,CAAE1L,IAAK,KAAe0L,QAAS,wCAC/B,CAAE1L,IAAK,KAAe0L,QAAS,8CAC/B,CAAE1L,IAAK,SAAmB0L,QAAS,0CACnC,CAAE1L,IAAK,UAAoB0L,QAAS,2CACpC,CAAE1L,IAAK,SAAmB0L,QAAS,sEAIrC,IAAK,MAAM,IAAE1L,EAAG,QAAE0L,KAAaD,EAAiB,CAC9C,MAAME,EAAQD,EAAQE,KAAKJ,UAAUK,WACrC,GAAIF,EAAO,CACT,MAAMG,EAAQH,EAAM,IAAM,EACpBI,EAAQJ,EAAM,IAAM,EACpBxI,EAAQwI,EAAM,IAAM,EAE1B,MAAO,CAAEK,QAAShM,EAAKkL,QAAS,GAAFpW,OAAKgX,EAAK,KAAAhX,OAAIiX,EAAK,KAAAjX,OAAIqO,G,EAIzD,OAAO,IACT,CA3DsB8I,GACpB,OAAIV,EACK,CACL,mBAAoB,KACpB,8BAA+BnY,EAC/B,iBAAkB,UAClB,mBAAoB,UACpB,sBAAuB,WAAF0B,OAAayW,EAAYS,SAC9C,8BAA+BT,EAAYL,SAKxC,CACL,mBAAoB,KACpB,8BAA+B9X,EAC/B,iBAAkB,UAClB,mBAAoB,UACpB,sBAAuB,UACvB,8BAA+B,UAChC,EAyCH,MAAM4X,GAAiBC,GAKR,QAATA,EAAuB,MACd,WAATA,GAA8B,QAATA,EAAuB,MACnC,QAATA,EAAuB,MACd,YAATA,GAA+B,UAATA,EAAyB,QAC/CA,EAAa,SAAPnW,OAAgBmW,GACnB,UAGHH,GAAqBQ,IAOzBA,EAAWA,EAASnB,eAMPnJ,SAAS,OAAe,MACpB,YAAbsK,EAA+B,UAClB,WAAbA,EAA8B,QACjB,UAAbA,EAA6B,UAChB,YAAbA,EAA+B,UAClB,YAAbA,EAA+B,UAClB,UAAbA,EAA6B,QAC7BA,EAAiB,SAAPxW,OAAgBwW,GACvB,UAGT,IAAIY,GACJ,MAAMtJ,GAAqBA,KAAK,IAAAuJ,EAC9B,OAAwB,QAAxBA,EAAQD,UAAgB,IAAAC,EAAAA,EAAhBD,GAAqBvB,IAAuB,EAGzCjE,GAAY1K,IACvB,IACE,OAAOxF,KAAKwC,MAAMgD,E,CAClB,MAAOnB,GACP,M,GAKEuR,GAAyB,IAAIC,OAAO,kBAAmB,KACvDxF,GAAiBnR,GACd0W,GAAuBE,KAAK5W,GAGxB+S,GAASlB,GAAe,IAAI1H,SAASwB,GAAYoG,WAAWpG,EAASkG,KAE5EhF,GAA0BA,CAACnJ,EAAcmT,KAC7C,GAAiB,kBAANA,IAAmBtE,OAAOuE,UAAUD,GAC7C,MAAM,IAAI3W,EAAY,GAADd,OAAIsE,EAAI,wBAE/B,GAAImT,EAAI,EACN,MAAM,IAAI3W,EAAY,GAADd,OAAIsE,EAAI,gCAE/B,OAAOmT,CAAC,EAGGzV,GAAe+D,GACtBA,aAAehG,MAAcgG,EAC1B,IAAIhG,MAAMgG,GAeN4R,GAAWC,IACc,IAAAC,EAAAC,EAGHC,EAAAC,EAHjC,MAAuB,qBAAZ1B,QACwB,QAAjCuB,EAAkB,QAAlBC,EAAOxB,CAAAA,SAAAA,aAAAA,WAAAA,GAAAA,qBAAAA,EAAAA,qBAAAA,EAAAA,qBAAAA,EAAAA,cAAAA,UAAW,IAAAwB,GAAO,QAAPA,EAAXA,EAAcF,UAAI,IAAAE,OAAA,EAAlBA,EAAoBG,cAAM,IAAAJ,EAAAA,OAAI3Y,EAEnB,qBAAT4W,KACM,QAAfiC,EAAOjC,KAAK8B,WAAG,IAAAG,GAAK,QAALC,EAARD,EAAU9L,WAAG,IAAA+L,GAAO,QAAPA,EAAbA,EAAA5R,KAAA2R,EAAgBH,UAAI,IAAAI,OAAA,EAApBA,EAAsBC,YAD/B,CAGgB,EA6CZ,SAAUhG,GAAWuD,GACzB,IAAKA,EAAK,OAAO,EACjB,IAAK,MAAM0C,KAAM1C,EAAK,OAAO,EAC7B,OAAO,CACT,CAGM,SAAUI,GAAOJ,EAAatK,GAClC,OAAOpB,OAAOyM,UAAU4B,eAAe/R,KAAKoP,EAAKtK,EACnD,CAQA,SAAS+E,GAAgBmI,EAAwBC,GAC/C,IAAK,MAAM1C,KAAK0C,EAAY,CAC1B,IAAKzC,GAAOyC,EAAY1C,GAAI,SAC5B,MAAM2C,EAAW3C,EAAEN,cACnB,IAAKiD,EAAU,SAEf,MAAMC,EAAMF,EAAW1C,GAEX,OAAR4C,SACKH,EAAcE,QACJpZ,IAARqZ,IACTH,EAAcE,GAAYC,E,CAGhC,CAEM,SAAU1M,GAAM2M,GACpB,GAAuB,qBAAZlC,SAAoD,SAAzBA,CAAAA,SAAAA,aAAAA,WAAAA,GAAAA,qBAAAA,EAAAA,qBAAAA,EAAAA,qBAAAA,EAAAA,cAAAA,GAAmB,MAAc,SAAAmC,EAAA5Y,UAAAC,OADhC4Y,EAAW,IAAAnN,MAAAkN,EAAA,EAAAA,EAAA,KAAAE,EAAA,EAAAA,EAAAF,EAAAE,IAAXD,EAAWC,EAAA,GAAA9Y,UAAA8Y,GAEhDvU,QAAQwU,IAAI,gBAAD5Y,OAAiBwY,MAAaE,E,CAE7C,CAKA,MAAMxK,GAAQA,IACL,uCAAuC2K,QAAQ,SAAUC,IAC9D,MAAMC,EAAqB,GAAhBlF,KAAKG,SAAiB,EAEjC,OADgB,MAAN8E,EAAYC,EAAS,EAAJA,EAAW,GAC7BlR,SAAS,GAAG,ICljCnB,MAAO8G,WAAmBsF,EAK9B1U,WAAAA,CAAY2U,EAAmBlR,EAAoBxD,EAA0BI,GAC3EsB,MAAMgT,EAAQlR,EAAUxD,EAAMI,GAE9BwB,KAAKC,KAAO7B,EAAK6B,MAAQ,GACzBD,KAAK4X,OAASxZ,EAAKwZ,MACrB,CAEAzE,iBAAAA,GAAiB,IAAA0E,EACf,OAAgB,QAAhBA,EAAO7X,KAAKC,YAAI,IAAA4X,EAAAA,EAAI,EACtB,CAOAC,cAAAA,GACE,OAAO,IACT,CAEA1E,YAAAA,GACE,OAAO,IACT,EAaI,MAAO2E,WACHlF,EAKR1U,WAAAA,CACE2U,EACAlR,EACAxD,EACAI,GAEAsB,MAAMgT,EAAQlR,EAAUxD,EAAMI,GAE9BwB,KAAKC,KAAO7B,EAAK6B,MAAQ,EAC3B,CAEAkT,iBAAAA,GAAiB,IAAA6E,EACf,OAAgB,QAAhBA,EAAOhY,KAAKC,YAAI,IAAA+X,EAAAA,EAAI,EACtB,CAGAF,cAAAA,GACE,MAAMG,EAAOjY,KAAKoT,eAClB,IAAK6E,EAAM,OAAO,KAClB,GAAI,WAAYA,EAAM,OAAOA,EAAKzE,OAClC,MAAMA,EAAS9K,OAAOyG,YAAY8I,EAAKzY,IAAIiU,cAC3C,OAAK/K,OAAO2L,KAAKb,GAAQ9U,OAClB8U,EADiC,IAE1C,CAEAJ,YAAAA,GAAY,IAAA8E,EACV,MAAMjY,EAAOD,KAAKmT,oBAClB,IAAKlT,EAAKvB,OACR,OAAO,KAGT,MAAMyZ,EAA0B,QAAxBD,EAAGjY,EAAKA,EAAKvB,OAAS,UAAE,IAAAwZ,OAAA,EAArBA,EAAuBC,GAClC,OAAKA,EAIE,CAAE3E,OAAQ,CAAE4E,MAAOD,IAHjB,IAIX,EC5FI,MAAOE,GAGXla,WAAAA,CAAY2U,GACV9S,KAAKsY,QAAUxF,CACjB,ECDI,MAAOyF,WAAoBF,GAa/BG,MAAAA,CACEpa,EACAI,GAA6B,IAAAia,EAE7B,OAAOzY,KAAKsY,QAAQtL,KAAK,eAAgB,CAAE5O,UAASI,EAASwI,OAAmB,QAAbyR,EAAEra,EAAK4I,cAAM,IAAAyR,GAAAA,GAGlF,EA6ReF,KAAAA,GAAW,IC/StB,MAAOA,WAAoBF,GAgB/BG,MAAAA,CACEpa,EACAI,GAA6B,IAAAia,EAE7B,OAAOzY,KAAKsY,QAAQtL,KAAK,oBAAqB,CAAE5O,UAASI,EAASwI,OAAmB,QAAbyR,EAAEra,EAAK4I,cAAM,IAAAyR,GAAAA,GAGvF,EA+4BeF,KAAAA,GAAW,IC36BtB,MAAOG,WAAaL,GAA1Bla,WAAAA,G,oBACE,KAAAwa,YAA0C,IAAIC,GAA2B5Y,KAAKsY,QAChF,GAEA,SAAiBI,GACDA,EAAAH,YAAcK,EA8B7B,CA/BD,CAAiBF,KAAAA,GAAI,KCHf,MAAOG,WAAmBR,GAI9BG,MAAAA,CACEpa,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,cAAe,CAAE5O,UAASI,GACrD,EAyGeqa,KAAAA,GAAU,IC5GrB,MAAOC,WAAcT,GAczBG,MAAAA,CAAOpa,EAAwBI,GAC7B,OAAOwB,KAAKsY,QAAQtL,KAAK,SAAUvD,EAA4B,CAAErL,UAASI,IAC5E,CAKAua,QAAAA,CAASC,EAAgBxa,GACvB,OAAOwB,KAAKsY,QAAQzN,IAAI,UAADjM,OAAWoa,GAAUxa,EAC9C,CAOAya,IAAAA,GAE+B,IAD7BhL,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA8C,CAAC,EAC/CD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAK,CAAC,EAAGhL,GAEhBjO,KAAKsY,QAAQhL,WAAW,SAAU4L,GAAiB,CAAEjL,WAAUzP,GACxE,CAKA2a,GAAAA,CAAIH,EAAgBxa,GAClB,OAAOwB,KAAKsY,QAAQnL,OAAO,UAADvO,OAAWoa,GAAUxa,EACjD,CAKA4a,OAAAA,CAAQJ,EAAgBxa,GACtB,OAAOwB,KAAKsY,QAAQzN,IAAI,UAADjM,OAAWoa,EAAM,YAAY,IAAKxa,EAASmM,kBAAkB,GACtF,CAOA0O,eAAAA,CAAgBL,EAAgBxa,GAC9B,OAAOwB,KAAKsY,QAAQzN,IAAI,UAADjM,OAAWoa,EAAM,YAAY,IAC/Cxa,EACHqB,QAAS,CAAE2M,OAAQ,sBAA8B,OAAPhO,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAEvD,CAKA,uBAAMyZ,CACJnB,GACmG,IAAnG,aAAEoB,EAAe,IAAI,QAAEC,EAAU,MAAc/a,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAkD,CAAC,EAElG,MAAMgb,EAAkB,IAAInS,IAAI,CAAC,YAAa,QAAS,YAEjDlD,EAAQ+N,KAAKC,MACnB,IAAIlI,QAAalK,KAAK+Y,SAASZ,GAE/B,MAAQjO,EAAKtK,SAAW6Z,EAAgBtT,IAAI+D,EAAKtK,SAI/C,SAHM2S,GAAMgH,GAEZrP,QAAalK,KAAK+Y,SAASZ,GACvBhG,KAAKC,MAAQhO,EAAQoV,EACvB,MAAM,IAAIjY,EAA0B,CAClCnC,QAAS,iCAAFR,OAAmCuZ,EAAE,gCAAAvZ,OAA+B4a,EAAO,oBAKxF,OAAOtP,CACT,EAMI,MAAOgP,WAAwB3L,KAsFrC,SAAiBuL,GAIDA,EAAAI,gBAAkBQ,EAGjC,CAPD,CAAiBZ,KAAAA,GAAK,KC3LhB,MAAOa,WAAetB,GAI1BuB,eAAAA,CACExb,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,qBAAsBvD,EAA4B,CAAErL,UAASI,IACxF,CAKAqb,IAAAA,CAAKzb,EAAuBI,GAC1B,OAAOwB,KAAKsY,QAAQtL,KAAK,gBAAiBvD,EAA4B,CAAErL,UAASI,IACnF,CAKAgC,QAAAA,CAASpC,EAA2BI,GAClC,OAAOwB,KAAKsY,QAAQtL,KAAK,sBAAuB,CAAE5O,UAASI,GAC7D,EA+Kemb,KAAAA,GAAM,ICtMjB,MAAOG,WAAezB,GAI1BG,MAAAA,CAAOpa,EAA0BI,GAC/B,OAAOwB,KAAKsY,QAAQtL,KAAK,gBAAiB,CAAE5O,UAASI,EAASmM,kBAAkB,GAClF,EAmCemP,KAAAA,GAAM,ICzCjB,MAAOC,WAAuB1B,GAIlCG,MAAAA,CAAOpa,EAAiCI,GACtC,OAAOwB,KAAKsY,QAAQtL,KAAK,wBAAyBvD,EAA4B,CAAErL,UAASI,IAC3F,EAkDeub,KAAAA,GAAc,ICxDzB,MAAOC,WAAqB3B,GAIhCG,MAAAA,CAAOpa,EAA+BI,GACpC,OAAOwB,KAAKsY,QAAQtL,KAAK,sBAAuBvD,EAA4B,CAAErL,UAASI,IACzF,EA2Cewb,KAAAA,GAAY,ICjDvB,MAAOC,WAAc5B,GAA3Bla,WAAAA,G,oBACE,KAAA+b,eAAmD,IAAIC,GAAiCna,KAAKsY,SAC7F,KAAA8B,aAA6C,IAAIC,GAA6Bra,KAAKsY,SACnF,KAAAgC,OAA2B,IAAIC,GAAiBva,KAAKsY,QACvD,GAEA,SAAiB2B,GACDA,EAAAF,eAAiBI,GAGjBF,EAAAD,aAAeK,GAGfJ,EAAAH,OAASS,EAExB,CATD,CAAiBN,KAAAA,GAAK,KCPhB,MAAOO,WAAoBnC,GAI/BG,MAAAA,CACEpa,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,eAAgB,CAAE5O,UAASI,GACtD,EAoMegc,KAAAA,GAAW,IC5MtB,MAAOC,WAAepC,GAK1BU,QAAAA,CAAS2B,EAAelc,GACtB,OAAOwB,KAAKsY,QAAQzN,IAAI,WAADjM,OAAY8b,GAASlc,EAC9C,CAMAya,IAAAA,CAAKza,GACH,OAAOwB,KAAKsY,QAAQhL,WAAW,UAAWqN,GAAYnc,EACxD,CAMA2a,GAAAA,CAAIuB,EAAelc,GACjB,OAAOwB,KAAKsY,QAAQnL,OAAO,WAADvO,OAAY8b,GAASlc,EACjD,EAMI,MAAOmc,WAAmBpN,KAmChC,SAAiBkN,GAGDA,EAAAE,WAAaC,EAC5B,CAJD,CAAiBH,KAAAA,GAAM,KC/DjB,MAAOI,WAAaxC,GAUxBG,MAAAA,CAAOpa,EAAuBI,GAC5B,OAAOwB,KAAKsY,QAAQtL,KAAK,oBAAqB,CAAE5O,UAASI,GAC3D,CAOAua,QAAAA,CAAS+B,EAAyBtc,GAChC,OAAOwB,KAAKsY,QAAQzN,IAAI,qBAADjM,OAAsBkc,GAAmBtc,EAClE,CAUAya,IAAAA,GAE+B,IAD7BhL,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA6C,CAAC,EAC9CD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAK,CAAC,EAAGhL,GAEhBjO,KAAKsY,QAAQhL,WAAW,oBAAqByN,GAAoB,CAAE9M,WAAUzP,GACtF,CAKAoG,MAAAA,CAAOkW,EAAyBtc,GAC9B,OAAOwB,KAAKsY,QAAQtL,KAAK,qBAADpO,OAAsBkc,EAAe,WAAWtc,EAC1E,CAcAwc,UAAAA,CACEF,GAE6B,IAD7B7M,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAmD,CAAC,EACpDD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKgb,WAAWF,EAAiB,CAAC,EAAG7M,GAEvCjO,KAAKsY,QAAQhL,WAAW,qBAAD1O,OAAsBkc,EAAe,WAAWG,GAAyB,CACrGhN,WACGzP,GAEP,EAGI,MAAOuc,WAA2BhD,IAElC,MAAOkD,WAAgClD,KAiO7C,SAAiB8C,GAGDA,EAAAE,mBAAqBG,GACrBL,EAAAI,wBAA0BC,EAIzC,CARD,CAAiBL,KAAAA,GAAI,KCjTf,MAAOM,WAAmB9C,GAAhCla,WAAAA,G,oBACE,KAAAid,KAAqB,IAAIF,GAAalb,KAAKsY,QAC7C,GAEA,SAAiB6C,GACDA,EAAAN,KAAOK,GAGPC,EAAAJ,mBAAqBG,GACrBC,EAAAF,wBAA0BC,EAIzC,CATD,CAAiBC,KAAAA,GAAU,KCDrB,MAAOrC,WAAcT,GAMzBG,MAAAA,CACE6C,EACAjd,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,eAADpO,OAAgByc,EAAW,UAAU,CAC3Djd,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAkZ,QAAAA,CACEsC,EACArC,EACAxa,GAEA,OAAOwB,KAAKsY,QAAQzN,IAAI,eAADjM,OAAgByc,EAAW,WAAAzc,OAAUoa,GAAU,IACjExa,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAcAoZ,IAAAA,CACEoC,GAE6B,IAD7BpN,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA8C,CAAC,EAC/CD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAKoC,EAAa,CAAC,EAAGpN,GAE7BjO,KAAKsY,QAAQhL,WAAW,eAAD1O,OAAgByc,EAAW,UAAUC,GAAoB,CACrFrN,WACGzP,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAsZ,GAAAA,CACEkC,EACArC,EACAxa,GAEA,OAAOwB,KAAKsY,QAAQnL,OAAO,eAADvO,OAAgByc,EAAW,WAAAzc,OAAUoa,GAAU,IACpExa,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,EAGI,MAAOyb,WAA2BvD,KAiExC,SAAiBe,GAGDA,EAAAwC,mBAAqB5B,EAGpC,CAND,CAAiBZ,KAAAA,GAAK,KCzIhB,MAAOyC,WAAmBlD,GAAhCla,WAAAA,G,oBACE,KAAAqd,MAAwB,IAAI9B,GAAe1Z,KAAKsY,QAqElD,CAhEEE,MAAAA,CAAOpa,EAA6BI,GAClC,OAAOwB,KAAKsY,QAAQtL,KAAK,cAAe,CACtC5O,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAkZ,QAAAA,CAASsC,EAAqB7c,GAC5B,OAAOwB,KAAKsY,QAAQzN,IAAI,eAADjM,OAAgByc,GAAe,IACjD7c,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKA4b,MAAAA,CACEJ,EACAjd,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,eAADpO,OAAgByc,GAAe,CACrDjd,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAUAoZ,IAAAA,GAE+B,IAD7BhL,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAmD,CAAC,EACpDD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAK,CAAC,EAAGhL,GAEhBjO,KAAKsY,QAAQhL,WAAW,cAAeoO,GAAgB,CAC5DzN,WACGzP,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAsZ,GAAAA,CAAIkC,EAAqB7c,GACvB,OAAOwB,KAAKsY,QAAQnL,OAAO,eAADvO,OAAgByc,GAAe,IACpD7c,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,EAGI,MAAO6b,WAAuB3D,ICL9B,SAAU4D,GACdC,GAEA,MAAoC,oBAArBA,EAAW9Y,KAC5B,EDkRA,SAAiByY,GAGDA,EAAAG,eAAiBG,GAIjBN,EAAAzC,MAAQY,GAGR6B,EAAAD,mBAAqB5B,EAGpC,CAbD,CAAiB6B,KAAAA,GAAU,KE5VpB,MAAMO,GACX1c,GAEyB,eAAX,OAAPA,QAAO,IAAPA,OAAO,EAAPA,EAAS2c,MAGLC,GACX5c,GAEyB,cAAX,OAAPA,QAAO,IAAPA,OAAO,EAAPA,EAAS2c,MAGLE,GACX7c,GAEyB,UAAX,OAAPA,QAAO,IAAPA,OAAO,EAAPA,EAAS2c,M,wtBCClB,MAAMG,GAA+B,GAM/B,MAAgBC,GAuBpBhe,WAAAA,G,aApBA,KAAAuD,WAA8B,IAAImO,gBAElCuM,GAAApJ,IAAA,aACAqJ,GAAArJ,IAAA,MAAuC,SACvCsJ,GAAAtJ,IAAA,MAAwD,SAExDuJ,GAAAvJ,IAAA,aACAwJ,GAAAxJ,IAAA,MAAiC,SACjCyJ,GAAAzJ,IAAA,MAAkD,SAElD0J,GAAA1J,IAAA,KAA6E,CAAC,GAEpE,KAAA2J,iBAAqC,GAC/C,KAAAC,SAAyC,GAEzCC,GAAA7J,IAAA,MAAS,GACT8J,GAAA9J,IAAA,MAAW,GACX+J,GAAA/J,IAAA,MAAW,GACXgK,GAAAhK,IAAA,MAA0B,GAkR1BiK,GAAAjK,IAAA,MAAgB7T,IAKd,GAJA8T,GAAAjT,KAAI8c,IAAY,EAAI,KAChB3d,aAAiBR,OAAwB,eAAfQ,EAAM+D,OAClC/D,EAAQ,IAAIkC,GAEVlC,aAAiBkC,EAEnB,OADA4R,GAAAjT,KAAI+c,IAAY,EAAI,KACb/c,KAAKkd,MAAM,QAAS/d,GAE7B,GAAIA,aAAiBO,EACnB,OAAOM,KAAKkd,MAAM,QAAS/d,GAE7B,GAAIA,aAAiBR,MAAO,CAC1B,MAAMwe,EAA2B,IAAIzd,EAAYP,EAAMC,SAGvD,OADA+d,EAAYxc,MAAQxB,EACba,KAAKkd,MAAM,QAASC,E,CAE7B,OAAOnd,KAAKkd,MAAM,QAAS,IAAIxd,EAAY6J,OAAOpK,IAAQ,IAjS1D8T,GAAAjT,KAAIoc,GAAqB,IAAIzS,SAAc,CAACwB,EAASiS,KACnDnK,GAAAjT,KAAIqc,GAA4BlR,EAAO,KACvC8H,GAAAjT,KAAIsc,GAA2Bc,EAAM,QACrC,KAEFnK,GAAAjT,KAAIuc,GAAe,IAAI5S,SAAc,CAACwB,EAASiS,KAC7CnK,GAAAjT,KAAIwc,GAAsBrR,EAAO,KACjC8H,GAAAjT,KAAIyc,GAAqBW,EAAM,QAC/B,KAMF1J,GAAA1T,KAAIoc,GAAA,KAAmBxQ,OAAM,SAC7B8H,GAAA1T,KAAIuc,GAAA,KAAa3Q,OAAM,QACzB,CAEUyR,IAAAA,CAAKC,GAGb/L,YAAW,KACT+L,IAAW/R,MAAK,KACdvL,KAAKud,aACLvd,KAAKkd,MAAM,MAAM,GAChBxJ,GAAA1T,KAAIid,GAAA,KAAc,GACpB,EACL,CAEUO,kBAAAA,CAAmBC,GAA8B,IAAAC,EACzD1d,KAAK2c,iBAAiB7Y,KAAK2Z,GAC3Bzd,KAAKkd,MAAM,iBAAkBO,GAC7B,MAAMre,EAAmC,QAA5Bse,EAAGD,EAAeE,QAAQ,UAAE,IAAAD,OAAA,EAAzBA,EAA2Bte,QAE3C,OADIA,GAASY,KAAK4d,YAAYxe,GACvBqe,CACT,CAEUG,WAAAA,CAAYxe,GAAgD,IAAXye,IAAIpf,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,KAAAA,UAAA,GAK7D,GAJM,YAAaW,IAAUA,EAAQga,QAAU,MAE/CpZ,KAAK4c,SAAS9Y,KAAK1E,GAEfye,EAEF,GADA7d,KAAKkd,MAAM,UAAW9d,IACjB4c,GAAkB5c,IAAY6c,GAAc7c,KAAaA,EAAQga,QAEpEpZ,KAAKkd,MAAM,qBAAsB9d,EAAQga,cACpC,GAAI0C,GAAmB1c,IAAYA,EAAQ0e,cAChD9d,KAAKkd,MAAM,eAAgB9d,EAAQ0e,oBAC9B,GAAIhC,GAAmB1c,IAAYA,EAAQ2e,WAChD,IAAK,MAAMC,KAAa5e,EAAQ2e,WACP,aAAnBC,EAAU5d,MACZJ,KAAKkd,MAAM,eAAgBc,EAAUC,SAK/C,CAEUC,UAAAA,GACJle,KAAKme,QACTzK,GAAA1T,KAAIqc,GAAA,KAAyBrX,KAA7BhF,MACAA,KAAKkd,MAAM,WACb,CAEA,SAAIiB,GACF,OAAOzK,GAAA1T,KAAI6c,GAAA,IACb,CAEA,WAAIuB,GACF,OAAO1K,GAAA1T,KAAI8c,GAAA,IACb,CAEA,WAAIlN,GACF,OAAO8D,GAAA1T,KAAI+c,GAAA,IACb,CAEA7a,KAAAA,GACElC,KAAK0B,WAAWQ,OAClB,CASAmc,EAAAA,CAA+Bxb,EAAcyb,GAI3C,OAFE5K,GAAA1T,KAAI0c,GAAA,KAAY7Z,KAAW6Q,GAAA1T,KAAI0c,GAAA,KAAY7Z,GAAS,KAC5CiB,KAAK,CAAEwa,aACVte,IACT,CASAue,GAAAA,CAAgC1b,EAAcyb,GAC5C,MAAME,EAAY9K,GAAA1T,KAAI0c,GAAA,KAAY7Z,GAClC,IAAK2b,EAAW,OAAOxe,KACvB,MAAMyF,EAAQ+Y,EAAUC,WAAWC,GAAMA,EAAEJ,WAAaA,IAExD,OADI7Y,GAAS,GAAG+Y,EAAUG,OAAOlZ,EAAO,GACjCzF,IACT,CAOA4e,IAAAA,CAAiC/b,EAAcyb,GAI7C,OAFE5K,GAAA1T,KAAI0c,GAAA,KAAY7Z,KAAW6Q,GAAA1T,KAAI0c,GAAA,KAAY7Z,GAAS,KAC5CiB,KAAK,CAAEwa,WAAUM,MAAM,IAC1B5e,IACT,CAaA6e,OAAAA,CACEhc,GAMA,OAAO,IAAI8G,SAAQ,CAACwB,EAASiS,KAC3BnK,GAAAjT,KAAIgd,IAA2B,EAAI,KACrB,UAAVna,GAAmB7C,KAAK4e,KAAK,QAASxB,GAC1Cpd,KAAK4e,KAAK/b,EAAOsI,EAAe,GAEpC,CAEA,UAAMnJ,GACJiR,GAAAjT,KAAIgd,IAA2B,EAAI,WAC7BtJ,GAAA1T,KAAIuc,GAAA,IACZ,CAMA,yBAAMuC,SACE9e,KAAKgC,OACX,MAAM+c,EAAa/e,KAAK2c,iBAAiB3c,KAAK2c,iBAAiBje,OAAS,GACxE,IAAKqgB,EAAY,MAAM,IAAIrf,EAAY,mDACvC,OAAOqf,CACT,CAUA,kBAAMC,GAEJ,aADMhf,KAAKgC,OACJ0R,GAAA1T,KAAIif,GAAA,IAAAC,IAAiBla,KAArBhF,KACT,CAiBA,kBAAMmf,GAEJ,aADMnf,KAAKgC,OACJ0R,GAAA1T,KAAIif,GAAA,IAAAG,IAAiBpa,KAArBhF,KACT,CAoBA,uBAAMqf,GAEJ,aADMrf,KAAKgC,OACJ0R,GAAA1T,KAAIif,GAAA,IAAAK,IAAsBta,KAA1BhF,KACT,CAwBA,6BAAMuf,GAEJ,aADMvf,KAAKgC,OACJ0R,GAAA1T,KAAIif,GAAA,IAAAO,IAA4Bxa,KAAhChF,KACT,CAkBA,gBAAMyf,GAEJ,aADMzf,KAAKgC,OACJ0R,GAAA1T,KAAIif,GAAA,IAAAS,IAAqB1a,KAAzBhF,KACT,CAEA2f,kBAAAA,GACE,MAAO,IAAI3f,KAAK2c,iBAClB,CAuBUO,KAAAA,CAAkCra,GAAqD,QAAAwU,EAAA5Y,UAAAC,OAApC4Y,EAAoC,IAAAnN,MAAAkN,EAAA,EAAAA,EAAA,KAAAE,EAAA,EAAAA,EAAAF,EAAAE,IAApCD,EAAoCC,EAAA,GAAA9Y,UAAA8Y,GAE/F,GAAI7D,GAAA1T,KAAI6c,GAAA,KACN,OAGY,QAAVha,IACFoQ,GAAAjT,KAAI6c,IAAU,EAAI,KAClBnJ,GAAA1T,KAAIwc,GAAA,KAAmBxX,KAAvBhF,OAGF,MAAMwe,EAA0D9K,GAAA1T,KAAI0c,GAAA,KAAY7Z,GAMhF,GALI2b,IACF9K,GAAA1T,KAAI0c,GAAA,KAAY7Z,GAAS2b,EAAUxN,QAAQ0N,IAAOA,EAAEE,OACpDJ,EAAUoB,SAAQte,IAAA,IAAC,SAAEgd,GAAehd,EAAA,OAAKgd,KAAYhH,EAAK,KAG9C,UAAVzU,EAAmB,CACrB,MAAM1D,EAAQmY,EAAK,GAOnB,OANK5D,GAAA1T,KAAIgd,GAAA,MAAsC,OAATwB,QAAS,IAATA,GAAAA,EAAW9f,QAC/CiL,QAAQyT,OAAOje,GAEjBuU,GAAA1T,KAAIsc,GAAA,KAAwBtX,KAA5BhF,KAA6Bb,GAC7BuU,GAAA1T,KAAIyc,GAAA,KAAkBzX,KAAtBhF,KAAuBb,QACvBa,KAAKkd,MAAM,M,CAIb,GAAc,UAAVra,EAAmB,CAGrB,MAAM1D,EAAQmY,EAAK,GACd5D,GAAA1T,KAAIgd,GAAA,MAAsC,OAATwB,QAAS,IAATA,GAAAA,EAAW9f,QAO/CiL,QAAQyT,OAAOje,GAEjBuU,GAAA1T,KAAIsc,GAAA,KAAwBtX,KAA5BhF,KAA6Bb,GAC7BuU,GAAA1T,KAAIyc,GAAA,KAAkBzX,KAAtBhF,KAAuBb,GACvBa,KAAKkd,MAAM,M,CAEf,CAEUK,UAAAA,GACR,MAAMwB,EAAa/e,KAAK2c,iBAAiB3c,KAAK2c,iBAAiBje,OAAS,GACpEqgB,GAAY/e,KAAKkd,MAAM,sBAAuB6B,GAClD,MAAMI,EAAezL,GAAA1T,KAAIif,GAAA,IAAAG,IAAiBpa,KAArBhF,MACjBmf,GAAcnf,KAAKkd,MAAM,eAAgBiC,GAC7C,MAAMH,EAAetL,GAAA1T,KAAIif,GAAA,IAAAC,IAAiBla,KAArBhF,MACjBgf,GAAchf,KAAKkd,MAAM,eAAgB8B,GAE7C,MAAMK,EAAoB3L,GAAA1T,KAAIif,GAAA,IAAAK,IAAsBta,KAA1BhF,MACtBqf,GAAmBrf,KAAKkd,MAAM,oBAAqBmC,GAEvD,MAAME,EAA0B7L,GAAA1T,KAAIif,GAAA,IAAAO,IAA4Bxa,KAAhChF,MACD,MAA3Buf,GAAiCvf,KAAKkd,MAAM,0BAA2BqC,GAEvEvf,KAAK2c,iBAAiBkD,MAAMnI,GAAMA,EAAEoI,SACtC9f,KAAKkd,MAAM,aAAcxJ,GAAA1T,KAAIif,GAAA,IAAAS,IAAqB1a,KAAzBhF,MAE7B,CAUU,2BAAM+f,CACdpH,EACAnF,EACAhV,GAEA,MAAMkQ,EAAgB,OAAPlQ,QAAO,IAAPA,OAAO,EAAPA,EAASkQ,OACpBA,IACEA,EAAOkB,SAAS5P,KAAK0B,WAAWQ,QACpCwM,EAAO4C,iBAAiB,SAAS,IAAMtR,KAAK0B,WAAWQ,WAEzDwR,GAAA1T,KAAIif,GAAA,IAAAe,IAAgBhb,KAApBhF,KAAqBwT,GAErB,MAAMiK,QAAuB9E,EAAYH,OACvC,IAAKhF,EAAQxM,QAAQ,GACrB,IAAKxI,EAASkQ,OAAQ1O,KAAK0B,WAAWgN,SAGxC,OADA1O,KAAKke,aACEle,KAAKwd,mBAAmBC,EACjC,CAEU,wBAAMwC,CACdtH,EACAnF,EACAhV,GAEA,IAAK,MAAMY,KAAWoU,EAAOoJ,SAC3B5c,KAAK4d,YAAYxe,GAAS,GAE5B,aAAaY,KAAK+f,sBAAsBpH,EAAanF,EAAQhV,EAC/D,CAEU,mBAAM0hB,CACdvH,EACAnF,EAGAhV,GAEA,MAAMud,EAAO,YACP,cAAE+B,EAAgB,OAAM,OAAE9W,KAAWmZ,GAAe3M,EACpD4M,EAAgD,kBAAlBtC,IAA2C,OAAbA,QAAa,IAAbA,OAAa,EAAbA,EAAe5a,OAC3E,mBAAEmd,EAAqBnE,IAAiC1d,GAAW,CAAC,EAEpE8hB,EAAyD,CAAC,EAChE,IAAK,MAAMC,KAAK/M,EAAOgN,UACrBF,EAAgBC,EAAErd,MAAQqd,EAAEtC,SAAS/a,MAAQqd,EAG/C,MAAMC,EAAmDhN,EAAOgN,UAAU5X,KACvE2X,IAAC,CACArd,KAAMqd,EAAErd,MAAQqd,EAAEtC,SAAS/a,KAC3Bud,WAAYF,EAAEE,WACdC,YAAaH,EAAEG,gBAInB,IAAK,MAAMthB,KAAWoU,EAAOoJ,SAC3B5c,KAAK4d,YAAYxe,GAAS,GAG5B,IAAK,IAAIuhB,EAAI,EAAGA,EAAIN,IAAsBM,EAAG,KAAAC,EAC3C,MAUMxhB,EAAmC,QAA5BwhB,SAVgC5gB,KAAK+f,sBAChDpH,EACA,IACKwH,EACHrC,gBACA0C,YACA5D,SAAU,IAAI5c,KAAK4c,WAErBpe,IAE6Bmf,QAAQ,UAAE,IAAAiD,OAAA,EAAzBA,EAA2BxhB,QAC3C,IAAKA,EACH,MAAM,IAAIM,EAAY,8CAExB,IAAKN,EAAQ0e,cAAe,OAC5B,MAAM,KAAE5a,EAAMzE,UAAW6Y,GAASlY,EAAQ0e,cACpClC,EAAK0E,EAAgBpd,GAC3B,IAAK0Y,EAAI,CACP,MAAMxC,EAAU,0BAAHxa,OAA6B0B,KAAKC,UAAU2C,GAAK,6BAAAtE,OAA4B4hB,EACvF5X,KAAK2X,GAAMjgB,KAAKC,UAAUggB,EAAErd,QAC5BkC,KAAK,MAAK,sBAEbpF,KAAK4d,YAAY,CAAE7B,OAAM7Y,OAAMkW,YAC/B,Q,CACK,GAAIgH,GAAwBA,IAAyBld,EAAM,CAChE,MAAMkW,EAAU,0BAAHxa,OAA6B0B,KAAKC,UAAU2C,GAAK,MAAAtE,OAAK0B,KAAKC,UACtE6f,GACD,gCAEDpgB,KAAK4d,YAAY,CAAE7B,OAAM7Y,OAAMkW,YAC/B,Q,CAGF,IAAIyH,EACJ,IACEA,EAASlF,GAA4BC,SAAYA,EAAG9Y,MAAMwU,GAAQA,C,CAClE,MAAOnY,GACPa,KAAK4d,YAAY,CACf7B,OACA7Y,OACAkW,QAASja,aAAiBR,MAAQQ,EAAMC,QAAUmK,OAAOpK,KAE3D,Q,CAIF,MAAM2hB,QAAmBlF,EAAGqC,SAAS4C,EAAQ7gB,MACvCoZ,EAAU1F,GAAA1T,KAAIif,GAAA,IAAA8B,IAA6B/b,KAAjChF,KAAkC8gB,GAIlD,GAFA9gB,KAAK4d,YAAY,CAAE7B,OAAM7Y,OAAMkW,YAE3BgH,EAAsB,M,CAE9B,CAEU,eAAMY,CACdrI,EACAnF,EAGAhV,GAAuB,IAAAyiB,EAEvB,MAAMlF,EAAO,QACP,YAAEmF,EAAc,OAAM,OAAEla,KAAWmZ,GAAe3M,EAClD4M,EAA8C,kBAAhBc,IAAuC,OAAXA,QAAW,IAAXA,GAAqB,QAAVD,EAAXC,EAAajD,gBAAQ,IAAAgD,OAAA,EAArBA,EAAuB/d,OACjF,mBAAEmd,EAAqBnE,IAAiC1d,GAAW,CAAC,EAEpE8hB,EAAyD,CAAC,EAChE,IAAK,MAAMC,KAAK/M,EAAO2N,MACN,aAAXZ,EAAEngB,OACJkgB,EAAgBC,EAAEtC,SAAS/a,MAAQqd,EAAEtC,SAASA,SAAS/a,MAAQqd,EAAEtC,UAIrE,MAAMkD,EACJ,UAAW3N,EACTA,EAAO2N,MAAMvY,KAAKwY,GACL,aAAXA,EAAEhhB,KACA,CACEA,KAAM,WACN6d,SAAU,CACR/a,KAAMke,EAAEnD,SAAS/a,MAAQke,EAAEnD,SAASA,SAAS/a,KAC7Cud,WAAYW,EAAEnD,SAASwC,WACvBC,YAAaU,EAAEnD,SAASyC,cAG3BU,SAEJtjB,EAEL,IAAK,MAAMsB,KAAWoU,EAAOoJ,SAC3B5c,KAAK4d,YAAYxe,GAAS,GAG5B,IAAK,IAAIuhB,EAAI,EAAGA,EAAIN,IAAsBM,EAAG,KAAAU,EAC3C,MAUMjiB,EAAmC,QAA5BiiB,SAVgCrhB,KAAK+f,sBAChDpH,EACA,IACKwH,EACHe,cACAC,QACAvE,SAAU,IAAI5c,KAAK4c,WAErBpe,IAE6Bmf,QAAQ,UAAE,IAAA0D,OAAA,EAAzBA,EAA2BjiB,QAC3C,IAAKA,EACH,MAAM,IAAIM,EAAY,8CAExB,IAAKN,EAAQ2e,WACX,OAGF,IAAK,MAAMC,KAAa5e,EAAQ2e,WAAY,CAC1C,GAAuB,aAAnBC,EAAU5d,KAAqB,SACnC,MAAMkhB,EAAetD,EAAU7F,IACzB,KAAEjV,EAAMzE,UAAW6Y,GAAS0G,EAAUC,SACtCrC,EAAK0E,EAAgBpd,GAE3B,IAAK0Y,EAAI,CACP,MAAMxC,EAAU,sBAAHxa,OAAyB0B,KAAKC,UAAU2C,GAAK,6BAAAtE,OAA4BuiB,EACnFvY,KAAK2X,GAAMjgB,KAAKC,UAAUggB,EAAEtC,SAAS/a,QACrCkC,KAAK,MAAK,sBAEbpF,KAAK4d,YAAY,CAAE7B,OAAMuF,eAAclI,YACvC,Q,CACK,GAAIgH,GAAwBA,IAAyBld,EAAM,CAChE,MAAMkW,EAAU,sBAAHxa,OAAyB0B,KAAKC,UAAU2C,GAAK,MAAAtE,OAAK0B,KAAKC,UAClE6f,GACD,gCAEDpgB,KAAK4d,YAAY,CAAE7B,OAAMuF,eAAclI,YACvC,Q,CAGF,IAAIyH,EACJ,IACEA,EAASlF,GAA4BC,SAAYA,EAAG9Y,MAAMwU,GAAQA,C,CAClE,MAAOnY,GACP,MAAMia,EAAUja,aAAiBR,MAAQQ,EAAMC,QAAUmK,OAAOpK,GAChEa,KAAK4d,YAAY,CAAE7B,OAAMuF,eAAclI,YACvC,Q,CAIF,MAAM0H,QAAmBlF,EAAGqC,SAAS4C,EAAQ7gB,MACvCoZ,EAAU1F,GAAA1T,KAAIif,GAAA,IAAA8B,IAA6B/b,KAAjChF,KAAkC8gB,GAGlD,GAFA9gB,KAAK4d,YAAY,CAAE7B,OAAMuF,eAAclI,YAEnCgH,EACF,M,EAMR,E,uNAnaE,OAAsC,QAAtCmB,EAAO7N,GAAA1T,KAAIif,GAAA,IAAAG,IAAiBpa,KAArBhF,MAAwBoZ,eAAO,IAAAmI,EAAAA,EAAI,IAC5C,EAACnC,GAAA,WAYC,IAAIuB,EAAI3gB,KAAK4c,SAASle,OACtB,KAAOiiB,KAAM,GAAG,CACd,MAAMvhB,EAAUY,KAAK4c,SAAS+D,GACG,IAAAa,EAAjC,GAAI1F,GAAmB1c,GACrB,MAAO,IAAKA,EAASga,QAAwB,QAAjBoI,EAAEpiB,EAAQga,eAAO,IAAAoI,EAAAA,EAAI,K,CAGrD,MAAM,IAAI9hB,EAAY,6EACxB,EAAC4f,GAAA,WAYC,IAAK,IAAIqB,EAAI3gB,KAAK4c,SAASle,OAAS,EAAGiiB,GAAK,EAAGA,IAAK,KAAAc,EAClD,MAAMriB,EAAUY,KAAK4c,SAAS+D,GAC9B,GAAI7E,GAAmB1c,IAAmB,OAAPA,QAAO,IAAPA,GAAAA,EAAS0e,cAC1C,OAAO1e,EAAQ0e,cAE+C,IAAA4D,EAAhE,GAAI5F,GAAmB1c,IAAmB,OAAPA,QAAO,IAAPA,GAAmB,QAAZqiB,EAAPriB,EAAS2e,kBAAU,IAAA0D,GAAnBA,EAAqB/iB,OACtD,OAAgC,QAAhCgjB,EAAOtiB,EAAQ2e,WAAW4D,IAAI,UAAE,IAAAD,OAAA,EAAzBA,EAA2BzD,Q,CAKxC,EAACuB,GAAA,WAYC,IAAK,IAAImB,EAAI3gB,KAAK4c,SAASle,OAAS,EAAGiiB,GAAK,EAAGA,IAAK,CAClD,MAAMvhB,EAAUY,KAAK4c,SAAS+D,GAC9B,GAAI3E,GAAkB5c,IAA+B,MAAnBA,EAAQga,QACxC,OAAOha,EAAQga,QAEjB,GACE6C,GAAc7c,IACK,MAAnBA,EAAQga,SACRpZ,KAAK4c,SAASiD,MACXvW,IAAC,IAAAsY,EAAA,MACW,cAAXtY,EAAEyS,OACU,QADU6F,EACtBtY,EAAEyU,kBAAU,IAAA6D,OAAA,EAAZA,EAAc/B,MAAMgC,GAAiB,aAAXA,EAAEzhB,MAAuByhB,EAAE1J,KAAO/Y,EAAQkiB,eAAa,IAGrF,OAAOliB,EAAQga,O,CAKrB,EAACsG,GAAA,WAQC,MAAMoC,EAAyB,CAC7BC,kBAAmB,EACnBC,cAAe,EACfC,aAAc,GAEhB,IAAK,MAAM,MAAEnC,KAAW9f,KAAK2c,iBACvBmD,IACFgC,EAAMC,mBAAqBjC,EAAMiC,kBACjCD,EAAME,eAAiBlC,EAAMkC,cAC7BF,EAAMG,cAAgBnC,EAAMmC,cAGhC,OAAOH,CACT,EAAC9B,GAAA,SAkGexM,GACd,GAAgB,MAAZA,EAAO6C,GAAa7C,EAAO6C,EAAI,EACjC,MAAM,IAAI3W,EACR,+HAGN,EAACqhB,GAAA,SA6N4BD,GAC3B,MACwB,kBAAfA,EAA0BA,OAChBhjB,IAAfgjB,EAA2B,YAC3BxgB,KAAKC,UAAUugB,EAErB,ECrmBI,MAAOoB,WAA6B/F,GAExC,mBAAOgG,CACLxJ,EACAnF,EACAhV,GAEA,MAAM4jB,EAAS,IAAIF,GACb3iB,EAAO,IACRf,EACHqB,QAAS,IAAY,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,QAAS,4BAA6B,iBAG/D,OADAuiB,EAAO/E,MAAK,IAAM+E,EAAOlC,cAAcvH,EAAanF,EAAQjU,KACrD6iB,CACT,CAEA,eAAOC,CACL1J,EACAnF,EACAhV,GAEA,MAAM4jB,EAAS,IAAIF,GACb3iB,EAAO,IACRf,EACHqB,QAAS,IAAY,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,QAAS,4BAA6B,aAG/D,OADAuiB,EAAO/E,MAAK,IAAM+E,EAAOpB,UAAUrI,EAAanF,EAAQjU,KACjD6iB,CACT,CAESxE,WAAAA,CAAYxe,GACnBU,MAAM8d,YAAYxe,GACd0c,GAAmB1c,IAAYA,EAAQga,SACzCpZ,KAAKkd,MAAM,UAAW9d,EAAQga,QAElC,E,irBCzCI,MAAOkJ,WACHnG,GADVhe,WAAAA,G,iCAIEokB,GAAAvP,IAAA,YA+NF,CA7NE,iCAAIwP,GACF,OAAO9O,GAAA1T,KAAIuiB,GAAA,IACb,CASA,yBAAOpf,CAAmB6D,GACxB,MAAMob,EAAS,IAAIE,GAEnB,OADAF,EAAO/E,MAAK,IAAM+E,EAAOK,oBAAoBzb,KACtCob,CACT,CAEA,2BAAOM,CACL/J,EACAnF,EACAhV,GAEA,MAAM4jB,EAAS,IAAIE,GAQnB,OAPAF,EAAO/E,MAAK,IACV+E,EAAOnC,mBACLtH,EACA,IAAKnF,EAAQxM,QAAQ,GACrB,IAAKxI,EAASqB,QAAS,IAAY,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,QAAS,4BAA6B,cAGxEuiB,CACT,CA4BmB,2BAAMrC,CACvBpH,EACAnF,EACAhV,GAA6B,IAAAmkB,EAE7B,MAAMjU,EAAgB,OAAPlQ,QAAO,IAAPA,OAAO,EAAPA,EAASkQ,OACpBA,IACEA,EAAOkB,SAAS5P,KAAK0B,WAAWQ,QACpCwM,EAAO4C,iBAAiB,SAAS,IAAMtR,KAAK0B,WAAWQ,WAEzDwR,GAAA1T,KAAI4iB,GAAA,IAAAC,IAAc7d,KAAlBhF,MACA,MAAMgH,QAAe2R,EAAYH,OAC/B,IAAKhF,EAAQxM,QAAQ,GACrB,IAAKxI,EAASkQ,OAAQ1O,KAAK0B,WAAWgN,SAExC1O,KAAKke,aACL,UAAW,MAAM3b,KAASyE,EACxB0M,GAAA1T,KAAI4iB,GAAA,IAAAE,IAAU9d,KAAdhF,KAAeuC,GAEjB,GAA4B,QAA5BogB,EAAI3b,EAAOtF,WAAWgN,cAAM,IAAAiU,GAAxBA,EAA0B/S,QAC5B,MAAM,IAAIvO,EAEZ,OAAOrB,KAAKwd,mBAAmB9J,GAAA1T,KAAI4iB,GAAA,IAAAG,IAAY/d,KAAhBhF,MACjC,CAEU,yBAAMyiB,CACdrf,EACA5E,GAA6B,IAAAwkB,EAE7B,MAAMtU,EAAgB,OAAPlQ,QAAO,IAAPA,OAAO,EAAPA,EAASkQ,OACpBA,IACEA,EAAOkB,SAAS5P,KAAK0B,WAAWQ,QACpCwM,EAAO4C,iBAAiB,SAAS,IAAMtR,KAAK0B,WAAWQ,WAEzDwR,GAAA1T,KAAI4iB,GAAA,IAAAC,IAAc7d,KAAlBhF,MACAA,KAAKke,aACL,MAAMlX,EAASxF,EAAO2B,mBAAwCC,EAAgBpD,KAAK0B,YACnF,IAAIuhB,EACJ,UAAW,MAAM1gB,KAASyE,EACpBic,GAAUA,IAAW1gB,EAAM4V,IAE7BnY,KAAKwd,mBAAmB9J,GAAA1T,KAAI4iB,GAAA,IAAAG,IAAY/d,KAAhBhF,OAG1B0T,GAAA1T,KAAI4iB,GAAA,IAAAE,IAAU9d,KAAdhF,KAAeuC,GACf0gB,EAAS1gB,EAAM4V,GAEjB,GAA4B,QAA5B6K,EAAIhc,EAAOtF,WAAWgN,cAAM,IAAAsU,GAAxBA,EAA0BpT,QAC5B,MAAM,IAAIvO,EAEZ,OAAOrB,KAAKwd,mBAAmB9J,GAAA1T,KAAI4iB,GAAA,IAAAG,IAAY/d,KAAhBhF,MACjC,CAqEA,EAAAuiB,GAAA,IAAA1O,QAAA+O,GAAA,IAAAM,QAAAL,GAAA,WAjJM7iB,KAAKme,OACTlL,GAAAjT,KAAIuiB,QAAkCzkB,EAAS,IACjD,EAACglB,GAAA,SACSvgB,GAA0B,IAAA4gB,EAAAC,EAClC,GAAIpjB,KAAKme,MAAO,OAChB,MAAMY,EAAarL,GAAA1T,KAAI4iB,GAAA,IAAAS,IAA0Bre,KAA9BhF,KAA+BuC,GAClDvC,KAAKkd,MAAM,QAAS3a,EAAOwc,GAC3B,MAAMuE,EAAwB,QAAnBH,EAAG5gB,EAAMob,QAAQ,UAAE,IAAAwF,GAAO,QAAPA,EAAhBA,EAAkBG,aAAK,IAAAH,OAAA,EAAvBA,EAAyB/J,QACjCmK,EAAgC,QAAxBH,EAAGrE,EAAWpB,QAAQ,UAAE,IAAAyF,OAAA,EAArBA,EAAuBhkB,QAC3B,MAATkkB,GAAoC,eAAX,OAARC,QAAQ,IAARA,OAAQ,EAARA,EAAUxH,OAAgC,OAARwH,QAAQ,IAARA,GAAAA,EAAUnK,SAC/DpZ,KAAKkd,MAAM,UAAWoG,EAAOC,EAASnK,QAE1C,EAAC2J,GAAA,WAEC,GAAI/iB,KAAKme,MACP,MAAM,IAAIze,EAAY,2CAExB,MAAM6jB,EAAW7P,GAAA1T,KAAIuiB,GAAA,KACrB,IAAKgB,EACH,MAAM,IAAI7jB,EAAY,4CAGxB,OADAuT,GAAAjT,KAAIuiB,QAAkCzkB,EAAS,KAwKnD,SAAgCylB,GAC9B,MAAM,GAAEpL,EAAE,QAAEwF,EAAO,QAAE6F,EAAO,MAAE9I,EAAK,mBAAE+I,KAAuBC,GAASH,EACrE,MAAO,IACFG,EACHvL,KACAwF,QAASA,EAAQ/U,KACftH,IAAsF,IAArF,QAAElC,EAAO,cAAEukB,EAAa,MAAEle,EAAK,SAAEme,KAAaC,GAAYviB,EACzD,IAAKqiB,EAAe,MAAM,IAAIjkB,EAAY,oCAADd,OAAqC6G,IAC9E,MAAM,QAAE2T,EAAU,KAAI,cAAE0E,EAAa,WAAEC,KAAe+F,GAAgB1kB,EAChE2c,EAAO3c,EAAQ2c,KACrB,IAAKA,EAAM,MAAM,IAAIrc,EAAY,2BAADd,OAA4B6G,IAC5D,GAAIqY,EAAe,CACjB,MAAQrf,UAAW6Y,EAAI,KAAEpU,GAAS4a,EAClC,GAAY,MAARxG,EAAc,MAAM,IAAI5X,EAAY,8CAADd,OAA+C6G,IACtF,IAAKvC,EAAM,MAAM,IAAIxD,EAAY,yCAADd,OAA0C6G,IAC1E,MAAO,IACFoe,EACHzkB,QAAS,CAAEga,UAAS0E,cAAe,CAAErf,UAAW6Y,EAAMpU,QAAQ6Y,QAC9D4H,gBACAle,QACAme,W,CAGJ,OAAI7F,EACK,IACF8F,EACHpe,QACAke,gBACAC,WACAxkB,QAAS,IACJ0kB,EACH/H,OACA3C,UACA2E,WAAYA,EAAWnV,KAAI,CAACoV,EAAW2C,KACrC,MAAQ1C,SAAUrC,EAAE,KAAExb,EAAI,GAAE+X,KAAO4L,GAAa/F,GACxCvf,UAAW6Y,EAAI,KAAEpU,KAAS8gB,GAAWpI,GAAM,CAAC,EACpD,GAAU,MAANzD,EACF,MAAM,IAAIzY,EAAY,mBAADd,OAAoB6G,EAAK,iBAAA7G,OAAgB+hB,EAAC,UAAA/hB,OAAS2G,GAAIge,KAC9E,GAAY,MAARnjB,EACF,MAAM,IAAIV,EAAY,mBAADd,OAAoB6G,EAAK,iBAAA7G,OAAgB+hB,EAAC,YAAA/hB,OAAW2G,GAAIge,KAChF,GAAY,MAARrgB,EACF,MAAM,IAAIxD,EAAY,mBAADd,OACA6G,EAAK,iBAAA7G,OAAgB+hB,EAAC,qBAAA/hB,OAAoB2G,GAAIge,KAErE,GAAY,MAARjM,EACF,MAAM,IAAI5X,EAAY,mBAADd,OACA6G,EAAK,iBAAA7G,OAAgB+hB,EAAC,0BAAA/hB,OAAyB2G,GAAIge,KAG1E,MAAO,IAAKQ,EAAU5L,KAAI/X,OAAM6d,SAAU,IAAK+F,EAAQ9gB,OAAMzE,UAAW6Y,GAAQ,MAKjF,IACFuM,EACHzkB,QAAS,IAAK0kB,EAAa1K,UAAS2C,QACpC4H,gBACAle,QACAme,WACD,IAGLJ,UACA9I,QACA9C,OAAQ,qBACJ6L,EAAqB,CAAEA,sBAAuB,CAAC,EAEvD,CA3OWQ,CAAuBV,EAChC,EAACF,GAAA,SAuDyB9gB,G,UACxB,IAAIghB,EAAW7P,GAAA1T,KAAIuiB,GAAA,KACnB,MAAM,QAAE5E,KAAY+F,GAASnhB,EACxBghB,EAMH7a,OAAOwb,OAAOX,EAAUG,GALxBH,EAAWtQ,GAAAjT,KAAIuiB,GAAkC,IAC5CmB,EACH/F,QAAS,IACV,KAKH,IAAK,MAAM,MAAE2F,EAAK,cAAEK,EAAa,MAAEle,EAAK,SAAEme,EAAW,QAASO,KAAW5hB,EAAMob,QAAS,CACtF,IAAIyG,EAASb,EAAS5F,QAAQlY,GAK9B,GAJK2e,IACHA,EAASb,EAAS5F,QAAQlY,GAAS,CAAEke,gBAAele,QAAOrG,QAAS,CAAC,EAAGwkB,cAAaO,IAGnFP,EACF,GAAKQ,EAAOR,SAEL,CACL,MAAM,QAAExK,KAAYsK,GAASE,EAEhB,IAAAS,EAAb,GADA3b,OAAOwb,OAAOE,EAAOR,SAAUF,GAC3BtK,EACqB,QAAvBiL,GAAAC,EAAAF,EAAOR,UAASxK,eAAO,IAAAiL,IAAAC,EAAPlL,QAAY,IAC5BgL,EAAOR,SAASxK,QAAQtV,QAAQsV,E,MANlCgL,EAAOR,SAAWlb,OAAOwb,OAAO,CAAC,EAAGN,GAcxC,GAHID,IAAeS,EAAOT,cAAgBA,GAC1Cjb,OAAOwb,OAAOE,EAAQD,IAEjBb,EAAO,SACZ,MAAM,QAAElK,EAAO,cAAE0E,EAAa,KAAE/B,EAAI,WAAEgC,KAAe2F,GAASJ,EAU7B,IAAAiB,EALjC,GAJA7b,OAAOwb,OAAOE,EAAOhlB,QAASskB,GAE1BtK,IAASgL,EAAOhlB,QAAQga,SAAWgL,EAAOhlB,QAAQga,SAAW,IAAMA,GACnE2C,IAAMqI,EAAOhlB,QAAQ2c,KAAOA,GAC5B+B,EACF,GAAKsG,EAAOhlB,QAAQ0e,eAIlB,GADIA,EAAc5a,OAAMkhB,EAAOhlB,QAAQ0e,cAAc5a,KAAO4a,EAAc5a,MACtE4a,EAAcrf,UACsB,QAAtC8lB,GAAAC,EAAAJ,EAAOhlB,QAAQ0e,eAAcrf,iBAAS,IAAA8lB,IAAAC,EAAT/lB,UAAc,IAC3C2lB,EAAOhlB,QAAQ0e,cAAcrf,WAAaqf,EAAcrf,eAL1D2lB,EAAOhlB,QAAQ0e,cAAgBA,EASnC,GAAIC,EAAY,CACTqG,EAAOhlB,QAAQ2e,aAAYqG,EAAOhlB,QAAQ2e,WAAa,IAC5D,IAAK,MAAM,MAAEtY,EAAK,GAAE0S,EAAE,KAAE/X,EAAM6d,SAAUrC,KAAO8H,KAAU3F,EAAY,KAAA0G,EAAAC,EACnE,MAAM1G,EAA4C,QAAnCyG,GAAGE,EAACP,EAAOhlB,QAAQ2e,YAAWtY,UAAK,IAAAgf,EAAAA,EAAAE,EAALlf,GAAW,CAAC,EACzDiD,OAAOwb,OAAOlG,EAAW0F,GACrBvL,IAAI6F,EAAU7F,GAAKA,GACnB/X,IAAM4d,EAAU5d,KAAOA,GACvBwb,IAAsB,QAAlB8I,EAAA1G,EAAUC,gBAAQ,IAAAyG,IAAlB1G,EAAUC,SAAa,CAAExf,UAAW,MACtC,OAAFmd,QAAE,IAAFA,GAAAA,EAAI1Y,OAAM8a,EAAUC,SAAU/a,KAAO0Y,EAAG1Y,MACtC,OAAF0Y,QAAE,IAAFA,GAAAA,EAAInd,YAAWuf,EAAUC,SAAUxf,WAAamd,EAAGnd,U,GAI7D,OAAO8kB,CACT,EAECllB,OAAOiF,kBACN,MAAMshB,EAAmC,GACnCC,EAAkE,GACxE,IAAI7iB,GAAO,EAmBX,OAjBAhC,KAAKqe,GAAG,SAAU9b,IAChB,MAAM0E,EAAS4d,EAAU9gB,QACrBkD,EACFA,EAAO1E,GAEPqiB,EAAU9gB,KAAKvB,E,IAInBvC,KAAKqe,GAAG,OAAO,KACbrc,GAAO,EACP,IAAK,MAAMiF,KAAU4d,EACnB5d,OAAOnJ,GAET+mB,EAAUnmB,OAAS,CAAC,IAGf,CACLkF,KAAMvE,UACJ,IAAKulB,EAAUlmB,OACb,OAAIsD,EACK,CAAEvC,WAAO3B,EAAWkE,MAAM,GAE5B,IAAI2H,SAA0CwB,GAAY0Z,EAAU/gB,KAAKqH,KAAUI,MACvFhJ,GAAWA,EAAQ,CAAE9C,MAAO8C,EAAOP,MAAM,GAAU,CAAEvC,WAAO3B,EAAWkE,MAAM,KAIlF,MAAO,CAAEvC,MADKmlB,EAAU7gB,QACD/B,MAAM,EAAO,EAG1C,CAEAgC,gBAAAA,GAEE,OADe,IAAIxC,EAAOxB,KAAK3B,OAAOiF,eAAewhB,KAAK9kB,MAAOA,KAAK0B,YACxDsC,kBAChB,EAyEF,SAASuB,GAAI+D,GACX,OAAOhJ,KAAKC,UAAU+I,EACxB,CCzSM,MAAOyb,WACHzC,GAGR,yBAAgBnf,CAAmB6D,GACjC,MAAMob,EAAS,IAAI2C,GAEnB,OADA3C,EAAO/E,MAAK,IAAM+E,EAAOK,oBAAoBzb,KACtCob,CACT,CAGA,mBAAOD,CACLxJ,EACAnF,EACAhV,GAEA,MAAM4jB,EAAS,IAAI2C,GACbxlB,EAAO,IACRf,EACHqB,QAAS,IAAY,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,QAAS,4BAA6B,iBAG/D,OADAuiB,EAAO/E,MAAK,IAAM+E,EAAOlC,cAAcvH,EAAanF,EAAQjU,KACrD6iB,CACT,CAEA,eAAOC,CACL1J,EACAnF,EACAhV,GAEA,MAAM4jB,EAAS,IAAI2C,GACbxlB,EAAO,IACRf,EACHqB,QAAS,IAAY,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,QAAS,4BAA6B,aAG/D,OADAuiB,EAAO/E,MAAK,IAAM+E,EAAOpB,UAAUrI,EAAanF,EAAQjU,KACjD6iB,CACT,ECpCI,MAAO7J,WAAoBF,GAY/B8J,YAAAA,CACE/jB,EAGAI,GAEA,OAAIJ,EAAK4I,OACA+d,GAA8B5C,aACnCniB,KAAKsY,QAAQ0M,KAAKrM,YAClBva,EACAI,GAGG0jB,GAAqBC,aAC1BniB,KAAKsY,QAAQ0M,KAAKrM,YAClBva,EACAI,EAEJ,CAmBA6jB,QAAAA,CACEjkB,EAGAI,GAEA,OAAIJ,EAAK4I,OACA+d,GAA8B1C,SACnCriB,KAAKsY,QAAQ0M,KAAKrM,YAClBva,EACAI,GAGG0jB,GAAqBG,SAC1BriB,KAAKsY,QAAQ0M,KAAKrM,YAClBva,EACAI,EAEJ,CAKAwI,MAAAA,CAAO5I,EAAkCI,GACvC,OAAO8jB,GAAqBI,qBAAqB1iB,KAAKsY,QAAQ0M,KAAKrM,YAAava,EAAMI,EACxF,ECnGI,MAAOka,WAAaL,GAA1Bla,WAAAA,G,oBACE,KAAAwa,YAA0C,IAAIC,GAA2B5Y,KAAKsY,QAChF,GAEA,SAAiBI,GACDA,EAAAH,YAAcK,EAC7B,CAFD,CAAiBF,KAAAA,GAAI,KCDf,MAAOI,WAAcT,GAIzBU,QAAAA,CACEkM,EACAC,EACAlM,EACAxa,GAEA,OAAOwB,KAAKsY,QAAQzN,IAAI,YAADjM,OAAaqmB,EAAQ,cAAArmB,OAAasmB,EAAS,WAAAtmB,OAAUoa,GAAU,IACjFxa,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAgBAoZ,IAAAA,CACEgM,EACAC,GAE6B,IAD7BjX,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA8C,CAAC,EAC/CD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAKgM,EAAUC,EAAW,CAAC,EAAGjX,GAErCjO,KAAKsY,QAAQhL,WAAW,YAAD1O,OAAaqmB,EAAQ,cAAArmB,OAAasmB,EAAS,UAAUC,GAAkB,CACnGlX,WACGzP,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,EAGI,MAAOslB,WAAyBpN,KA6CtC,SAAiBe,GAEDA,EAAAqM,iBAAmBzL,EAElC,CAJD,CAAiBZ,KAAAA,GAAK,KC3FhB,MAAOsM,WAAiB/M,GAA9Bla,WAAAA,G,oBACE,KAAAqd,MAAwB,IAAI9B,GAAe1Z,KAAKsY,QAsElD,CAjEEE,MAAAA,CACEyM,EACA7mB,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,YAADpO,OAAaqmB,EAAQ,aAAa,CACxD7mB,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAkZ,QAAAA,CACEkM,EACAC,EACA1mB,GAEA,OAAOwB,KAAKsY,QAAQzN,IAAI,YAADjM,OAAaqmB,EAAQ,cAAArmB,OAAasmB,GAAa,IACjE1mB,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKA4b,MAAAA,CACEwJ,EACAC,EACA9mB,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,YAADpO,OAAaqmB,EAAQ,cAAArmB,OAAasmB,GAAa,CACrE9mB,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAWAoZ,IAAAA,CACEgM,GAE6B,IAD7BhX,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAiD,CAAC,EAClDD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAKgM,EAAU,CAAC,EAAGhX,GAE1BjO,KAAKsY,QAAQhL,WAAW,YAAD1O,OAAaqmB,EAAQ,aAAaI,GAAoB,CAClFpX,WACGzP,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,EAGI,MAAOwlB,WAA2BtN,KAuPxC,SAAiBqN,GAKDA,EAAAC,mBAAqBC,GAIrBF,EAAAtM,MAAQY,GAER0L,EAAAD,iBAAmBzL,EAElC,CAbD,CAAiB0L,KAAAA,GAAQ,KCjUnB,MAAOG,WAAclN,GAIzBU,QAAAA,CACEkM,EACAO,EACAC,EACAjnB,GAEA,OAAOwB,KAAKsY,QAAQzN,IAAI,YAADjM,OAAaqmB,EAAQ,UAAArmB,OAAS4mB,EAAK,WAAA5mB,OAAU6mB,GAAU,IACzEjnB,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAgBAoZ,IAAAA,CACEgM,EACAO,GAE6B,IAD7BvX,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA8C,CAAC,EAC/CD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAKgM,EAAUO,EAAO,CAAC,EAAGvX,GAEjCjO,KAAKsY,QAAQhL,WAAW,YAAD1O,OAAaqmB,EAAQ,UAAArmB,OAAS4mB,EAAK,UAAUE,GAAc,CACvFzX,WACGzP,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,EAGI,MAAO6lB,WAAqB3N,KAuUlC,SAAiBwN,GAODA,EAAAG,aAAeC,EAE9B,CATD,CAAiBJ,KAAAA,GAAK,KCpXhB,MAAOK,WAAavN,GAA1Bla,WAAAA,G,oBACE,KAAA0nB,MAAwB,IAAIF,GAAe3lB,KAAKsY,QA2FlD,CAtFEE,MAAAA,CAAOyM,EAAkB7mB,EAAuBI,GAC9C,OAAOwB,KAAKsY,QAAQtL,KAAK,YAADpO,OAAaqmB,EAAQ,SAAS,CACpD7mB,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAkZ,QAAAA,CAASkM,EAAkBO,EAAehnB,GACxC,OAAOwB,KAAKsY,QAAQzN,IAAI,YAADjM,OAAaqmB,EAAQ,UAAArmB,OAAS4mB,GAAS,IACzDhnB,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKA4b,MAAAA,CACEwJ,EACAO,EACApnB,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,YAADpO,OAAaqmB,EAAQ,UAAArmB,OAAS4mB,GAAS,CAC7DpnB,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAWAoZ,IAAAA,CACEgM,GAE6B,IAD7BhX,EAAAxP,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAA6C,CAAC,EAC9CD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiBlG,GACZjO,KAAKiZ,KAAKgM,EAAU,CAAC,EAAGhX,GAE1BjO,KAAKsY,QAAQhL,WAAW,YAAD1O,OAAaqmB,EAAQ,SAASa,GAAU,CACpE7X,WACGzP,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKA+E,MAAAA,CAAOqgB,EAAkBO,EAAehnB,GACtC,OAAOwB,KAAKsY,QAAQtL,KAAK,YAADpO,OAAaqmB,EAAQ,UAAArmB,OAAS4mB,EAAK,WAAW,IACjEhnB,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAQAkmB,iBAAAA,CACEd,EACAO,EACApnB,EACAI,GAEA,OAAOwB,KAAKsY,QAAQtL,KAAK,YAADpO,OAAaqmB,EAAQ,UAAArmB,OAAS4mB,EAAK,wBAAwB,CACjFpnB,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,EAGI,MAAOimB,WAAiB/N,KA6X9B,SAAiB6N,GAGDA,EAAAE,SAAWE,GAKXJ,EAAAL,MAAQI,GAORC,EAAAF,aAAeC,EAE9B,CAjBD,CAAiBC,KAAAA,GAAI,KC3df,MAAOK,WAAgB5N,GAA7Bla,WAAAA,G,oBACE,KAAA+nB,KAAqB,IAAIF,GAAahmB,KAAKsY,SAC3C,KAAAsE,SAAiC,IAAI0I,GAAqBtlB,KAAKsY,QA8DjE,CAvDEE,MAAAA,GAE+B,IAD7Bpa,EAAAK,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GAAiD,CAAC,EAClDD,EAA6BC,UAAAC,OAAA,EAAAD,UAAA,QAAAX,EAE7B,OAAIqW,GAAiB/V,GACZ4B,KAAKwY,OAAO,CAAC,EAAGpa,GAElB4B,KAAKsY,QAAQtL,KAAK,WAAY,CACnC5O,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAkZ,QAAAA,CAASkM,EAAkBzmB,GACzB,OAAOwB,KAAKsY,QAAQzN,IAAI,YAADjM,OAAaqmB,GAAY,IAC3CzmB,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKA4b,MAAAA,CAAOwJ,EAAkB7mB,EAA0BI,GACjD,OAAOwB,KAAKsY,QAAQtL,KAAK,YAADpO,OAAaqmB,GAAY,CAC/C7mB,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAsZ,GAAAA,CAAI8L,EAAkBzmB,GACpB,OAAOwB,KAAKsY,QAAQnL,OAAO,YAADvO,OAAaqmB,GAAY,IAC9CzmB,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,CAKAsmB,YAAAA,CAAa/nB,EAAgCI,GAC3C,OAAOwB,KAAKsY,QAAQtL,KAAK,gBAAiB,CACxC5O,UACGI,EACHqB,QAAS,CAAE,cAAe,mBAA2B,OAAPrB,QAAO,IAAPA,OAAO,EAAPA,EAASqB,UAE3D,GA0NF,SAAiBomB,GAMDA,EAAAL,KAAOI,GAGPC,EAAAH,SAAWE,GAKXC,EAAAb,SAAWE,GAKXW,EAAAZ,mBAAqBC,EAIpC,CAvBD,CAAiBW,KAAAA,GAAO,KC5RlB,MAAOG,WAAa/N,GAA1Bla,WAAAA,G,oBACE,KAAA6mB,KAAqB,IAAIqB,GAAarmB,KAAKsY,SAC3C,KAAAgO,WAAuC,IAAIzK,GAAyB7b,KAAKsY,SACzE,KAAAiO,QAA8B,IAAIC,GAAmBxmB,KAAKsY,QAC5D,GAEA,SAAiB8N,GACDA,EAAA1N,KAAO2N,GACPD,EAAA7K,WAAaM,GAGbuK,EAAA1K,eAAiBG,GAIjBuK,EAAAH,QAAUO,EAMzB,CAfD,CAAiBJ,KAAAA,GAAI,KCuEf,MAAOK,WAAeC,EAoB1BvoB,WAAAA,GAKqB,IAAAwoB,EAAA/Y,EAAA,IALT,QACV5B,EAAU0a,GAAa,mBAAkB,OACzCE,EAASF,GAAa,kBAAiB,aACvCG,GAA4C,QAAhCF,EAAGD,GAAa,wBAAgB,IAAAC,EAAAA,EAAI,SAC7CpnB,GAAId,UAAAC,OAAA,QAAAZ,IAAAW,UAAA,GAAAA,UAAA,GACU,CAAC,EAClB,QAAeX,IAAX8oB,EACF,MAAM,IAAIE,EACR,sLAIJ,MAAMtoB,EAAyB,CAC7BooB,SACAC,kBACGtnB,EACHyM,QAASA,GAAW,6BAGtB,IAAKxN,EAAQuoB,yBjC28BK,qBAAXC,QAEoB,qBAApBA,OAAOC,UAEO,qBAAd3R,UiC98BL,MAAM,IAAIwR,EACR,sbAIJhnB,MAAM,CACJkM,QAASxN,EAAQwN,QACjBE,QAAwB,QAAjB0B,EAAEpP,EAAQ0N,eAAO,IAAA0B,EAAAA,EAAI,IAC5BzB,UAAW3N,EAAQ2N,UACnBF,WAAYzN,EAAQyN,WACpB7O,MAAOoB,EAAQpB,QAQnB,KAAAub,YAA+B,IAAIuO,GAAgBlnB,MACnD,KAAAglB,KAAiB,IAAIkC,GAASlnB,MAC9B,KAAAmnB,WAA6B,IAAID,GAAelnB,MAChD,KAAAwb,MAAmB,IAAI0L,GAAUlnB,MACjC,KAAAonB,OAAqB,IAAIF,GAAWlnB,MACpC,KAAAqnB,MAAmB,IAAIH,GAAUlnB,MACjC,KAAAsnB,YAA+B,IAAIJ,GAAgBlnB,MACnD,KAAAunB,OAAqB,IAAIL,GAAWlnB,MACpC,KAAAwnB,WAA6B,IAAIN,GAAelnB,MAChD,KAAAynB,KAAiB,IAAIP,GAASlnB,MAf5BA,KAAK0nB,SAAWlpB,EAEhBwB,KAAK4mB,OAASA,EACd5mB,KAAK6mB,aAAeA,CACtB,CAamBjW,YAAAA,GACjB,OAAO5Q,KAAK0nB,SAAS9W,YACvB,CAEmBrE,cAAAA,CAAehN,GAChC,MAAO,IACFO,MAAMyM,eAAehN,GACxB,sBAAuBS,KAAK6mB,gBACzB7mB,KAAK0nB,SAASnb,eAErB,CAEmBD,WAAAA,CAAY/M,GAC7B,MAAO,CAAEooB,cAAe,UAAF/oB,OAAYoB,KAAK4mB,QACzC,E,MAEOH,GAAAA,OAASnC,GAETmC,GAAA/mB,YAAconB,EACdL,GAAA9mB,SAAWmnB,EACXL,GAAA/lB,mBAAqBomB,EACrBL,GAAAllB,0BAA4BulB,EAC5BL,GAAAplB,kBAAoBylB,EACpBL,GAAAzlB,cAAgB8lB,EAChBL,GAAAxlB,cAAgB6lB,EAChBL,GAAAtlB,eAAiB2lB,EACjBL,GAAA5lB,gBAAkBimB,EAClBL,GAAA3lB,oBAAsBgmB,EACtBL,GAAArlB,oBAAsB0lB,EACtBL,GAAA1lB,sBAAwB+lB,EACxBL,GAAAvlB,yBAA2B4lB,EAG7B,MACLpnB,YAAW,GACXC,SAAQ,GACRe,mBAAkB,GAClBa,0BAAyB,GACzBF,kBAAiB,GACjBL,cAAa,GACbC,cAAa,GACbE,eAAc,GACdN,gBAAe,GACfC,oBAAmB,GACnBM,oBAAmB,GACnBL,sBAAqB,GACrBG,yBAAwBA,IACtB4lB,GAKJ,SAAiBL,GAEDA,EAAA1e,OAAS6f,EACTnB,EAAA/oB,aAAekqB,EAIfnB,EAAAlZ,KAAOsa,GAGPpB,EAAA1O,WAAa8P,GAIbpB,EAAAlO,YAAc2O,GAQdT,EAAA/N,KAAOwO,GAwBPT,EAAA5N,WAAaqO,GAKbT,EAAA3N,MAAQoO,GAIRT,EAAAvN,gBAAkBgO,GAIlBT,EAAA9M,OAASuN,GAOTT,EAAAxM,MAAQiN,GAERT,EAAAjM,YAAc0M,GAKdT,EAAAhM,OAASyM,GAGTT,EAAA9L,WAAauM,GAEbT,EAAAtL,WAAa+L,GAEbT,EAAAL,KAAOc,EAItB,CApFD,CAAiBT,KAAAA,GAAM,I","sources":["../node_modules/openai/src/version.ts","../node_modules/openai/src/_shims/registry.ts","../node_modules/openai/src/_shims/MultipartBody.ts","../node_modules/openai/_shims/index.mjs","../node_modules/openai/src/_shims/web-runtime.ts","../node_modules/openai/src/error.ts","../node_modules/openai/src/streaming.ts","../node_modules/openai/src/uploads.ts","../node_modules/openai/src/core.ts","../node_modules/openai/src/pagination.ts","../node_modules/openai/src/resource.ts","../node_modules/openai/src/resources/completions.ts","../node_modules/openai/src/resources/chat/completions.ts","../node_modules/openai/src/resources/chat/chat.ts","../node_modules/openai/src/resources/embeddings.ts","../node_modules/openai/src/resources/files.ts","../node_modules/openai/src/resources/images.ts","../node_modules/openai/src/resources/audio/speech.ts","../node_modules/openai/src/resources/audio/transcriptions.ts","../node_modules/openai/src/resources/audio/translations.ts","../node_modules/openai/src/resources/audio/audio.ts","../node_modules/openai/src/resources/moderations.ts","../node_modules/openai/src/resources/models.ts","../node_modules/openai/src/resources/fine-tuning/jobs.ts","../node_modules/openai/src/resources/fine-tuning/fine-tuning.ts","../node_modules/openai/src/resources/beta/assistants/files.ts","../node_modules/openai/src/resources/beta/assistants/assistants.ts","../node_modules/openai/src/lib/RunnableFunction.ts","../node_modules/openai/src/lib/chatCompletionUtils.ts","../node_modules/openai/src/lib/AbstractChatCompletionRunner.ts","../node_modules/openai/src/lib/ChatCompletionRunner.ts","../node_modules/openai/src/lib/ChatCompletionStream.ts","../node_modules/openai/src/lib/ChatCompletionStreamingRunner.ts","../node_modules/openai/src/resources/beta/chat/completions.ts","../node_modules/openai/src/resources/beta/chat/chat.ts","../node_modules/openai/src/resources/beta/threads/messages/files.ts","../node_modules/openai/src/resources/beta/threads/messages/messages.ts","../node_modules/openai/src/resources/beta/threads/runs/steps.ts","../node_modules/openai/src/resources/beta/threads/runs/runs.ts","../node_modules/openai/src/resources/beta/threads/threads.ts","../node_modules/openai/src/resources/beta/beta.ts","../node_modules/openai/src/index.ts"],"sourcesContent":["export const VERSION = '4.26.0'; // x-release-please-version\n","/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\nimport { type RequestOptions } from '../core';\n\nexport interface Shims {\n  kind: string;\n  fetch: any;\n  Request: any;\n  Response: any;\n  Headers: any;\n  FormData: any;\n  Blob: any;\n  File: any;\n  ReadableStream: any;\n  getMultipartRequestOptions: <T = Record<string, unknown>>(\n    form: Shims['FormData'],\n    opts: RequestOptions<T>,\n  ) => Promise<RequestOptions<T>>;\n  getDefaultAgent: (url: string) => any;\n  fileFromPath:\n    | ((path: string, filename?: string, options?: {}) => Promise<Shims['File']>)\n    | ((path: string, options?: {}) => Promise<Shims['File']>);\n  isFsReadStream: (value: any) => boolean;\n}\n\nexport let auto = false;\nexport let kind: Shims['kind'] | undefined = undefined;\nexport let fetch: Shims['fetch'] | undefined = undefined;\nexport let Request: Shims['Request'] | undefined = undefined;\nexport let Response: Shims['Response'] | undefined = undefined;\nexport let Headers: Shims['Headers'] | undefined = undefined;\nexport let FormData: Shims['FormData'] | undefined = undefined;\nexport let Blob: Shims['Blob'] | undefined = undefined;\nexport let File: Shims['File'] | undefined = undefined;\nexport let ReadableStream: Shims['ReadableStream'] | undefined = undefined;\nexport let getMultipartRequestOptions: Shims['getMultipartRequestOptions'] | undefined = undefined;\nexport let getDefaultAgent: Shims['getDefaultAgent'] | undefined = undefined;\nexport let fileFromPath: Shims['fileFromPath'] | undefined = undefined;\nexport let isFsReadStream: Shims['isFsReadStream'] | undefined = undefined;\n\nexport function setShims(shims: Shims, options: { auto: boolean } = { auto: false }) {\n  if (auto) {\n    throw new Error(\n      `you must \\`import 'openai/shims/${shims.kind}'\\` before importing anything else from openai`,\n    );\n  }\n  if (kind) {\n    throw new Error(`can't \\`import 'openai/shims/${shims.kind}'\\` after \\`import 'openai/shims/${kind}'\\``);\n  }\n  auto = options.auto;\n  kind = shims.kind;\n  fetch = shims.fetch;\n  Request = shims.Request;\n  Response = shims.Response;\n  Headers = shims.Headers;\n  FormData = shims.FormData;\n  Blob = shims.Blob;\n  File = shims.File;\n  ReadableStream = shims.ReadableStream;\n  getMultipartRequestOptions = shims.getMultipartRequestOptions;\n  getDefaultAgent = shims.getDefaultAgent;\n  fileFromPath = shims.fileFromPath;\n  isFsReadStream = shims.isFsReadStream;\n}\n","/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\nexport class MultipartBody {\n  constructor(public body: any) {}\n  get [Symbol.toStringTag](): string {\n    return 'MultipartBody';\n  }\n}\n","/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\nimport * as shims from './registry.mjs';\nimport * as auto from 'openai/_shims/auto/runtime';\nif (!shims.kind) shims.setShims(auto.getRuntime(), { auto: true });\nexport * from './registry.mjs';\n","/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\nimport { MultipartBody } from './MultipartBody';\nimport { type RequestOptions } from '../core';\nimport { type Shims } from './registry';\n\nexport function getRuntime({ manuallyImported }: { manuallyImported?: boolean } = {}): Shims {\n  const recommendation =\n    manuallyImported ?\n      `You may need to use polyfills`\n    : `Add one of these imports before your first \\`import  from 'openai'\\`:\n- \\`import 'openai/shims/node'\\` (if you're running on Node)\n- \\`import 'openai/shims/web'\\` (otherwise)\n`;\n\n  let _fetch, _Request, _Response, _Headers;\n  try {\n    // @ts-ignore\n    _fetch = fetch;\n    // @ts-ignore\n    _Request = Request;\n    // @ts-ignore\n    _Response = Response;\n    // @ts-ignore\n    _Headers = Headers;\n  } catch (error) {\n    throw new Error(\n      `this environment is missing the following Web Fetch API type: ${\n        (error as any).message\n      }. ${recommendation}`,\n    );\n  }\n\n  return {\n    kind: 'web',\n    fetch: _fetch,\n    Request: _Request,\n    Response: _Response,\n    Headers: _Headers,\n    FormData:\n      // @ts-ignore\n      typeof FormData !== 'undefined' ? FormData : (\n        class FormData {\n          // @ts-ignore\n          constructor() {\n            throw new Error(\n              `file uploads aren't supported in this environment yet as 'FormData' is undefined. ${recommendation}`,\n            );\n          }\n        }\n      ),\n    Blob:\n      typeof Blob !== 'undefined' ? Blob : (\n        class Blob {\n          constructor() {\n            throw new Error(\n              `file uploads aren't supported in this environment yet as 'Blob' is undefined. ${recommendation}`,\n            );\n          }\n        }\n      ),\n    File:\n      // @ts-ignore\n      typeof File !== 'undefined' ? File : (\n        class File {\n          // @ts-ignore\n          constructor() {\n            throw new Error(\n              `file uploads aren't supported in this environment yet as 'File' is undefined. ${recommendation}`,\n            );\n          }\n        }\n      ),\n    ReadableStream:\n      // @ts-ignore\n      typeof ReadableStream !== 'undefined' ? ReadableStream : (\n        class ReadableStream {\n          // @ts-ignore\n          constructor() {\n            throw new Error(\n              `streaming isn't supported in this environment yet as 'ReadableStream' is undefined. ${recommendation}`,\n            );\n          }\n        }\n      ),\n    getMultipartRequestOptions: async <T = Record<string, unknown>>(\n      // @ts-ignore\n      form: FormData,\n      opts: RequestOptions<T>,\n    ): Promise<RequestOptions<T>> => ({\n      ...opts,\n      body: new MultipartBody(form) as any,\n    }),\n    getDefaultAgent: (url: string) => undefined,\n    fileFromPath: () => {\n      throw new Error(\n        'The `fileFromPath` function is only supported in Node. See the README for more details: https://www.github.com/openai/openai-node#file-uploads',\n      );\n    },\n    isFsReadStream: (value: any) => false,\n  };\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport { castToError, Headers } from './core';\n\nexport class OpenAIError extends Error {}\n\nexport class APIError extends OpenAIError {\n  readonly status: number | undefined;\n  readonly headers: Headers | undefined;\n  readonly error: Object | undefined;\n\n  readonly code: string | null | undefined;\n  readonly param: string | null | undefined;\n  readonly type: string | undefined;\n\n  constructor(\n    status: number | undefined,\n    error: Object | undefined,\n    message: string | undefined,\n    headers: Headers | undefined,\n  ) {\n    super(`${APIError.makeMessage(status, error, message)}`);\n    this.status = status;\n    this.headers = headers;\n\n    const data = error as Record<string, any>;\n    this.error = data;\n    this.code = data?.['code'];\n    this.param = data?.['param'];\n    this.type = data?.['type'];\n  }\n\n  private static makeMessage(status: number | undefined, error: any, message: string | undefined) {\n    const msg =\n      error?.message ?\n        typeof error.message === 'string' ?\n          error.message\n        : JSON.stringify(error.message)\n      : error ? JSON.stringify(error)\n      : message;\n\n    if (status && msg) {\n      return `${status} ${msg}`;\n    }\n    if (status) {\n      return `${status} status code (no body)`;\n    }\n    if (msg) {\n      return msg;\n    }\n    return '(no status code or body)';\n  }\n\n  static generate(\n    status: number | undefined,\n    errorResponse: Object | undefined,\n    message: string | undefined,\n    headers: Headers | undefined,\n  ) {\n    if (!status) {\n      return new APIConnectionError({ cause: castToError(errorResponse) });\n    }\n\n    const error = (errorResponse as Record<string, any>)?.['error'];\n\n    if (status === 400) {\n      return new BadRequestError(status, error, message, headers);\n    }\n\n    if (status === 401) {\n      return new AuthenticationError(status, error, message, headers);\n    }\n\n    if (status === 403) {\n      return new PermissionDeniedError(status, error, message, headers);\n    }\n\n    if (status === 404) {\n      return new NotFoundError(status, error, message, headers);\n    }\n\n    if (status === 409) {\n      return new ConflictError(status, error, message, headers);\n    }\n\n    if (status === 422) {\n      return new UnprocessableEntityError(status, error, message, headers);\n    }\n\n    if (status === 429) {\n      return new RateLimitError(status, error, message, headers);\n    }\n\n    if (status >= 500) {\n      return new InternalServerError(status, error, message, headers);\n    }\n\n    return new APIError(status, error, message, headers);\n  }\n}\n\nexport class APIUserAbortError extends APIError {\n  override readonly status: undefined = undefined;\n\n  constructor({ message }: { message?: string } = {}) {\n    super(undefined, undefined, message || 'Request was aborted.', undefined);\n  }\n}\n\nexport class APIConnectionError extends APIError {\n  override readonly status: undefined = undefined;\n\n  constructor({ message, cause }: { message?: string; cause?: Error | undefined }) {\n    super(undefined, undefined, message || 'Connection error.', undefined);\n    // in some environments the 'cause' property is already declared\n    // @ts-ignore\n    if (cause) this.cause = cause;\n  }\n}\n\nexport class APIConnectionTimeoutError extends APIConnectionError {\n  constructor({ message }: { message?: string } = {}) {\n    super({ message: message ?? 'Request timed out.' });\n  }\n}\n\nexport class BadRequestError extends APIError {\n  override readonly status: 400 = 400;\n}\n\nexport class AuthenticationError extends APIError {\n  override readonly status: 401 = 401;\n}\n\nexport class PermissionDeniedError extends APIError {\n  override readonly status: 403 = 403;\n}\n\nexport class NotFoundError extends APIError {\n  override readonly status: 404 = 404;\n}\n\nexport class ConflictError extends APIError {\n  override readonly status: 409 = 409;\n}\n\nexport class UnprocessableEntityError extends APIError {\n  override readonly status: 422 = 422;\n}\n\nexport class RateLimitError extends APIError {\n  override readonly status: 429 = 429;\n}\n\nexport class InternalServerError extends APIError {}\n","import { ReadableStream, type Response } from './_shims/index';\nimport { OpenAIError } from './error';\n\nimport { APIError } from \"./error\";\n\ntype Bytes = string | ArrayBuffer | Uint8Array | Buffer | null | undefined;\n\nexport type ServerSentEvent = {\n  event: string | null;\n  data: string;\n  raw: string[];\n};\n\nexport class Stream<Item> implements AsyncIterable<Item> {\n  controller: AbortController;\n\n  constructor(\n    private iterator: () => AsyncIterator<Item>,\n    controller: AbortController,\n  ) {\n    this.controller = controller;\n  }\n\n  static fromSSEResponse<Item>(response: Response, controller: AbortController) {\n    let consumed = false;\n    const decoder = new SSEDecoder();\n\n    async function* iterMessages(): AsyncGenerator<ServerSentEvent, void, unknown> {\n      if (!response.body) {\n        controller.abort();\n        throw new OpenAIError(`Attempted to iterate over a response with no body`);\n      }\n\n      const lineDecoder = new LineDecoder();\n\n      const iter = readableStreamAsyncIterable<Bytes>(response.body);\n      for await (const chunk of iter) {\n        for (const line of lineDecoder.decode(chunk)) {\n          const sse = decoder.decode(line);\n          if (sse) yield sse;\n        }\n      }\n\n      for (const line of lineDecoder.flush()) {\n        const sse = decoder.decode(line);\n        if (sse) yield sse;\n      }\n    }\n\n    async function* iterator(): AsyncIterator<Item, any, undefined> {\n      if (consumed) {\n        throw new Error('Cannot iterate over a consumed stream, use `.tee()` to split the stream.');\n      }\n      consumed = true;\n      let done = false;\n      try {\n        for await (const sse of iterMessages()) {\n          if (done) continue;\n\n          if (sse.data.startsWith('[DONE]')) {\n            done = true;\n            continue;\n          }\n\n          if (sse.event === null) {\n            let data;\n\n            try {\n              data = JSON.parse(sse.data);\n            } catch (e) {\n              console.error(`Could not parse message into JSON:`, sse.data);\n              console.error(`From chunk:`, sse.raw);\n              throw e;\n            }\n\n            if (data && data.error) {\n              throw new APIError(undefined, data.error, undefined, undefined);\n            }\n\n            yield data;\n          }\n        }\n        done = true;\n      } catch (e) {\n        // If the user calls `stream.controller.abort()`, we should exit without throwing.\n        if (e instanceof Error && e.name === 'AbortError') return;\n        throw e;\n      } finally {\n        // If the user `break`s, abort the ongoing request.\n        if (!done) controller.abort();\n      }\n    }\n\n    return new Stream(iterator, controller);\n  }\n\n  /**\n   * Generates a Stream from a newline-separated ReadableStream\n   * where each item is a JSON value.\n   */\n  static fromReadableStream<Item>(readableStream: ReadableStream, controller: AbortController) {\n    let consumed = false;\n\n    async function* iterLines(): AsyncGenerator<string, void, unknown> {\n      const lineDecoder = new LineDecoder();\n\n      const iter = readableStreamAsyncIterable<Bytes>(readableStream);\n      for await (const chunk of iter) {\n        for (const line of lineDecoder.decode(chunk)) {\n          yield line;\n        }\n      }\n\n      for (const line of lineDecoder.flush()) {\n        yield line;\n      }\n    }\n\n    async function* iterator(): AsyncIterator<Item, any, undefined> {\n      if (consumed) {\n        throw new Error('Cannot iterate over a consumed stream, use `.tee()` to split the stream.');\n      }\n      consumed = true;\n      let done = false;\n      try {\n        for await (const line of iterLines()) {\n          if (done) continue;\n          if (line) yield JSON.parse(line);\n        }\n        done = true;\n      } catch (e) {\n        // If the user calls `stream.controller.abort()`, we should exit without throwing.\n        if (e instanceof Error && e.name === 'AbortError') return;\n        throw e;\n      } finally {\n        // If the user `break`s, abort the ongoing request.\n        if (!done) controller.abort();\n      }\n    }\n\n    return new Stream(iterator, controller);\n  }\n\n  [Symbol.asyncIterator](): AsyncIterator<Item> {\n    return this.iterator();\n  }\n\n  /**\n   * Splits the stream into two streams which can be\n   * independently read from at different speeds.\n   */\n  tee(): [Stream<Item>, Stream<Item>] {\n    const left: Array<Promise<IteratorResult<Item>>> = [];\n    const right: Array<Promise<IteratorResult<Item>>> = [];\n    const iterator = this.iterator();\n\n    const teeIterator = (queue: Array<Promise<IteratorResult<Item>>>): AsyncIterator<Item> => {\n      return {\n        next: () => {\n          if (queue.length === 0) {\n            const result = iterator.next();\n            left.push(result);\n            right.push(result);\n          }\n          return queue.shift()!;\n        },\n      };\n    };\n\n    return [\n      new Stream(() => teeIterator(left), this.controller),\n      new Stream(() => teeIterator(right), this.controller),\n    ];\n  }\n\n  /**\n   * Converts this stream to a newline-separated ReadableStream of\n   * JSON stringified values in the stream\n   * which can be turned back into a Stream with `Stream.fromReadableStream()`.\n   */\n  toReadableStream(): ReadableStream {\n    const self = this;\n    let iter: AsyncIterator<Item>;\n    const encoder = new TextEncoder();\n\n    return new ReadableStream({\n      async start() {\n        iter = self[Symbol.asyncIterator]();\n      },\n      async pull(ctrl) {\n        try {\n          const { value, done } = await iter.next();\n          if (done) return ctrl.close();\n\n          const bytes = encoder.encode(JSON.stringify(value) + '\\n');\n\n          ctrl.enqueue(bytes);\n        } catch (err) {\n          ctrl.error(err);\n        }\n      },\n      async cancel() {\n        await iter.return?.();\n      },\n    });\n  }\n}\n\nclass SSEDecoder {\n  private data: string[];\n  private event: string | null;\n  private chunks: string[];\n\n  constructor() {\n    this.event = null;\n    this.data = [];\n    this.chunks = [];\n  }\n\n  decode(line: string) {\n    if (line.endsWith('\\r')) {\n      line = line.substring(0, line.length - 1);\n    }\n\n    if (!line) {\n      // empty line and we didn't previously encounter any messages\n      if (!this.event && !this.data.length) return null;\n\n      const sse: ServerSentEvent = {\n        event: this.event,\n        data: this.data.join('\\n'),\n        raw: this.chunks,\n      };\n\n      this.event = null;\n      this.data = [];\n      this.chunks = [];\n\n      return sse;\n    }\n\n    this.chunks.push(line);\n\n    if (line.startsWith(':')) {\n      return null;\n    }\n\n    let [fieldname, _, value] = partition(line, ':');\n\n    if (value.startsWith(' ')) {\n      value = value.substring(1);\n    }\n\n    if (fieldname === 'event') {\n      this.event = value;\n    } else if (fieldname === 'data') {\n      this.data.push(value);\n    }\n\n    return null;\n  }\n}\n\n/**\n * A re-implementation of httpx's `LineDecoder` in Python that handles incrementally\n * reading lines from text.\n *\n * https://github.com/encode/httpx/blob/920333ea98118e9cf617f246905d7b202510941c/httpx/_decoders.py#L258\n */\nclass LineDecoder {\n  // prettier-ignore\n  static NEWLINE_CHARS = new Set(['\\n', '\\r', '\\x0b', '\\x0c', '\\x1c', '\\x1d', '\\x1e', '\\x85', '\\u2028', '\\u2029']);\n  static NEWLINE_REGEXP = /\\r\\n|[\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029]/g;\n\n  buffer: string[];\n  trailingCR: boolean;\n  textDecoder: any; // TextDecoder found in browsers; not typed to avoid pulling in either \"dom\" or \"node\" types.\n\n  constructor() {\n    this.buffer = [];\n    this.trailingCR = false;\n  }\n\n  decode(chunk: Bytes): string[] {\n    let text = this.decodeText(chunk);\n\n    if (this.trailingCR) {\n      text = '\\r' + text;\n      this.trailingCR = false;\n    }\n    if (text.endsWith('\\r')) {\n      this.trailingCR = true;\n      text = text.slice(0, -1);\n    }\n\n    if (!text) {\n      return [];\n    }\n\n    const trailingNewline = LineDecoder.NEWLINE_CHARS.has(text[text.length - 1] || '');\n    let lines = text.split(LineDecoder.NEWLINE_REGEXP);\n\n    if (lines.length === 1 && !trailingNewline) {\n      this.buffer.push(lines[0]!);\n      return [];\n    }\n\n    if (this.buffer.length > 0) {\n      lines = [this.buffer.join('') + lines[0], ...lines.slice(1)];\n      this.buffer = [];\n    }\n\n    if (!trailingNewline) {\n      this.buffer = [lines.pop() || ''];\n    }\n\n    return lines;\n  }\n\n  decodeText(bytes: Bytes): string {\n    if (bytes == null) return '';\n    if (typeof bytes === 'string') return bytes;\n\n    // Node:\n    if (typeof Buffer !== 'undefined') {\n      if (bytes instanceof Buffer) {\n        return bytes.toString();\n      }\n      if (bytes instanceof Uint8Array) {\n        return Buffer.from(bytes).toString();\n      }\n\n      throw new OpenAIError(\n        `Unexpected: received non-Uint8Array (${bytes.constructor.name}) stream chunk in an environment with a global \"Buffer\" defined, which this library assumes to be Node. Please report this error.`,\n      );\n    }\n\n    // Browser\n    if (typeof TextDecoder !== 'undefined') {\n      if (bytes instanceof Uint8Array || bytes instanceof ArrayBuffer) {\n        this.textDecoder ??= new TextDecoder('utf8');\n        return this.textDecoder.decode(bytes);\n      }\n\n      throw new OpenAIError(\n        `Unexpected: received non-Uint8Array/ArrayBuffer (${\n          (bytes as any).constructor.name\n        }) in a web platform. Please report this error.`,\n      );\n    }\n\n    throw new OpenAIError(\n      `Unexpected: neither Buffer nor TextDecoder are available as globals. Please report this error.`,\n    );\n  }\n\n  flush(): string[] {\n    if (!this.buffer.length && !this.trailingCR) {\n      return [];\n    }\n\n    const lines = [this.buffer.join('')];\n    this.buffer = [];\n    this.trailingCR = false;\n    return lines;\n  }\n}\n\nfunction partition(str: string, delimiter: string): [string, string, string] {\n  const index = str.indexOf(delimiter);\n  if (index !== -1) {\n    return [str.substring(0, index), delimiter, str.substring(index + delimiter.length)];\n  }\n\n  return [str, '', ''];\n}\n\n/**\n * Most browsers don't yet have async iterable support for ReadableStream,\n * and Node has a very different way of reading bytes from its \"ReadableStream\".\n *\n * This polyfill was pulled from https://github.com/MattiasBuelens/web-streams-polyfill/pull/122#issuecomment-1627354490\n */\nexport function readableStreamAsyncIterable<T>(stream: any): AsyncIterableIterator<T> {\n  if (stream[Symbol.asyncIterator]) return stream;\n\n  const reader = stream.getReader();\n  return {\n    async next() {\n      try {\n        const result = await reader.read();\n        if (result?.done) reader.releaseLock(); // release lock when stream becomes closed\n        return result;\n      } catch (e) {\n        reader.releaseLock(); // release lock when stream becomes errored\n        throw e;\n      }\n    },\n    async return() {\n      const cancelPromise = reader.cancel();\n      reader.releaseLock();\n      await cancelPromise;\n      return { done: true, value: undefined };\n    },\n    [Symbol.asyncIterator]() {\n      return this;\n    },\n  };\n}\n","import { type RequestOptions } from './core';\nimport {\n  FormData,\n  File,\n  type Blob,\n  type FilePropertyBag,\n  getMultipartRequestOptions,\n  type FsReadStream,\n  isFsReadStream,\n} from './_shims/index';\nimport { MultipartBody } from './_shims/MultipartBody';\nexport { fileFromPath } from './_shims/index';\n\ntype BlobLikePart = string | ArrayBuffer | ArrayBufferView | BlobLike | Uint8Array | DataView;\nexport type BlobPart = string | ArrayBuffer | ArrayBufferView | Blob | Uint8Array | DataView;\n\n/**\n * Typically, this is a native \"File\" class.\n *\n * We provide the {@link toFile} utility to convert a variety of objects\n * into the File class.\n *\n * For convenience, you can also pass a fetch Response, or in Node,\n * the result of fs.createReadStream().\n */\nexport type Uploadable = FileLike | ResponseLike | FsReadStream;\n\n/**\n * Intended to match web.Blob, node.Blob, node-fetch.Blob, etc.\n */\nexport interface BlobLike {\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/size) */\n  readonly size: number;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/type) */\n  readonly type: string;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/text) */\n  text(): Promise<string>;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/Blob/slice) */\n  slice(start?: number, end?: number): BlobLike;\n  // unfortunately @types/node-fetch@^2.6.4 doesn't type the arrayBuffer method\n}\n\n/**\n * Intended to match web.File, node.File, node-fetch.File, etc.\n */\nexport interface FileLike extends BlobLike {\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/File/lastModified) */\n  readonly lastModified: number;\n  /** [MDN Reference](https://developer.mozilla.org/docs/Web/API/File/name) */\n  readonly name: string;\n}\n\n/**\n * Intended to match web.Response, node.Response, node-fetch.Response, etc.\n */\nexport interface ResponseLike {\n  url: string;\n  blob(): Promise<BlobLike>;\n}\n\nexport const isResponseLike = (value: any): value is ResponseLike =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.url === 'string' &&\n  typeof value.blob === 'function';\n\nexport const isFileLike = (value: any): value is FileLike =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.name === 'string' &&\n  typeof value.lastModified === 'number' &&\n  isBlobLike(value);\n\n/**\n * The BlobLike type omits arrayBuffer() because @types/node-fetch@^2.6.4 lacks it; but this check\n * adds the arrayBuffer() method type because it is available and used at runtime\n */\nexport const isBlobLike = (value: any): value is BlobLike & { arrayBuffer(): Promise<ArrayBuffer> } =>\n  value != null &&\n  typeof value === 'object' &&\n  typeof value.size === 'number' &&\n  typeof value.type === 'string' &&\n  typeof value.text === 'function' &&\n  typeof value.slice === 'function' &&\n  typeof value.arrayBuffer === 'function';\n\nexport const isUploadable = (value: any): value is Uploadable => {\n  return isFileLike(value) || isResponseLike(value) || isFsReadStream(value);\n};\n\nexport type ToFileInput = Uploadable | Exclude<BlobLikePart, string> | AsyncIterable<BlobLikePart>;\n\n/**\n * Helper for creating a {@link File} to pass to an SDK upload method from a variety of different data formats\n * @param value the raw content of the file.  Can be an {@link Uploadable}, {@link BlobLikePart}, or {@link AsyncIterable} of {@link BlobLikePart}s\n * @param {string=} name the name of the file. If omitted, toFile will try to determine a file name from bits if possible\n * @param {Object=} options additional properties\n * @param {string=} options.type the MIME type of the content\n * @param {number=} options.lastModified the last modified timestamp\n * @returns a {@link File} with the given properties\n */\nexport async function toFile(\n  value: ToFileInput | PromiseLike<ToFileInput>,\n  name?: string | null | undefined,\n  options: FilePropertyBag | undefined = {},\n): Promise<FileLike> {\n  // If it's a promise, resolve it.\n  value = await value;\n\n  if (isResponseLike(value)) {\n    const blob = await value.blob();\n    name ||= new URL(value.url).pathname.split(/[\\\\/]/).pop() ?? 'unknown_file';\n\n    return new File([blob as any], name, options);\n  }\n\n  const bits = await getBytes(value);\n\n  name ||= getName(value) ?? 'unknown_file';\n\n  if (!options.type) {\n    const type = (bits[0] as any)?.type;\n    if (typeof type === 'string') {\n      options = { ...options, type };\n    }\n  }\n\n  return new File(bits, name, options);\n}\n\nasync function getBytes(value: ToFileInput): Promise<Array<BlobPart>> {\n  let parts: Array<BlobPart> = [];\n  if (\n    typeof value === 'string' ||\n    ArrayBuffer.isView(value) || // includes Uint8Array, Buffer, etc.\n    value instanceof ArrayBuffer\n  ) {\n    parts.push(value);\n  } else if (isBlobLike(value)) {\n    parts.push(await value.arrayBuffer());\n  } else if (\n    isAsyncIterableIterator(value) // includes Readable, ReadableStream, etc.\n  ) {\n    for await (const chunk of value) {\n      parts.push(chunk as BlobPart); // TODO, consider validating?\n    }\n  } else {\n    throw new Error(\n      `Unexpected data type: ${typeof value}; constructor: ${value?.constructor\n        ?.name}; props: ${propsForError(value)}`,\n    );\n  }\n\n  return parts;\n}\n\nfunction propsForError(value: any): string {\n  const props = Object.getOwnPropertyNames(value);\n  return `[${props.map((p) => `\"${p}\"`).join(', ')}]`;\n}\n\nfunction getName(value: any): string | undefined {\n  return (\n    getStringFromMaybeBuffer(value.name) ||\n    getStringFromMaybeBuffer(value.filename) ||\n    // For fs.ReadStream\n    getStringFromMaybeBuffer(value.path)?.split(/[\\\\/]/).pop()\n  );\n}\n\nconst getStringFromMaybeBuffer = (x: string | Buffer | unknown): string | undefined => {\n  if (typeof x === 'string') return x;\n  if (typeof Buffer !== 'undefined' && x instanceof Buffer) return String(x);\n  return undefined;\n};\n\nconst isAsyncIterableIterator = (value: any): value is AsyncIterableIterator<unknown> =>\n  value != null && typeof value === 'object' && typeof value[Symbol.asyncIterator] === 'function';\n\nexport const isMultipartBody = (body: any): body is MultipartBody =>\n  body && typeof body === 'object' && body.body && body[Symbol.toStringTag] === 'MultipartBody';\n\n/**\n * Returns a multipart/form-data request if any part of the given request body contains a File / Blob value.\n * Otherwise returns the request as is.\n */\nexport const maybeMultipartFormRequestOptions = async <T = Record<string, unknown>>(\n  opts: RequestOptions<T>,\n): Promise<RequestOptions<T | MultipartBody>> => {\n  if (!hasUploadableValue(opts.body)) return opts;\n\n  const form = await createForm(opts.body);\n  return getMultipartRequestOptions(form, opts);\n};\n\nexport const multipartFormRequestOptions = async <T = Record<string, unknown>>(\n  opts: RequestOptions<T>,\n): Promise<RequestOptions<T | MultipartBody>> => {\n  const form = await createForm(opts.body);\n  return getMultipartRequestOptions(form, opts);\n};\n\nexport const createForm = async <T = Record<string, unknown>>(body: T | undefined): Promise<FormData> => {\n  const form = new FormData();\n  await Promise.all(Object.entries(body || {}).map(([key, value]) => addFormValue(form, key, value)));\n  return form;\n};\n\nconst hasUploadableValue = (value: unknown): boolean => {\n  if (isUploadable(value)) return true;\n  if (Array.isArray(value)) return value.some(hasUploadableValue);\n  if (value && typeof value === 'object') {\n    for (const k in value) {\n      if (hasUploadableValue((value as any)[k])) return true;\n    }\n  }\n  return false;\n};\n\nconst addFormValue = async (form: FormData, key: string, value: unknown): Promise<void> => {\n  if (value === undefined) return;\n  if (value == null) {\n    throw new TypeError(\n      `Received null for \"${key}\"; to pass null in FormData, you must use the string 'null'`,\n    );\n  }\n\n  // TODO: make nested formats configurable\n  if (typeof value === 'string' || typeof value === 'number' || typeof value === 'boolean') {\n    form.append(key, String(value));\n  } else if (isUploadable(value)) {\n    const file = await toFile(value);\n    form.append(key, file as File);\n  } else if (Array.isArray(value)) {\n    await Promise.all(value.map((entry) => addFormValue(form, key + '[]', entry)));\n  } else if (typeof value === 'object') {\n    await Promise.all(\n      Object.entries(value).map(([name, prop]) => addFormValue(form, `${key}[${name}]`, prop)),\n    );\n  } else {\n    throw new TypeError(\n      `Invalid value given to form, expected a string, number, boolean, object, Array, File or Blob but got ${value} instead`,\n    );\n  }\n};\n","import { VERSION } from './version';\nimport { Stream } from './streaming';\nimport {\n  OpenAIError,\n  APIError,\n  APIConnectionError,\n  APIConnectionTimeoutError,\n  APIUserAbortError,\n} from './error';\nimport {\n  kind as shimsKind,\n  type Readable,\n  getDefaultAgent,\n  type Agent,\n  fetch,\n  type RequestInfo,\n  type RequestInit,\n  type Response,\n  type HeadersInit,\n} from './_shims/index';\nexport { type Response };\nimport { isMultipartBody } from './uploads';\nexport {\n  maybeMultipartFormRequestOptions,\n  multipartFormRequestOptions,\n  createForm,\n  type Uploadable,\n} from './uploads';\n\nexport type Fetch = (url: RequestInfo, init?: RequestInit) => Promise<Response>;\n\ntype PromiseOrValue<T> = T | Promise<T>;\n\ntype APIResponseProps = {\n  response: Response;\n  options: FinalRequestOptions;\n  controller: AbortController;\n};\n\nasync function defaultParseResponse<T>(props: APIResponseProps): Promise<T> {\n  const { response } = props;\n  if (props.options.stream) {\n    debug('response', response.status, response.url, response.headers, response.body);\n\n    // Note: there is an invariant here that isn't represented in the type system\n    // that if you set `stream: true` the response type must also be `Stream<T>`\n\n    if (props.options.__streamClass) {\n      return props.options.__streamClass.fromSSEResponse(response, props.controller) as any;\n    }\n\n    return Stream.fromSSEResponse(response, props.controller) as any;\n  }\n\n  // fetch refuses to read the body when the status code is 204.\n  if (response.status === 204) {\n    return null as T;\n  }\n\n  if (props.options.__binaryResponse) {\n    return response as unknown as T;\n  }\n\n  const contentType = response.headers.get('content-type');\n  if (contentType?.includes('application/json')) {\n    const json = await response.json();\n\n    debug('response', response.status, response.url, response.headers, json);\n\n    return json as T;\n  }\n\n  const text = await response.text();\n  debug('response', response.status, response.url, response.headers, text);\n\n  // TODO handle blob, arraybuffer, other content types, etc.\n  return text as unknown as T;\n}\n\n/**\n * A subclass of `Promise` providing additional helper methods\n * for interacting with the SDK.\n */\nexport class APIPromise<T> extends Promise<T> {\n  private parsedPromise: Promise<T> | undefined;\n\n  constructor(\n    private responsePromise: Promise<APIResponseProps>,\n    private parseResponse: (props: APIResponseProps) => PromiseOrValue<T> = defaultParseResponse,\n  ) {\n    super((resolve) => {\n      // this is maybe a bit weird but this has to be a no-op to not implicitly\n      // parse the response body; instead .then, .catch, .finally are overridden\n      // to parse the response\n      resolve(null as any);\n    });\n  }\n\n  _thenUnwrap<U>(transform: (data: T) => U): APIPromise<U> {\n    return new APIPromise(this.responsePromise, async (props) => transform(await this.parseResponse(props)));\n  }\n\n  /**\n   * Gets the raw `Response` instance instead of parsing the response\n   * data.\n   *\n   * If you want to parse the response body but still get the `Response`\n   * instance, you can use {@link withResponse()}.\n   *\n   *  Getting the wrong TypeScript type for `Response`?\n   * Try setting `\"moduleResolution\": \"NodeNext\"` if you can,\n   * or add one of these imports before your first `import  from 'openai'`:\n   * - `import 'openai/shims/node'` (if you're running on Node)\n   * - `import 'openai/shims/web'` (otherwise)\n   */\n  asResponse(): Promise<Response> {\n    return this.responsePromise.then((p) => p.response);\n  }\n  /**\n   * Gets the parsed response data and the raw `Response` instance.\n   *\n   * If you just want to get the raw `Response` instance without parsing it,\n   * you can use {@link asResponse()}.\n   *\n   *\n   *  Getting the wrong TypeScript type for `Response`?\n   * Try setting `\"moduleResolution\": \"NodeNext\"` if you can,\n   * or add one of these imports before your first `import  from 'openai'`:\n   * - `import 'openai/shims/node'` (if you're running on Node)\n   * - `import 'openai/shims/web'` (otherwise)\n   */\n  async withResponse(): Promise<{ data: T; response: Response }> {\n    const [data, response] = await Promise.all([this.parse(), this.asResponse()]);\n    return { data, response };\n  }\n\n  private parse(): Promise<T> {\n    if (!this.parsedPromise) {\n      this.parsedPromise = this.responsePromise.then(this.parseResponse);\n    }\n    return this.parsedPromise;\n  }\n\n  override then<TResult1 = T, TResult2 = never>(\n    onfulfilled?: ((value: T) => TResult1 | PromiseLike<TResult1>) | undefined | null,\n    onrejected?: ((reason: any) => TResult2 | PromiseLike<TResult2>) | undefined | null,\n  ): Promise<TResult1 | TResult2> {\n    return this.parse().then(onfulfilled, onrejected);\n  }\n\n  override catch<TResult = never>(\n    onrejected?: ((reason: any) => TResult | PromiseLike<TResult>) | undefined | null,\n  ): Promise<T | TResult> {\n    return this.parse().catch(onrejected);\n  }\n\n  override finally(onfinally?: (() => void) | undefined | null): Promise<T> {\n    return this.parse().finally(onfinally);\n  }\n}\n\nexport abstract class APIClient {\n  baseURL: string;\n  maxRetries: number;\n  timeout: number;\n  httpAgent: Agent | undefined;\n\n  private fetch: Fetch;\n  protected idempotencyHeader?: string;\n\n  constructor({\n    baseURL,\n    maxRetries = 2,\n    timeout = 600000, // 10 minutes\n    httpAgent,\n    fetch: overridenFetch,\n  }: {\n    baseURL: string;\n    maxRetries?: number | undefined;\n    timeout: number | undefined;\n    httpAgent: Agent | undefined;\n    fetch: Fetch | undefined;\n  }) {\n    this.baseURL = baseURL;\n    this.maxRetries = validatePositiveInteger('maxRetries', maxRetries);\n    this.timeout = validatePositiveInteger('timeout', timeout);\n    this.httpAgent = httpAgent;\n\n    this.fetch = overridenFetch ?? fetch;\n  }\n\n  protected authHeaders(opts: FinalRequestOptions): Headers {\n    return {};\n  }\n\n  /**\n   * Override this to add your own default headers, for example:\n   *\n   *  {\n   *    ...super.defaultHeaders(),\n   *    Authorization: 'Bearer 123',\n   *  }\n   */\n  protected defaultHeaders(opts: FinalRequestOptions): Headers {\n    return {\n      Accept: 'application/json',\n      'Content-Type': 'application/json',\n      'User-Agent': this.getUserAgent(),\n      ...getPlatformHeaders(),\n      ...this.authHeaders(opts),\n    };\n  }\n\n  protected abstract defaultQuery(): DefaultQuery | undefined;\n\n  /**\n   * Override this to add your own headers validation:\n   */\n  protected validateHeaders(headers: Headers, customHeaders: Headers) {}\n\n  protected defaultIdempotencyKey(): string {\n    return `stainless-node-retry-${uuid4()}`;\n  }\n\n  get<Req, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('get', path, opts);\n  }\n\n  post<Req, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('post', path, opts);\n  }\n\n  patch<Req, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('patch', path, opts);\n  }\n\n  put<Req, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('put', path, opts);\n  }\n\n  delete<Req, Rsp>(path: string, opts?: PromiseOrValue<RequestOptions<Req>>): APIPromise<Rsp> {\n    return this.methodRequest('delete', path, opts);\n  }\n\n  private methodRequest<Req, Rsp>(\n    method: HTTPMethod,\n    path: string,\n    opts?: PromiseOrValue<RequestOptions<Req>>,\n  ): APIPromise<Rsp> {\n    return this.request(Promise.resolve(opts).then((opts) => ({ method, path, ...opts })));\n  }\n\n  getAPIList<Item, PageClass extends AbstractPage<Item> = AbstractPage<Item>>(\n    path: string,\n    Page: new (...args: any[]) => PageClass,\n    opts?: RequestOptions<any>,\n  ): PagePromise<PageClass, Item> {\n    return this.requestAPIList(Page, { method: 'get', path, ...opts });\n  }\n\n  private calculateContentLength(body: unknown): string | null {\n    if (typeof body === 'string') {\n      if (typeof Buffer !== 'undefined') {\n        return Buffer.byteLength(body, 'utf8').toString();\n      }\n\n      if (typeof TextEncoder !== 'undefined') {\n        const encoder = new TextEncoder();\n        const encoded = encoder.encode(body);\n        return encoded.length.toString();\n      }\n    }\n\n    return null;\n  }\n\n  buildRequest<Req>(options: FinalRequestOptions<Req>): { req: RequestInit; url: string; timeout: number } {\n    const { method, path, query, headers: headers = {} } = options;\n\n    const body =\n      isMultipartBody(options.body) ? options.body.body\n      : options.body ? JSON.stringify(options.body, null, 2)\n      : null;\n    const contentLength = this.calculateContentLength(body);\n\n    const url = this.buildURL(path!, query);\n    if ('timeout' in options) validatePositiveInteger('timeout', options.timeout);\n    const timeout = options.timeout ?? this.timeout;\n    const httpAgent = options.httpAgent ?? this.httpAgent ?? getDefaultAgent(url);\n    const minAgentTimeout = timeout + 1000;\n    if (\n      typeof (httpAgent as any)?.options?.timeout === 'number' &&\n      minAgentTimeout > ((httpAgent as any).options.timeout ?? 0)\n    ) {\n      // Allow any given request to bump our agent active socket timeout.\n      // This may seem strange, but leaking active sockets should be rare and not particularly problematic,\n      // and without mutating agent we would need to create more of them.\n      // This tradeoff optimizes for performance.\n      (httpAgent as any).options.timeout = minAgentTimeout;\n    }\n\n    if (this.idempotencyHeader && method !== 'get') {\n      if (!options.idempotencyKey) options.idempotencyKey = this.defaultIdempotencyKey();\n      headers[this.idempotencyHeader] = options.idempotencyKey;\n    }\n\n    const reqHeaders = this.buildHeaders({ options, headers, contentLength });\n\n    const req: RequestInit = {\n      method,\n      ...(body && { body: body as any }),\n      headers: reqHeaders,\n      ...(httpAgent && { agent: httpAgent }),\n      // @ts-ignore node-fetch uses a custom AbortSignal type that is\n      // not compatible with standard web types\n      signal: options.signal ?? null,\n    };\n\n    return { req, url, timeout };\n  }\n\n  private buildHeaders({\n    options,\n    headers,\n    contentLength,\n  }: {\n    options: FinalRequestOptions;\n    headers: Record<string, string | null | undefined>;\n    contentLength: string | null | undefined;\n  }): Record<string, string> {\n    const reqHeaders: Record<string, string> = {};\n    if (contentLength) {\n      reqHeaders['content-length'] = contentLength;\n    }\n\n    const defaultHeaders = this.defaultHeaders(options);\n    applyHeadersMut(reqHeaders, defaultHeaders);\n    applyHeadersMut(reqHeaders, headers);\n\n    // let builtin fetch set the Content-Type for multipart bodies\n    if (isMultipartBody(options.body) && shimsKind !== 'node') {\n      delete reqHeaders['content-type'];\n    }\n\n    this.validateHeaders(reqHeaders, headers);\n\n    return reqHeaders;\n  }\n\n  /**\n   * Used as a callback for mutating the given `FinalRequestOptions` object.\n   */\n  protected async prepareOptions(options: FinalRequestOptions): Promise<void> {}\n\n  /**\n   * Used as a callback for mutating the given `RequestInit` object.\n   *\n   * This is useful for cases where you want to add certain headers based off of\n   * the request properties, e.g. `method` or `url`.\n   */\n  protected async prepareRequest(\n    request: RequestInit,\n    { url, options }: { url: string; options: FinalRequestOptions },\n  ): Promise<void> {}\n\n  protected parseHeaders(headers: HeadersInit | null | undefined): Record<string, string> {\n    return (\n      !headers ? {}\n      : Symbol.iterator in headers ?\n        Object.fromEntries(Array.from(headers as Iterable<string[]>).map((header) => [...header]))\n      : { ...headers }\n    );\n  }\n\n  protected makeStatusError(\n    status: number | undefined,\n    error: Object | undefined,\n    message: string | undefined,\n    headers: Headers | undefined,\n  ) {\n    return APIError.generate(status, error, message, headers);\n  }\n\n  request<Req, Rsp>(\n    options: PromiseOrValue<FinalRequestOptions<Req>>,\n    remainingRetries: number | null = null,\n  ): APIPromise<Rsp> {\n    return new APIPromise(this.makeRequest(options, remainingRetries));\n  }\n\n  private async makeRequest<Req>(\n    optionsInput: PromiseOrValue<FinalRequestOptions<Req>>,\n    retriesRemaining: number | null,\n  ): Promise<APIResponseProps> {\n    const options = await optionsInput;\n    if (retriesRemaining == null) {\n      retriesRemaining = options.maxRetries ?? this.maxRetries;\n    }\n\n    await this.prepareOptions(options);\n\n    const { req, url, timeout } = this.buildRequest(options);\n\n    await this.prepareRequest(req, { url, options });\n\n    debug('request', url, options, req.headers);\n\n    if (options.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n\n    const controller = new AbortController();\n    const response = await this.fetchWithTimeout(url, req, timeout, controller).catch(castToError);\n\n    if (response instanceof Error) {\n      if (options.signal?.aborted) {\n        throw new APIUserAbortError();\n      }\n      if (retriesRemaining) {\n        return this.retryRequest(options, retriesRemaining);\n      }\n      if (response.name === 'AbortError') {\n        throw new APIConnectionTimeoutError();\n      }\n      throw new APIConnectionError({ cause: response });\n    }\n\n    const responseHeaders = createResponseHeaders(response.headers);\n\n    if (!response.ok) {\n      if (retriesRemaining && this.shouldRetry(response)) {\n        const retryMessage = `retrying, ${retriesRemaining} attempts remaining`;\n        debug(`response (error; ${retryMessage})`, response.status, url, responseHeaders);\n        return this.retryRequest(options, retriesRemaining, responseHeaders);\n      }\n\n      const errText = await response.text().catch((e) => castToError(e).message);\n      const errJSON = safeJSON(errText);\n      const errMessage = errJSON ? undefined : errText;\n      const retryMessage = retriesRemaining ? `(error; no more retries left)` : `(error; not retryable)`;\n\n      debug(`response (error; ${retryMessage})`, response.status, url, responseHeaders, errMessage);\n\n      const err = this.makeStatusError(response.status, errJSON, errMessage, responseHeaders);\n      throw err;\n    }\n\n    return { response, options, controller };\n  }\n\n  requestAPIList<Item = unknown, PageClass extends AbstractPage<Item> = AbstractPage<Item>>(\n    Page: new (...args: ConstructorParameters<typeof AbstractPage>) => PageClass,\n    options: FinalRequestOptions,\n  ): PagePromise<PageClass, Item> {\n    const request = this.makeRequest(options, null);\n    return new PagePromise<PageClass, Item>(this, request, Page);\n  }\n\n  buildURL<Req>(path: string, query: Req | null | undefined): string {\n    const url =\n      isAbsoluteURL(path) ?\n        new URL(path)\n      : new URL(this.baseURL + (this.baseURL.endsWith('/') && path.startsWith('/') ? path.slice(1) : path));\n\n    const defaultQuery = this.defaultQuery();\n    if (!isEmptyObj(defaultQuery)) {\n      query = { ...defaultQuery, ...query } as Req;\n    }\n\n    if (typeof query === 'object' && query && !Array.isArray(query)) {\n      url.search = this.stringifyQuery(query as Record<string, unknown>);\n    }\n\n    return url.toString();\n  }\n\n  protected stringifyQuery(query: Record<string, unknown>): string {\n    return Object.entries(query)\n      .filter(([_, value]) => typeof value !== 'undefined')\n      .map(([key, value]) => {\n        if (typeof value === 'string' || typeof value === 'number' || typeof value === 'boolean') {\n          return `${encodeURIComponent(key)}=${encodeURIComponent(value)}`;\n        }\n        if (value === null) {\n          return `${encodeURIComponent(key)}=`;\n        }\n        throw new OpenAIError(\n          `Cannot stringify type ${typeof value}; Expected string, number, boolean, or null. If you need to pass nested query parameters, you can manually encode them, e.g. { query: { 'foo[key1]': value1, 'foo[key2]': value2 } }, and please open a GitHub issue requesting better support for your use case.`,\n        );\n      })\n      .join('&');\n  }\n\n  async fetchWithTimeout(\n    url: RequestInfo,\n    init: RequestInit | undefined,\n    ms: number,\n    controller: AbortController,\n  ): Promise<Response> {\n    const { signal, ...options } = init || {};\n    if (signal) signal.addEventListener('abort', () => controller.abort());\n\n    const timeout = setTimeout(() => controller.abort(), ms);\n\n    return (\n      this.getRequestClient()\n        // use undefined this binding; fetch errors if bound to something else in browser/cloudflare\n        .fetch.call(undefined, url, { signal: controller.signal as any, ...options })\n        .finally(() => {\n          clearTimeout(timeout);\n        })\n    );\n  }\n\n  protected getRequestClient(): RequestClient {\n    return { fetch: this.fetch };\n  }\n\n  private shouldRetry(response: Response): boolean {\n    // Note this is not a standard header.\n    const shouldRetryHeader = response.headers.get('x-should-retry');\n\n    // If the server explicitly says whether or not to retry, obey.\n    if (shouldRetryHeader === 'true') return true;\n    if (shouldRetryHeader === 'false') return false;\n\n    // Retry on request timeouts.\n    if (response.status === 408) return true;\n\n    // Retry on lock timeouts.\n    if (response.status === 409) return true;\n\n    // Retry on rate limits.\n    if (response.status === 429) return true;\n\n    // Retry internal errors.\n    if (response.status >= 500) return true;\n\n    return false;\n  }\n\n  private async retryRequest(\n    options: FinalRequestOptions,\n    retriesRemaining: number,\n    responseHeaders?: Headers | undefined,\n  ): Promise<APIResponseProps> {\n    let timeoutMillis: number | undefined;\n\n    // Note the `retry-after-ms` header may not be standard, but is a good idea and we'd like proactive support for it.\n    const retryAfterMillisHeader = responseHeaders?.['retry-after-ms'];\n    if (retryAfterMillisHeader) {\n      const timeoutMs = parseFloat(retryAfterMillisHeader);\n      if (!Number.isNaN(timeoutMs)) {\n        timeoutMillis = timeoutMs;\n      }\n    }\n\n    // About the Retry-After header: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After\n    const retryAfterHeader = responseHeaders?.['retry-after'];\n    if (retryAfterHeader && !timeoutMillis) {\n      const timeoutSeconds = parseFloat(retryAfterHeader);\n      if (!Number.isNaN(timeoutSeconds)) {\n        timeoutMillis = timeoutSeconds * 1000;\n      } else {\n        timeoutMillis = Date.parse(retryAfterHeader) - Date.now();\n      }\n    }\n\n    // If the API asks us to wait a certain amount of time (and it's a reasonable amount),\n    // just do what it says, but otherwise calculate a default\n    if (!(timeoutMillis && 0 <= timeoutMillis && timeoutMillis < 60 * 1000)) {\n      const maxRetries = options.maxRetries ?? this.maxRetries;\n      timeoutMillis = this.calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries);\n    }\n    await sleep(timeoutMillis);\n\n    return this.makeRequest(options, retriesRemaining - 1);\n  }\n\n  private calculateDefaultRetryTimeoutMillis(retriesRemaining: number, maxRetries: number): number {\n    const initialRetryDelay = 0.5;\n    const maxRetryDelay = 8.0;\n\n    const numRetries = maxRetries - retriesRemaining;\n\n    // Apply exponential backoff, but not more than the max.\n    const sleepSeconds = Math.min(initialRetryDelay * Math.pow(2, numRetries), maxRetryDelay);\n\n    // Apply some jitter, take up to at most 25 percent of the retry time.\n    const jitter = 1 - Math.random() * 0.25;\n\n    return sleepSeconds * jitter * 1000;\n  }\n\n  private getUserAgent(): string {\n    return `${this.constructor.name}/JS ${VERSION}`;\n  }\n}\n\nexport type PageInfo = { url: URL } | { params: Record<string, unknown> | null };\n\nexport abstract class AbstractPage<Item> implements AsyncIterable<Item> {\n  #client: APIClient;\n  protected options: FinalRequestOptions;\n\n  protected response: Response;\n  protected body: unknown;\n\n  constructor(client: APIClient, response: Response, body: unknown, options: FinalRequestOptions) {\n    this.#client = client;\n    this.options = options;\n    this.response = response;\n    this.body = body;\n  }\n\n  /**\n   * @deprecated Use nextPageInfo instead\n   */\n  abstract nextPageParams(): Partial<Record<string, unknown>> | null;\n  abstract nextPageInfo(): PageInfo | null;\n\n  abstract getPaginatedItems(): Item[];\n\n  hasNextPage(): boolean {\n    const items = this.getPaginatedItems();\n    if (!items.length) return false;\n    return this.nextPageInfo() != null;\n  }\n\n  async getNextPage(): Promise<this> {\n    const nextInfo = this.nextPageInfo();\n    if (!nextInfo) {\n      throw new OpenAIError(\n        'No next page expected; please check `.hasNextPage()` before calling `.getNextPage()`.',\n      );\n    }\n    const nextOptions = { ...this.options };\n    if ('params' in nextInfo && typeof nextOptions.query === 'object') {\n      nextOptions.query = { ...nextOptions.query, ...nextInfo.params };\n    } else if ('url' in nextInfo) {\n      const params = [...Object.entries(nextOptions.query || {}), ...nextInfo.url.searchParams.entries()];\n      for (const [key, value] of params) {\n        nextInfo.url.searchParams.set(key, value as any);\n      }\n      nextOptions.query = undefined;\n      nextOptions.path = nextInfo.url.toString();\n    }\n    return await this.#client.requestAPIList(this.constructor as any, nextOptions);\n  }\n\n  async *iterPages() {\n    // eslint-disable-next-line @typescript-eslint/no-this-alias\n    let page: AbstractPage<Item> = this;\n    yield page;\n    while (page.hasNextPage()) {\n      page = await page.getNextPage();\n      yield page;\n    }\n  }\n\n  async *[Symbol.asyncIterator]() {\n    for await (const page of this.iterPages()) {\n      for (const item of page.getPaginatedItems()) {\n        yield item;\n      }\n    }\n  }\n}\n\n/**\n * This subclass of Promise will resolve to an instantiated Page once the request completes.\n *\n * It also implements AsyncIterable to allow auto-paginating iteration on an unawaited list call, eg:\n *\n *    for await (const item of client.items.list()) {\n *      console.log(item)\n *    }\n */\nexport class PagePromise<\n    PageClass extends AbstractPage<Item>,\n    Item = ReturnType<PageClass['getPaginatedItems']>[number],\n  >\n  extends APIPromise<PageClass>\n  implements AsyncIterable<Item>\n{\n  constructor(\n    client: APIClient,\n    request: Promise<APIResponseProps>,\n    Page: new (...args: ConstructorParameters<typeof AbstractPage>) => PageClass,\n  ) {\n    super(\n      request,\n      async (props) => new Page(client, props.response, await defaultParseResponse(props), props.options),\n    );\n  }\n\n  /**\n   * Allow auto-paginating iteration on an unawaited list call, eg:\n   *\n   *    for await (const item of client.items.list()) {\n   *      console.log(item)\n   *    }\n   */\n  async *[Symbol.asyncIterator]() {\n    const page = await this;\n    for await (const item of page) {\n      yield item;\n    }\n  }\n}\n\nexport const createResponseHeaders = (\n  headers: Awaited<ReturnType<Fetch>>['headers'],\n): Record<string, string> => {\n  return new Proxy(\n    Object.fromEntries(\n      // @ts-ignore\n      headers.entries(),\n    ),\n    {\n      get(target, name) {\n        const key = name.toString();\n        return target[key.toLowerCase()] || target[key];\n      },\n    },\n  );\n};\n\ntype HTTPMethod = 'get' | 'post' | 'put' | 'patch' | 'delete';\n\nexport type RequestClient = { fetch: Fetch };\nexport type Headers = Record<string, string | null | undefined>;\nexport type DefaultQuery = Record<string, string | undefined>;\nexport type KeysEnum<T> = { [P in keyof Required<T>]: true };\n\nexport type RequestOptions<Req = unknown | Record<string, unknown> | Readable> = {\n  method?: HTTPMethod;\n  path?: string;\n  query?: Req | undefined;\n  body?: Req | null | undefined;\n  headers?: Headers | undefined;\n\n  maxRetries?: number;\n  stream?: boolean | undefined;\n  timeout?: number;\n  httpAgent?: Agent;\n  signal?: AbortSignal | undefined | null;\n  idempotencyKey?: string;\n\n  __binaryResponse?: boolean | undefined;\n  __streamClass?: typeof Stream;\n};\n\n// This is required so that we can determine if a given object matches the RequestOptions\n// type at runtime. While this requires duplication, it is enforced by the TypeScript\n// compiler such that any missing / extraneous keys will cause an error.\nconst requestOptionsKeys: KeysEnum<RequestOptions> = {\n  method: true,\n  path: true,\n  query: true,\n  body: true,\n  headers: true,\n\n  maxRetries: true,\n  stream: true,\n  timeout: true,\n  httpAgent: true,\n  signal: true,\n  idempotencyKey: true,\n\n  __binaryResponse: true,\n  __streamClass: true,\n};\n\nexport const isRequestOptions = (obj: unknown): obj is RequestOptions => {\n  return (\n    typeof obj === 'object' &&\n    obj !== null &&\n    !isEmptyObj(obj) &&\n    Object.keys(obj).every((k) => hasOwn(requestOptionsKeys, k))\n  );\n};\n\nexport type FinalRequestOptions<Req = unknown | Record<string, unknown> | Readable> = RequestOptions<Req> & {\n  method: HTTPMethod;\n  path: string;\n};\n\ndeclare const Deno: any;\ndeclare const EdgeRuntime: any;\ntype Arch = 'x32' | 'x64' | 'arm' | 'arm64' | `other:${string}` | 'unknown';\ntype PlatformName =\n  | 'MacOS'\n  | 'Linux'\n  | 'Windows'\n  | 'FreeBSD'\n  | 'OpenBSD'\n  | 'iOS'\n  | 'Android'\n  | `Other:${string}`\n  | 'Unknown';\ntype Browser = 'ie' | 'edge' | 'chrome' | 'firefox' | 'safari';\ntype PlatformProperties = {\n  'X-Stainless-Lang': 'js';\n  'X-Stainless-Package-Version': string;\n  'X-Stainless-OS': PlatformName;\n  'X-Stainless-Arch': Arch;\n  'X-Stainless-Runtime': 'node' | 'deno' | 'edge' | `browser:${Browser}` | 'unknown';\n  'X-Stainless-Runtime-Version': string;\n};\nconst getPlatformProperties = (): PlatformProperties => {\n  if (typeof Deno !== 'undefined' && Deno.build != null) {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': normalizePlatform(Deno.build.os),\n      'X-Stainless-Arch': normalizeArch(Deno.build.arch),\n      'X-Stainless-Runtime': 'deno',\n      'X-Stainless-Runtime-Version': Deno.version,\n    };\n  }\n  if (typeof EdgeRuntime !== 'undefined') {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': 'Unknown',\n      'X-Stainless-Arch': `other:${EdgeRuntime}`,\n      'X-Stainless-Runtime': 'edge',\n      'X-Stainless-Runtime-Version': process.version,\n    };\n  }\n  // Check if Node.js\n  if (Object.prototype.toString.call(typeof process !== 'undefined' ? process : 0) === '[object process]') {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': normalizePlatform(process.platform),\n      'X-Stainless-Arch': normalizeArch(process.arch),\n      'X-Stainless-Runtime': 'node',\n      'X-Stainless-Runtime-Version': process.version,\n    };\n  }\n\n  const browserInfo = getBrowserInfo();\n  if (browserInfo) {\n    return {\n      'X-Stainless-Lang': 'js',\n      'X-Stainless-Package-Version': VERSION,\n      'X-Stainless-OS': 'Unknown',\n      'X-Stainless-Arch': 'unknown',\n      'X-Stainless-Runtime': `browser:${browserInfo.browser}`,\n      'X-Stainless-Runtime-Version': browserInfo.version,\n    };\n  }\n\n  // TODO add support for Cloudflare workers, etc.\n  return {\n    'X-Stainless-Lang': 'js',\n    'X-Stainless-Package-Version': VERSION,\n    'X-Stainless-OS': 'Unknown',\n    'X-Stainless-Arch': 'unknown',\n    'X-Stainless-Runtime': 'unknown',\n    'X-Stainless-Runtime-Version': 'unknown',\n  };\n};\n\ntype BrowserInfo = {\n  browser: Browser;\n  version: string;\n};\n\ndeclare const navigator: { userAgent: string } | undefined;\n\n// Note: modified from https://github.com/JS-DevTools/host-environment/blob/b1ab79ecde37db5d6e163c050e54fe7d287d7c92/src/isomorphic.browser.ts\nfunction getBrowserInfo(): BrowserInfo | null {\n  if (typeof navigator === 'undefined' || !navigator) {\n    return null;\n  }\n\n  // NOTE: The order matters here!\n  const browserPatterns = [\n    { key: 'edge' as const, pattern: /Edge(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'ie' as const, pattern: /MSIE(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'ie' as const, pattern: /Trident(?:.*rv\\:(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'chrome' as const, pattern: /Chrome(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'firefox' as const, pattern: /Firefox(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n    { key: 'safari' as const, pattern: /(?:Version\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?(?:\\W+Mobile\\S*)?\\W+Safari/ },\n  ];\n\n  // Find the FIRST matching browser\n  for (const { key, pattern } of browserPatterns) {\n    const match = pattern.exec(navigator.userAgent);\n    if (match) {\n      const major = match[1] || 0;\n      const minor = match[2] || 0;\n      const patch = match[3] || 0;\n\n      return { browser: key, version: `${major}.${minor}.${patch}` };\n    }\n  }\n\n  return null;\n}\n\nconst normalizeArch = (arch: string): Arch => {\n  // Node docs:\n  // - https://nodejs.org/api/process.html#processarch\n  // Deno docs:\n  // - https://doc.deno.land/deno/stable/~/Deno.build\n  if (arch === 'x32') return 'x32';\n  if (arch === 'x86_64' || arch === 'x64') return 'x64';\n  if (arch === 'arm') return 'arm';\n  if (arch === 'aarch64' || arch === 'arm64') return 'arm64';\n  if (arch) return `other:${arch}`;\n  return 'unknown';\n};\n\nconst normalizePlatform = (platform: string): PlatformName => {\n  // Node platforms:\n  // - https://nodejs.org/api/process.html#processplatform\n  // Deno platforms:\n  // - https://doc.deno.land/deno/stable/~/Deno.build\n  // - https://github.com/denoland/deno/issues/14799\n\n  platform = platform.toLowerCase();\n\n  // NOTE: this iOS check is untested and may not work\n  // Node does not work natively on IOS, there is a fork at\n  // https://github.com/nodejs-mobile/nodejs-mobile\n  // however it is unknown at the time of writing how to detect if it is running\n  if (platform.includes('ios')) return 'iOS';\n  if (platform === 'android') return 'Android';\n  if (platform === 'darwin') return 'MacOS';\n  if (platform === 'win32') return 'Windows';\n  if (platform === 'freebsd') return 'FreeBSD';\n  if (platform === 'openbsd') return 'OpenBSD';\n  if (platform === 'linux') return 'Linux';\n  if (platform) return `Other:${platform}`;\n  return 'Unknown';\n};\n\nlet _platformHeaders: PlatformProperties;\nconst getPlatformHeaders = () => {\n  return (_platformHeaders ??= getPlatformProperties());\n};\n\nexport const safeJSON = (text: string) => {\n  try {\n    return JSON.parse(text);\n  } catch (err) {\n    return undefined;\n  }\n};\n\n// https://stackoverflow.com/a/19709846\nconst startsWithSchemeRegexp = new RegExp('^(?:[a-z]+:)?//', 'i');\nconst isAbsoluteURL = (url: string): boolean => {\n  return startsWithSchemeRegexp.test(url);\n};\n\nexport const sleep = (ms: number) => new Promise((resolve) => setTimeout(resolve, ms));\n\nconst validatePositiveInteger = (name: string, n: unknown): number => {\n  if (typeof n !== 'number' || !Number.isInteger(n)) {\n    throw new OpenAIError(`${name} must be an integer`);\n  }\n  if (n < 0) {\n    throw new OpenAIError(`${name} must be a positive integer`);\n  }\n  return n;\n};\n\nexport const castToError = (err: any): Error => {\n  if (err instanceof Error) return err;\n  return new Error(err);\n};\n\nexport const ensurePresent = <T>(value: T | null | undefined): T => {\n  if (value == null) throw new OpenAIError(`Expected a value to be given but received ${value} instead.`);\n  return value;\n};\n\n/**\n * Read an environment variable.\n *\n * Trims beginning and trailing whitespace.\n *\n * Will return undefined if the environment variable doesn't exist or cannot be accessed.\n */\nexport const readEnv = (env: string): string | undefined => {\n  if (typeof process !== 'undefined') {\n    return process.env?.[env]?.trim() ?? undefined;\n  }\n  if (typeof Deno !== 'undefined') {\n    return Deno.env?.get?.(env)?.trim();\n  }\n  return undefined;\n};\n\nexport const coerceInteger = (value: unknown): number => {\n  if (typeof value === 'number') return Math.round(value);\n  if (typeof value === 'string') return parseInt(value, 10);\n\n  throw new OpenAIError(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\n\nexport const coerceFloat = (value: unknown): number => {\n  if (typeof value === 'number') return value;\n  if (typeof value === 'string') return parseFloat(value);\n\n  throw new OpenAIError(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\n\nexport const coerceBoolean = (value: unknown): boolean => {\n  if (typeof value === 'boolean') return value;\n  if (typeof value === 'string') return value === 'true';\n  return Boolean(value);\n};\n\nexport const maybeCoerceInteger = (value: unknown): number | undefined => {\n  if (value === undefined) {\n    return undefined;\n  }\n  return coerceInteger(value);\n};\n\nexport const maybeCoerceFloat = (value: unknown): number | undefined => {\n  if (value === undefined) {\n    return undefined;\n  }\n  return coerceFloat(value);\n};\n\nexport const maybeCoerceBoolean = (value: unknown): boolean | undefined => {\n  if (value === undefined) {\n    return undefined;\n  }\n  return coerceBoolean(value);\n};\n\n// https://stackoverflow.com/a/34491287\nexport function isEmptyObj(obj: Object | null | undefined): boolean {\n  if (!obj) return true;\n  for (const _k in obj) return false;\n  return true;\n}\n\n// https://eslint.org/docs/latest/rules/no-prototype-builtins\nexport function hasOwn(obj: Object, key: string): boolean {\n  return Object.prototype.hasOwnProperty.call(obj, key);\n}\n\n/**\n * Copies headers from \"newHeaders\" onto \"targetHeaders\",\n * using lower-case for all properties,\n * ignoring any keys with undefined values,\n * and deleting any keys with null values.\n */\nfunction applyHeadersMut(targetHeaders: Headers, newHeaders: Headers): void {\n  for (const k in newHeaders) {\n    if (!hasOwn(newHeaders, k)) continue;\n    const lowerKey = k.toLowerCase();\n    if (!lowerKey) continue;\n\n    const val = newHeaders[k];\n\n    if (val === null) {\n      delete targetHeaders[lowerKey];\n    } else if (val !== undefined) {\n      targetHeaders[lowerKey] = val;\n    }\n  }\n}\n\nexport function debug(action: string, ...args: any[]) {\n  if (typeof process !== 'undefined' && process.env['DEBUG'] === 'true') {\n    console.log(`OpenAI:DEBUG:${action}`, ...args);\n  }\n}\n\n/**\n * https://stackoverflow.com/a/2117523\n */\nconst uuid4 = () => {\n  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, (c) => {\n    const r = (Math.random() * 16) | 0;\n    const v = c === 'x' ? r : (r & 0x3) | 0x8;\n    return v.toString(16);\n  });\n};\n\nexport const isRunningInBrowser = () => {\n  return (\n    // @ts-ignore\n    typeof window !== 'undefined' &&\n    // @ts-ignore\n    typeof window.document !== 'undefined' &&\n    // @ts-ignore\n    typeof navigator !== 'undefined'\n  );\n};\n\nexport interface HeadersProtocol {\n  get: (header: string) => string | null | undefined;\n}\nexport type HeadersLike = Record<string, string | string[] | undefined> | HeadersProtocol;\n\nexport const isHeadersProtocol = (headers: any): headers is HeadersProtocol => {\n  return typeof headers?.get === 'function';\n};\n\nexport const getRequiredHeader = (headers: HeadersLike, header: string): string => {\n  const lowerCasedHeader = header.toLowerCase();\n  if (isHeadersProtocol(headers)) {\n    // to deal with the case where the header looks like Stainless-Event-Id\n    const intercapsHeader =\n      header[0]?.toUpperCase() +\n      header.substring(1).replace(/([^\\w])(\\w)/g, (_m, g1, g2) => g1 + g2.toUpperCase());\n    for (const key of [header, lowerCasedHeader, header.toUpperCase(), intercapsHeader]) {\n      const value = headers.get(key);\n      if (value) {\n        return value;\n      }\n    }\n  }\n\n  for (const [key, value] of Object.entries(headers)) {\n    if (key.toLowerCase() === lowerCasedHeader) {\n      if (Array.isArray(value)) {\n        if (value.length <= 1) return value[0];\n        console.warn(`Received ${value.length} entries for the ${header} header, using the first entry.`);\n        return value[0];\n      }\n      return value;\n    }\n  }\n\n  throw new Error(`Could not find ${header} header`);\n};\n\n/**\n * Encodes a string to Base64 format.\n */\nexport const toBase64 = (str: string | null | undefined): string => {\n  if (!str) return '';\n  if (typeof Buffer !== 'undefined') {\n    return Buffer.from(str).toString('base64');\n  }\n\n  if (typeof btoa !== 'undefined') {\n    return btoa(str);\n  }\n\n  throw new OpenAIError('Cannot generate b64 string; Expected `Buffer` or `btoa` to be defined');\n};\n\nexport function isObj(obj: unknown): obj is Record<string, unknown> {\n  return obj != null && typeof obj === 'object' && !Array.isArray(obj);\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport { AbstractPage, Response, APIClient, FinalRequestOptions, PageInfo } from './core';\n\nexport interface PageResponse<Item> {\n  data: Array<Item>;\n\n  object: string;\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class Page<Item> extends AbstractPage<Item> implements PageResponse<Item> {\n  data: Array<Item>;\n\n  object: string;\n\n  constructor(client: APIClient, response: Response, body: PageResponse<Item>, options: FinalRequestOptions) {\n    super(client, response, body, options);\n\n    this.data = body.data || [];\n    this.object = body.object;\n  }\n\n  getPaginatedItems(): Item[] {\n    return this.data ?? [];\n  }\n\n  // @deprecated Please use `nextPageInfo()` instead\n  /**\n   * This page represents a response that isn't actually paginated at the API level\n   * so there will never be any next page params.\n   */\n  nextPageParams(): null {\n    return null;\n  }\n\n  nextPageInfo(): null {\n    return null;\n  }\n}\n\nexport interface CursorPageResponse<Item> {\n  data: Array<Item>;\n}\n\nexport interface CursorPageParams {\n  after?: string;\n\n  limit?: number;\n}\n\nexport class CursorPage<Item extends { id: string }>\n  extends AbstractPage<Item>\n  implements CursorPageResponse<Item>\n{\n  data: Array<Item>;\n\n  constructor(\n    client: APIClient,\n    response: Response,\n    body: CursorPageResponse<Item>,\n    options: FinalRequestOptions,\n  ) {\n    super(client, response, body, options);\n\n    this.data = body.data || [];\n  }\n\n  getPaginatedItems(): Item[] {\n    return this.data ?? [];\n  }\n\n  // @deprecated Please use `nextPageInfo()` instead\n  nextPageParams(): Partial<CursorPageParams> | null {\n    const info = this.nextPageInfo();\n    if (!info) return null;\n    if ('params' in info) return info.params;\n    const params = Object.fromEntries(info.url.searchParams);\n    if (!Object.keys(params).length) return null;\n    return params;\n  }\n\n  nextPageInfo(): PageInfo | null {\n    const data = this.getPaginatedItems();\n    if (!data.length) {\n      return null;\n    }\n\n    const id = data[data.length - 1]?.id;\n    if (!id) {\n      return null;\n    }\n\n    return { params: { after: id } };\n  }\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport type { OpenAI } from './index';\n\nexport class APIResource {\n  protected _client: OpenAI;\n\n  constructor(client: OpenAI) {\n    this._client = client;\n  }\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIPromise } from \"../core\";\nimport { APIResource } from \"../resource\";\nimport * as CompletionsAPI from \"./completions\";\nimport { Stream } from \"../streaming\";\n\nexport class Completions extends APIResource {\n  /**\n   * Creates a completion for the provided prompt and parameters.\n   */\n  create(body: CompletionCreateParamsNonStreaming, options?: Core.RequestOptions): APIPromise<Completion>;\n  create(\n    body: CompletionCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<Completion>>;\n  create(\n    body: CompletionCreateParamsBase,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<Completion> | Completion>;\n  create(\n    body: CompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<Completion> | APIPromise<Stream<Completion>> {\n    return this._client.post('/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<Completion>\n      | APIPromise<Stream<Completion>>;\n  }\n}\n\n/**\n * Represents a completion response from the API. Note: both the streamed and\n * non-streamed response objects share the same shape (unlike the chat endpoint).\n */\nexport interface Completion {\n  /**\n   * A unique identifier for the completion.\n   */\n  id: string;\n\n  /**\n   * The list of completion choices the model generated for the input prompt.\n   */\n  choices: Array<CompletionChoice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"text_completion\"\n   */\n  object: 'text_completion';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   *\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionUsage;\n}\n\nexport interface CompletionChoice {\n  /**\n   * The reason the model stopped generating tokens. This will be `stop` if the model\n   * hit a natural stop point or a provided stop sequence, `length` if the maximum\n   * number of tokens specified in the request was reached, or `content_filter` if\n   * content was omitted due to a flag from our content filters.\n   */\n  finish_reason: 'stop' | 'length' | 'content_filter';\n\n  index: number;\n\n  logprobs: CompletionChoice.Logprobs | null;\n\n  text: string;\n}\n\nexport namespace CompletionChoice {\n  export interface Logprobs {\n    text_offset?: Array<number>;\n\n    token_logprobs?: Array<number>;\n\n    tokens?: Array<string>;\n\n    top_logprobs?: Array<Record<string, number>>;\n  }\n}\n\n/**\n * Usage statistics for the completion request.\n */\nexport interface CompletionUsage {\n  /**\n   * Number of tokens in the generated completion.\n   */\n  completion_tokens: number;\n\n  /**\n   * Number of tokens in the prompt.\n   */\n  prompt_tokens: number;\n\n  /**\n   * Total number of tokens used in the request (prompt + completion).\n   */\n  total_tokens: number;\n}\n\nexport type CompletionCreateParams = CompletionCreateParamsNonStreaming | CompletionCreateParamsStreaming;\n\nexport interface CompletionCreateParamsBase {\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models/overview) for\n   * descriptions of them.\n   */\n  model: (string & {}) | 'gpt-3.5-turbo-instruct' | 'davinci-002' | 'babbage-002';\n\n  /**\n   * The prompt(s) to generate completions for, encoded as a string, array of\n   * strings, array of tokens, or array of token arrays.\n   *\n   * Note that <|endoftext|> is the document separator that the model sees during\n   * training, so if a prompt is not specified the model will generate as if from the\n   * beginning of a new document.\n   */\n  prompt: string | Array<string> | Array<number> | Array<Array<number>> | null;\n\n  /**\n   * Generates `best_of` completions server-side and returns the \"best\" (the one with\n   * the highest log probability per token). Results cannot be streamed.\n   *\n   * When used with `n`, `best_of` controls the number of candidate completions and\n   * `n` specifies how many to return  `best_of` must be greater than `n`.\n   *\n   * **Note:** Because this parameter generates many completions, it can quickly\n   * consume your token quota. Use carefully and ensure that you have reasonable\n   * settings for `max_tokens` and `stop`.\n   */\n  best_of?: number | null;\n\n  /**\n   * Echo back the prompt in addition to the completion\n   */\n  echo?: boolean | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in the GPT\n   * tokenizer) to an associated bias value from -100 to 100. You can use this\n   * [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.\n   * Mathematically, the bias is added to the logits generated by the model prior to\n   * sampling. The exact effect will vary per model, but values between -1 and 1\n   * should decrease or increase likelihood of selection; values like -100 or 100\n   * should result in a ban or exclusive selection of the relevant token.\n   *\n   * As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token\n   * from being generated.\n   */\n  logit_bias?: Record<string, number> | null;\n\n  /**\n   * Include the log probabilities on the `logprobs` most likely output tokens, as\n   * well the chosen tokens. For example, if `logprobs` is 5, the API will return a\n   * list of the 5 most likely tokens. The API will always return the `logprob` of\n   * the sampled token, so there may be up to `logprobs+1` elements in the response.\n   *\n   * The maximum value for `logprobs` is 5.\n   */\n  logprobs?: number | null;\n\n  /**\n   * The maximum number of [tokens](/tokenizer) that can be generated in the\n   * completion.\n   *\n   * The token count of your prompt plus `max_tokens` cannot exceed the model's\n   * context length.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n   * for counting tokens.\n   */\n  max_tokens?: number | null;\n\n  /**\n   * How many completions to generate for each prompt.\n   *\n   * **Note:** Because this parameter generates many completions, it can quickly\n   * consume your token quota. Use carefully and ensure that you have reasonable\n   * settings for `max_tokens` and `stop`.\n   */\n  n?: number | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * If specified, our system will make a best effort to sample deterministically,\n   * such that repeated requests with the same `seed` and parameters should return\n   * the same result.\n   *\n   * Determinism is not guaranteed, and you should refer to the `system_fingerprint`\n   * response parameter to monitor changes in the backend.\n   */\n  seed?: number | null;\n\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens. The\n   * returned text will not contain the stop sequence.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: boolean | null;\n\n  /**\n   * The suffix that comes after a completion of inserted text.\n   */\n  suffix?: string | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace CompletionCreateParams {\n  export type CompletionCreateParamsNonStreaming = CompletionsAPI.CompletionCreateParamsNonStreaming;\n  export type CompletionCreateParamsStreaming = CompletionsAPI.CompletionCreateParamsStreaming;\n}\n\nexport interface CompletionCreateParamsNonStreaming extends CompletionCreateParamsBase {\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: false | null;\n}\n\nexport interface CompletionCreateParamsStreaming extends CompletionCreateParamsBase {\n  /**\n   * Whether to stream back partial progress. If set, tokens will be sent as\n   * data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream: true;\n}\n\nexport namespace Completions {\n  export import Completion = CompletionsAPI.Completion;\n  export import CompletionChoice = CompletionsAPI.CompletionChoice;\n  export import CompletionUsage = CompletionsAPI.CompletionUsage;\n  export import CompletionCreateParams = CompletionsAPI.CompletionCreateParams;\n  export import CompletionCreateParamsNonStreaming = CompletionsAPI.CompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsStreaming = CompletionsAPI.CompletionCreateParamsStreaming;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../core\";\nimport { APIPromise } from \"../../core\";\nimport { APIResource } from \"../../resource\";\nimport * as ChatCompletionsAPI from \"./completions\";\nimport * as CompletionsAPI from \"../completions\";\nimport * as Shared from \"../shared\";\nimport { Stream } from \"../../streaming\";\n\nexport class Completions extends APIResource {\n  /**\n   * Creates a model response for the given chat conversation.\n   */\n  create(\n    body: ChatCompletionCreateParamsNonStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk>>;\n  create(\n    body: ChatCompletionCreateParamsBase,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk> | ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<ChatCompletion> | APIPromise<Stream<ChatCompletionChunk>> {\n    return this._client.post('/chat/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<ChatCompletion>\n      | APIPromise<Stream<ChatCompletionChunk>>;\n  }\n}\n\n/**\n * Represents a chat completion response returned by model, based on the provided\n * input.\n */\nexport interface ChatCompletion {\n  /**\n   * A unique identifier for the chat completion.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletion.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for the chat completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion`.\n   */\n  object: 'chat.completion';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   *\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionsAPI.CompletionUsage;\n}\n\nexport namespace ChatCompletion {\n  export interface Choice {\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, `tool_calls` if the\n     * model called a tool, or `function_call` (deprecated) if the model called a\n     * function.\n     */\n    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs: Choice.Logprobs | null;\n\n    /**\n     * A chat completion message generated by the model.\n     */\n    message: ChatCompletionsAPI.ChatCompletionMessage;\n  }\n\n  export namespace Choice {\n    /**\n     * Log probability information for the choice.\n     */\n    export interface Logprobs {\n      /**\n       * A list of message content tokens with log probability information.\n       */\n      content: Array<ChatCompletionsAPI.ChatCompletionTokenLogprob> | null;\n    }\n  }\n}\n\nexport interface ChatCompletionAssistantMessageParam {\n  /**\n   * The role of the messages author, in this case `assistant`.\n   */\n  role: 'assistant';\n\n  /**\n   * The contents of the assistant message. Required unless `tool_calls` or\n   * `function_call` is specified.\n   */\n  content?: string | null;\n\n  /**\n   * Deprecated and replaced by `tool_calls`. The name and arguments of a function\n   * that should be called, as generated by the model.\n   */\n  function_call?: ChatCompletionAssistantMessageParam.FunctionCall;\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: Array<ChatCompletionMessageToolCall>;\n}\n\nexport namespace ChatCompletionAssistantMessageParam {\n  /**\n   * Deprecated and replaced by `tool_calls`. The name and arguments of a function\n   * that should be called, as generated by the model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * Represents a streamed chunk of a chat completion response returned by model,\n * based on the provided input.\n */\nexport interface ChatCompletionChunk {\n  /**\n   * A unique identifier for the chat completion. Each chunk has the same ID.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletionChunk.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created. Each\n   * chunk has the same timestamp.\n   */\n  created: number;\n\n  /**\n   * The model to generate the completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion.chunk`.\n   */\n  object: 'chat.completion.chunk';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n}\n\nexport namespace ChatCompletionChunk {\n  export interface Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    delta: Choice.Delta;\n\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, `tool_calls` if the\n     * model called a tool, or `function_call` (deprecated) if the model called a\n     * function.\n     */\n    finish_reason: 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call' | null;\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs?: Choice.Logprobs | null;\n  }\n\n  export namespace Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    export interface Delta {\n      /**\n       * The contents of the chunk message.\n       */\n      content?: string | null;\n\n      /**\n       * Deprecated and replaced by `tool_calls`. The name and arguments of a function\n       * that should be called, as generated by the model.\n       */\n      function_call?: Delta.FunctionCall;\n\n      /**\n       * The role of the author of this message.\n       */\n      role?: 'system' | 'user' | 'assistant' | 'tool';\n\n      tool_calls?: Array<Delta.ToolCall>;\n    }\n\n    export namespace Delta {\n      /**\n       * Deprecated and replaced by `tool_calls`. The name and arguments of a function\n       * that should be called, as generated by the model.\n       */\n      export interface FunctionCall {\n        /**\n         * The arguments to call the function with, as generated by the model in JSON\n         * format. Note that the model does not always generate valid JSON, and may\n         * hallucinate parameters not defined by your function schema. Validate the\n         * arguments in your code before calling your function.\n         */\n        arguments?: string;\n\n        /**\n         * The name of the function to call.\n         */\n        name?: string;\n      }\n\n      export interface ToolCall {\n        index: number;\n\n        /**\n         * The ID of the tool call.\n         */\n        id?: string;\n\n        function?: ToolCall.Function;\n\n        /**\n         * The type of the tool. Currently, only `function` is supported.\n         */\n        type?: 'function';\n      }\n\n      export namespace ToolCall {\n        export interface Function {\n          /**\n           * The arguments to call the function with, as generated by the model in JSON\n           * format. Note that the model does not always generate valid JSON, and may\n           * hallucinate parameters not defined by your function schema. Validate the\n           * arguments in your code before calling your function.\n           */\n          arguments?: string;\n\n          /**\n           * The name of the function to call.\n           */\n          name?: string;\n        }\n      }\n    }\n\n    /**\n     * Log probability information for the choice.\n     */\n    export interface Logprobs {\n      /**\n       * A list of message content tokens with log probability information.\n       */\n      content: Array<ChatCompletionsAPI.ChatCompletionTokenLogprob> | null;\n    }\n  }\n}\n\nexport type ChatCompletionContentPart = ChatCompletionContentPartText | ChatCompletionContentPartImage;\n\nexport interface ChatCompletionContentPartImage {\n  image_url: ChatCompletionContentPartImage.ImageURL;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'image_url';\n}\n\nexport namespace ChatCompletionContentPartImage {\n  export interface ImageURL {\n    /**\n     * Either a URL of the image or the base64 encoded image data.\n     */\n    url: string;\n\n    /**\n     * Specifies the detail level of the image. Learn more in the\n     * [Vision guide](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding).\n     */\n    detail?: 'auto' | 'low' | 'high';\n  }\n}\n\nexport interface ChatCompletionContentPartText {\n  /**\n   * The text content.\n   */\n  text: string;\n\n  /**\n   * The type of the content part.\n   */\n  type: 'text';\n}\n\n/**\n * Specifying a particular function via `{\"name\": \"my_function\"}` forces the model\n * to call that function.\n */\nexport interface ChatCompletionFunctionCallOption {\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n}\n\nexport interface ChatCompletionFunctionMessageParam {\n  /**\n   * The contents of the function message.\n   */\n  content: string | null;\n\n  /**\n   * The name of the function to call.\n   */\n  name: string;\n\n  /**\n   * The role of the messages author, in this case `function`.\n   */\n  role: 'function';\n}\n\n/**\n * A chat completion message generated by the model.\n */\nexport interface ChatCompletionMessage {\n  /**\n   * The contents of the message.\n   */\n  content: string | null;\n\n  /**\n   * The role of the author of this message.\n   */\n  role: 'assistant';\n\n  /**\n   * Deprecated and replaced by `tool_calls`. The name and arguments of a function\n   * that should be called, as generated by the model.\n   */\n  function_call?: ChatCompletionMessage.FunctionCall;\n\n  /**\n   * The tool calls generated by the model, such as function calls.\n   */\n  tool_calls?: Array<ChatCompletionMessageToolCall>;\n}\n\nexport namespace ChatCompletionMessage {\n  /**\n   * Deprecated and replaced by `tool_calls`. The name and arguments of a function\n   * that should be called, as generated by the model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\nexport type ChatCompletionMessageParam =\n  | ChatCompletionSystemMessageParam\n  | ChatCompletionUserMessageParam\n  | ChatCompletionAssistantMessageParam\n  | ChatCompletionToolMessageParam\n  | ChatCompletionFunctionMessageParam;\n\nexport interface ChatCompletionMessageToolCall {\n  /**\n   * The ID of the tool call.\n   */\n  id: string;\n\n  /**\n   * The function that the model called.\n   */\n  function: ChatCompletionMessageToolCall.Function;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\nexport namespace ChatCompletionMessageToolCall {\n  /**\n   * The function that the model called.\n   */\n  export interface Function {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * Specifies a tool the model should use. Use to force the model to call a specific\n * function.\n */\nexport interface ChatCompletionNamedToolChoice {\n  function: ChatCompletionNamedToolChoice.Function;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\nexport namespace ChatCompletionNamedToolChoice {\n  export interface Function {\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * The role of the author of a message\n */\nexport type ChatCompletionRole = 'system' | 'user' | 'assistant' | 'tool' | 'function';\n\nexport interface ChatCompletionSystemMessageParam {\n  /**\n   * The contents of the system message.\n   */\n  content: string;\n\n  /**\n   * The role of the messages author, in this case `system`.\n   */\n  role: 'system';\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n}\n\nexport interface ChatCompletionTokenLogprob {\n  /**\n   * The token.\n   */\n  token: string;\n\n  /**\n   * A list of integers representing the UTF-8 bytes representation of the token.\n   * Useful in instances where characters are represented by multiple tokens and\n   * their byte representations must be combined to generate the correct text\n   * representation. Can be `null` if there is no bytes representation for the token.\n   */\n  bytes: Array<number> | null;\n\n  /**\n   * The log probability of this token.\n   */\n  logprob: number;\n\n  /**\n   * List of the most likely tokens and their log probability, at this token\n   * position. In rare cases, there may be fewer than the number of requested\n   * `top_logprobs` returned.\n   */\n  top_logprobs: Array<ChatCompletionTokenLogprob.TopLogprob>;\n}\n\nexport namespace ChatCompletionTokenLogprob {\n  export interface TopLogprob {\n    /**\n     * The token.\n     */\n    token: string;\n\n    /**\n     * A list of integers representing the UTF-8 bytes representation of the token.\n     * Useful in instances where characters are represented by multiple tokens and\n     * their byte representations must be combined to generate the correct text\n     * representation. Can be `null` if there is no bytes representation for the token.\n     */\n    bytes: Array<number> | null;\n\n    /**\n     * The log probability of this token.\n     */\n    logprob: number;\n  }\n}\n\nexport interface ChatCompletionTool {\n  function: Shared.FunctionDefinition;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type: 'function';\n}\n\n/**\n * Controls which (if any) function is called by the model. `none` means the model\n * will not call a function and instead generates a message. `auto` means the model\n * can pick between generating a message or calling a function. Specifying a\n * particular function via\n * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n * call that function.\n *\n * `none` is the default when no functions are present. `auto` is the default if\n * functions are present.\n */\nexport type ChatCompletionToolChoiceOption = 'none' | 'auto' | ChatCompletionNamedToolChoice;\n\nexport interface ChatCompletionToolMessageParam {\n  /**\n   * The contents of the tool message.\n   */\n  content: string;\n\n  /**\n   * The role of the messages author, in this case `tool`.\n   */\n  role: 'tool';\n\n  /**\n   * Tool call that this message is responding to.\n   */\n  tool_call_id: string;\n}\n\nexport interface ChatCompletionUserMessageParam {\n  /**\n   * The contents of the user message.\n   */\n  content: string | Array<ChatCompletionContentPart>;\n\n  /**\n   * The role of the messages author, in this case `user`.\n   */\n  role: 'user';\n\n  /**\n   * An optional name for the participant. Provides the model information to\n   * differentiate between participants of the same role.\n   */\n  name?: string;\n}\n\n/**\n * @deprecated ChatCompletionMessageParam should be used instead\n */\nexport type CreateChatCompletionRequestMessage = ChatCompletionMessageParam;\n\nexport type ChatCompletionCreateParams =\n  | ChatCompletionCreateParamsNonStreaming\n  | ChatCompletionCreateParamsStreaming;\n\nexport interface ChatCompletionCreateParamsBase {\n  /**\n   * A list of messages comprising the conversation so far.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).\n   */\n  messages: Array<ChatCompletionMessageParam>;\n\n  /**\n   * ID of the model to use. See the\n   * [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)\n   * table for details on which models work with the Chat API.\n   */\n  model:\n    | (string & {})\n    | 'gpt-4-0125-preview'\n    | 'gpt-4-turbo-preview'\n    | 'gpt-4-1106-preview'\n    | 'gpt-4-vision-preview'\n    | 'gpt-4'\n    | 'gpt-4-0314'\n    | 'gpt-4-0613'\n    | 'gpt-4-32k'\n    | 'gpt-4-32k-0314'\n    | 'gpt-4-32k-0613'\n    | 'gpt-3.5-turbo'\n    | 'gpt-3.5-turbo-16k'\n    | 'gpt-3.5-turbo-0301'\n    | 'gpt-3.5-turbo-0613'\n    | 'gpt-3.5-turbo-1106'\n    | 'gpt-3.5-turbo-16k-0613';\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * Deprecated in favor of `tool_choice`.\n   *\n   * Controls which (if any) function is called by the model. `none` means the model\n   * will not call a function and instead generates a message. `auto` means the model\n   * can pick between generating a message or calling a function. Specifying a\n   * particular function via `{\"name\": \"my_function\"}` forces the model to call that\n   * function.\n   *\n   * `none` is the default when no functions are present. `auto` is the default if\n   * functions are present.\n   */\n  function_call?: 'none' | 'auto' | ChatCompletionFunctionCallOption;\n\n  /**\n   * Deprecated in favor of `tools`.\n   *\n   * A list of functions the model may generate JSON inputs for.\n   */\n  functions?: Array<ChatCompletionCreateParams.Function>;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a JSON object that maps tokens (specified by their token ID in the\n   * tokenizer) to an associated bias value from -100 to 100. Mathematically, the\n   * bias is added to the logits generated by the model prior to sampling. The exact\n   * effect will vary per model, but values between -1 and 1 should decrease or\n   * increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: Record<string, number> | null;\n\n  /**\n   * Whether to return log probabilities of the output tokens or not. If true,\n   * returns the log probabilities of each output token returned in the `content` of\n   * `message`. This option is currently not available on the `gpt-4-vision-preview`\n   * model.\n   */\n  logprobs?: boolean | null;\n\n  /**\n   * The maximum number of [tokens](/tokenizer) that can be generated in the chat\n   * completion.\n   *\n   * The total length of input tokens and generated tokens is limited by the model's\n   * context length.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n   * for counting tokens.\n   */\n  max_tokens?: number | null;\n\n  /**\n   * How many chat completion choices to generate for each input message. Note that\n   * you will be charged based on the number of generated tokens across all of the\n   * choices. Keep `n` as `1` to minimize costs.\n   */\n  n?: number | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * An object specifying the format that the model must output. Compatible with\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) and\n   * `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  response_format?: ChatCompletionCreateParams.ResponseFormat;\n\n  /**\n   * This feature is in Beta. If specified, our system will make a best effort to\n   * sample deterministically, such that repeated requests with the same `seed` and\n   * parameters should return the same result. Determinism is not guaranteed, and you\n   * should refer to the `system_fingerprint` response parameter to monitor changes\n   * in the backend.\n   */\n  seed?: number | null;\n\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: boolean | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * Controls which (if any) function is called by the model. `none` means the model\n   * will not call a function and instead generates a message. `auto` means the model\n   * can pick between generating a message or calling a function. Specifying a\n   * particular function via\n   * `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n   * call that function.\n   *\n   * `none` is the default when no functions are present. `auto` is the default if\n   * functions are present.\n   */\n  tool_choice?: ChatCompletionToolChoiceOption;\n\n  /**\n   * A list of tools the model may call. Currently, only functions are supported as a\n   * tool. Use this to provide a list of functions the model may generate JSON inputs\n   * for.\n   */\n  tools?: Array<ChatCompletionTool>;\n\n  /**\n   * An integer between 0 and 5 specifying the number of most likely tokens to return\n   * at each token position, each with an associated log probability. `logprobs` must\n   * be set to `true` if this parameter is used.\n   */\n  top_logprobs?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace ChatCompletionCreateParams {\n  export interface Function {\n    /**\n     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain\n     * underscores and dashes, with a maximum length of 64.\n     */\n    name: string;\n\n    /**\n     * A description of what the function does, used by the model to choose when and\n     * how to call the function.\n     */\n    description?: string;\n\n    /**\n     * The parameters the functions accepts, described as a JSON Schema object. See the\n     * [guide](https://platform.openai.com/docs/guides/text-generation/function-calling)\n     * for examples, and the\n     * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n     * documentation about the format.\n     *\n     * Omitting `parameters` defines a function with an empty parameter list.\n     */\n    parameters?: Shared.FunctionParameters;\n  }\n\n  /**\n   * An object specifying the format that the model must output. Compatible with\n   * [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) and\n   * `gpt-3.5-turbo-1106`.\n   *\n   * Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which guarantees the\n   * message the model generates is valid JSON.\n   *\n   * **Important:** when using JSON mode, you **must** also instruct the model to\n   * produce JSON yourself via a system or user message. Without this, the model may\n   * generate an unending stream of whitespace until the generation reaches the token\n   * limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\n   * the message content may be partially cut off if `finish_reason=\"length\"`, which\n   * indicates the generation exceeded `max_tokens` or the conversation exceeded the\n   * max context length.\n   */\n  export interface ResponseFormat {\n    /**\n     * Must be one of `text` or `json_object`.\n     */\n    type?: 'text' | 'json_object';\n  }\n\n  export type ChatCompletionCreateParamsNonStreaming =\n    ChatCompletionsAPI.ChatCompletionCreateParamsNonStreaming;\n  export type ChatCompletionCreateParamsStreaming = ChatCompletionsAPI.ChatCompletionCreateParamsStreaming;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParams instead\n */\nexport type CompletionCreateParams = ChatCompletionCreateParams;\n\nexport interface ChatCompletionCreateParamsNonStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream?: false | null;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParamsNonStreaming instead\n */\nexport type CompletionCreateParamsNonStreaming = ChatCompletionCreateParamsNonStreaming;\n\nexport interface ChatCompletionCreateParamsStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n   */\n  stream: true;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParamsStreaming instead\n */\nexport type CompletionCreateParamsStreaming = ChatCompletionCreateParamsStreaming;\n\nexport namespace Completions {\n  export import ChatCompletion = ChatCompletionsAPI.ChatCompletion;\n  export import ChatCompletionAssistantMessageParam = ChatCompletionsAPI.ChatCompletionAssistantMessageParam;\n  export import ChatCompletionChunk = ChatCompletionsAPI.ChatCompletionChunk;\n  export import ChatCompletionContentPart = ChatCompletionsAPI.ChatCompletionContentPart;\n  export import ChatCompletionContentPartImage = ChatCompletionsAPI.ChatCompletionContentPartImage;\n  export import ChatCompletionContentPartText = ChatCompletionsAPI.ChatCompletionContentPartText;\n  export import ChatCompletionFunctionCallOption = ChatCompletionsAPI.ChatCompletionFunctionCallOption;\n  export import ChatCompletionFunctionMessageParam = ChatCompletionsAPI.ChatCompletionFunctionMessageParam;\n  export import ChatCompletionMessage = ChatCompletionsAPI.ChatCompletionMessage;\n  export import ChatCompletionMessageParam = ChatCompletionsAPI.ChatCompletionMessageParam;\n  export import ChatCompletionMessageToolCall = ChatCompletionsAPI.ChatCompletionMessageToolCall;\n  export import ChatCompletionNamedToolChoice = ChatCompletionsAPI.ChatCompletionNamedToolChoice;\n  export import ChatCompletionRole = ChatCompletionsAPI.ChatCompletionRole;\n  export import ChatCompletionSystemMessageParam = ChatCompletionsAPI.ChatCompletionSystemMessageParam;\n  export import ChatCompletionTokenLogprob = ChatCompletionsAPI.ChatCompletionTokenLogprob;\n  export import ChatCompletionTool = ChatCompletionsAPI.ChatCompletionTool;\n  export import ChatCompletionToolChoiceOption = ChatCompletionsAPI.ChatCompletionToolChoiceOption;\n  export import ChatCompletionToolMessageParam = ChatCompletionsAPI.ChatCompletionToolMessageParam;\n  export import ChatCompletionUserMessageParam = ChatCompletionsAPI.ChatCompletionUserMessageParam;\n  /**\n   * @deprecated ChatCompletionMessageParam should be used instead\n   */\n  export import CreateChatCompletionRequestMessage = ChatCompletionsAPI.CreateChatCompletionRequestMessage;\n  export import ChatCompletionCreateParams = ChatCompletionsAPI.ChatCompletionCreateParams;\n  export import CompletionCreateParams = ChatCompletionsAPI.CompletionCreateParams;\n  export import ChatCompletionCreateParamsNonStreaming = ChatCompletionsAPI.ChatCompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsNonStreaming = ChatCompletionsAPI.CompletionCreateParamsNonStreaming;\n  export import ChatCompletionCreateParamsStreaming = ChatCompletionsAPI.ChatCompletionCreateParamsStreaming;\n  export import CompletionCreateParamsStreaming = ChatCompletionsAPI.CompletionCreateParamsStreaming;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport { APIResource } from \"../../resource\";\nimport * as CompletionsAPI from \"./completions\";\n\nexport class Chat extends APIResource {\n  completions: CompletionsAPI.Completions = new CompletionsAPI.Completions(this._client);\n}\n\nexport namespace Chat {\n  export import Completions = CompletionsAPI.Completions;\n  export import ChatCompletion = CompletionsAPI.ChatCompletion;\n  export import ChatCompletionAssistantMessageParam = CompletionsAPI.ChatCompletionAssistantMessageParam;\n  export import ChatCompletionChunk = CompletionsAPI.ChatCompletionChunk;\n  export import ChatCompletionContentPart = CompletionsAPI.ChatCompletionContentPart;\n  export import ChatCompletionContentPartImage = CompletionsAPI.ChatCompletionContentPartImage;\n  export import ChatCompletionContentPartText = CompletionsAPI.ChatCompletionContentPartText;\n  export import ChatCompletionFunctionCallOption = CompletionsAPI.ChatCompletionFunctionCallOption;\n  export import ChatCompletionFunctionMessageParam = CompletionsAPI.ChatCompletionFunctionMessageParam;\n  export import ChatCompletionMessage = CompletionsAPI.ChatCompletionMessage;\n  export import ChatCompletionMessageParam = CompletionsAPI.ChatCompletionMessageParam;\n  export import ChatCompletionMessageToolCall = CompletionsAPI.ChatCompletionMessageToolCall;\n  export import ChatCompletionNamedToolChoice = CompletionsAPI.ChatCompletionNamedToolChoice;\n  export import ChatCompletionRole = CompletionsAPI.ChatCompletionRole;\n  export import ChatCompletionSystemMessageParam = CompletionsAPI.ChatCompletionSystemMessageParam;\n  export import ChatCompletionTokenLogprob = CompletionsAPI.ChatCompletionTokenLogprob;\n  export import ChatCompletionTool = CompletionsAPI.ChatCompletionTool;\n  export import ChatCompletionToolChoiceOption = CompletionsAPI.ChatCompletionToolChoiceOption;\n  export import ChatCompletionToolMessageParam = CompletionsAPI.ChatCompletionToolMessageParam;\n  export import ChatCompletionUserMessageParam = CompletionsAPI.ChatCompletionUserMessageParam;\n  /**\n   * @deprecated ChatCompletionMessageParam should be used instead\n   */\n  export import CreateChatCompletionRequestMessage = CompletionsAPI.CreateChatCompletionRequestMessage;\n  export import ChatCompletionCreateParams = CompletionsAPI.ChatCompletionCreateParams;\n  export import CompletionCreateParams = CompletionsAPI.CompletionCreateParams;\n  export import ChatCompletionCreateParamsNonStreaming = CompletionsAPI.ChatCompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsNonStreaming = CompletionsAPI.CompletionCreateParamsNonStreaming;\n  export import ChatCompletionCreateParamsStreaming = CompletionsAPI.ChatCompletionCreateParamsStreaming;\n  export import CompletionCreateParamsStreaming = CompletionsAPI.CompletionCreateParamsStreaming;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIResource } from \"../resource\";\nimport * as EmbeddingsAPI from \"./embeddings\";\n\nexport class Embeddings extends APIResource {\n  /**\n   * Creates an embedding vector representing the input text.\n   */\n  create(\n    body: EmbeddingCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<CreateEmbeddingResponse> {\n    return this._client.post('/embeddings', { body, ...options });\n  }\n}\n\nexport interface CreateEmbeddingResponse {\n  /**\n   * The list of embeddings generated by the model.\n   */\n  data: Array<Embedding>;\n\n  /**\n   * The name of the model used to generate the embedding.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"list\".\n   */\n  object: 'list';\n\n  /**\n   * The usage information for the request.\n   */\n  usage: CreateEmbeddingResponse.Usage;\n}\n\nexport namespace CreateEmbeddingResponse {\n  /**\n   * The usage information for the request.\n   */\n  export interface Usage {\n    /**\n     * The number of tokens used by the prompt.\n     */\n    prompt_tokens: number;\n\n    /**\n     * The total number of tokens used by the request.\n     */\n    total_tokens: number;\n  }\n}\n\n/**\n * Represents an embedding vector returned by embedding endpoint.\n */\nexport interface Embedding {\n  /**\n   * The embedding vector, which is a list of floats. The length of vector depends on\n   * the model as listed in the\n   * [embedding guide](https://platform.openai.com/docs/guides/embeddings).\n   */\n  embedding: Array<number>;\n\n  /**\n   * The index of the embedding in the list of embeddings.\n   */\n  index: number;\n\n  /**\n   * The object type, which is always \"embedding\".\n   */\n  object: 'embedding';\n}\n\nexport interface EmbeddingCreateParams {\n  /**\n   * Input text to embed, encoded as a string or array of tokens. To embed multiple\n   * inputs in a single request, pass an array of strings or array of token arrays.\n   * The input must not exceed the max input tokens for the model (8192 tokens for\n   * `text-embedding-ada-002`), cannot be an empty string, and any array must be 2048\n   * dimensions or less.\n   * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n   * for counting tokens.\n   */\n  input: string | Array<string> | Array<number> | Array<Array<number>>;\n\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models/overview) for\n   * descriptions of them.\n   */\n  model: (string & {}) | 'text-embedding-ada-002' | 'text-embedding-3-small' | 'text-embedding-3-large';\n\n  /**\n   * The number of dimensions the resulting output embeddings should have. Only\n   * supported in `text-embedding-3` and later models.\n   */\n  dimensions?: number;\n\n  /**\n   * The format to return the embeddings in. Can be either `float` or\n   * [`base64`](https://pypi.org/project/pybase64/).\n   */\n  encoding_format?: 'float' | 'base64';\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace Embeddings {\n  export import CreateEmbeddingResponse = EmbeddingsAPI.CreateEmbeddingResponse;\n  export import Embedding = EmbeddingsAPI.Embedding;\n  export import EmbeddingCreateParams = EmbeddingsAPI.EmbeddingCreateParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIResource } from \"../resource\";\nimport { isRequestOptions } from \"../core\";\nimport { type Response } from \"../_shims/index\";\nimport { sleep } from \"../core\";\nimport { APIConnectionTimeoutError } from \"../error\";\nimport * as FilesAPI from \"./files\";\nimport { type Uploadable, multipartFormRequestOptions } from \"../core\";\nimport { Page } from \"../pagination\";\n\nexport class Files extends APIResource {\n  /**\n   * Upload a file that can be used across various endpoints. The size of all the\n   * files uploaded by one organization can be up to 100 GB.\n   *\n   * The size of individual files can be a maximum of 512 MB or 2 million tokens for\n   * Assistants. See the\n   * [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools) to\n   * learn more about the types of files supported. The Fine-tuning API only supports\n   * `.jsonl` files.\n   *\n   * Please [contact us](https://help.openai.com/) if you need to increase these\n   * storage limits.\n   */\n  create(body: FileCreateParams, options?: Core.RequestOptions): Core.APIPromise<FileObject> {\n    return this._client.post('/files', multipartFormRequestOptions({ body, ...options }));\n  }\n\n  /**\n   * Returns information about a specific file.\n   */\n  retrieve(fileId: string, options?: Core.RequestOptions): Core.APIPromise<FileObject> {\n    return this._client.get(`/files/${fileId}`, options);\n  }\n\n  /**\n   * Returns a list of files that belong to the user's organization.\n   */\n  list(query?: FileListParams, options?: Core.RequestOptions): Core.PagePromise<FileObjectsPage, FileObject>;\n  list(options?: Core.RequestOptions): Core.PagePromise<FileObjectsPage, FileObject>;\n  list(\n    query: FileListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<FileObjectsPage, FileObject> {\n    if (isRequestOptions(query)) {\n      return this.list({}, query);\n    }\n    return this._client.getAPIList('/files', FileObjectsPage, { query, ...options });\n  }\n\n  /**\n   * Delete a file.\n   */\n  del(fileId: string, options?: Core.RequestOptions): Core.APIPromise<FileDeleted> {\n    return this._client.delete(`/files/${fileId}`, options);\n  }\n\n  /**\n   * Returns the contents of the specified file.\n   */\n  content(fileId: string, options?: Core.RequestOptions): Core.APIPromise<Response> {\n    return this._client.get(`/files/${fileId}/content`, { ...options, __binaryResponse: true });\n  }\n\n  /**\n   * Returns the contents of the specified file.\n   *\n   * @deprecated The `.content()` method should be used instead\n   */\n  retrieveContent(fileId: string, options?: Core.RequestOptions): Core.APIPromise<string> {\n    return this._client.get(`/files/${fileId}/content`, {\n      ...options,\n      headers: { Accept: 'application/json', ...options?.headers },\n    });\n  }\n\n  /**\n   * Waits for the given file to be processed, default timeout is 30 mins.\n   */\n  async waitForProcessing(\n    id: string,\n    { pollInterval = 5000, maxWait = 30 * 60 * 1000 }: { pollInterval?: number; maxWait?: number } = {},\n  ): Promise<FileObject> {\n    const TERMINAL_STATES = new Set(['processed', 'error', 'deleted']);\n\n    const start = Date.now();\n    let file = await this.retrieve(id);\n\n    while (!file.status || !TERMINAL_STATES.has(file.status)) {\n      await sleep(pollInterval);\n\n      file = await this.retrieve(id);\n      if (Date.now() - start > maxWait) {\n        throw new APIConnectionTimeoutError({\n          message: `Giving up on waiting for file ${id} to finish processing after ${maxWait} milliseconds.`,\n        });\n      }\n    }\n\n    return file;\n  }\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class FileObjectsPage extends Page<FileObject> {}\n\nexport type FileContent = string;\n\nexport interface FileDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'file';\n}\n\n/**\n * The `File` object represents a document that has been uploaded to OpenAI.\n */\nexport interface FileObject {\n  /**\n   * The file identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The size of the file, in bytes.\n   */\n  bytes: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the file was created.\n   */\n  created_at: number;\n\n  /**\n   * The name of the file.\n   */\n  filename: string;\n\n  /**\n   * The object type, which is always `file`.\n   */\n  object: 'file';\n\n  /**\n   * The intended purpose of the file. Supported values are `fine-tune`,\n   * `fine-tune-results`, `assistants`, and `assistants_output`.\n   */\n  purpose: 'fine-tune' | 'fine-tune-results' | 'assistants' | 'assistants_output';\n\n  /**\n   * Deprecated. The current status of the file, which can be either `uploaded`,\n   * `processed`, or `error`.\n   */\n  status: 'uploaded' | 'processed' | 'error';\n\n  /**\n   * Deprecated. For details on why a fine-tuning training file failed validation,\n   * see the `error` field on `fine_tuning.job`.\n   */\n  status_details?: string;\n}\n\nexport interface FileCreateParams {\n  /**\n   * The File object (not file name) to be uploaded.\n   */\n  file: Uploadable;\n\n  /**\n   * The intended purpose of the uploaded file.\n   *\n   * Use \"fine-tune\" for\n   * [Fine-tuning](https://platform.openai.com/docs/api-reference/fine-tuning) and\n   * \"assistants\" for\n   * [Assistants](https://platform.openai.com/docs/api-reference/assistants) and\n   * [Messages](https://platform.openai.com/docs/api-reference/messages). This allows\n   * us to validate the format of the uploaded file is correct for fine-tuning.\n   */\n  purpose: 'fine-tune' | 'assistants';\n}\n\nexport interface FileListParams {\n  /**\n   * Only return files with the given purpose.\n   */\n  purpose?: string;\n}\n\nexport namespace Files {\n  export import FileContent = FilesAPI.FileContent;\n  export import FileDeleted = FilesAPI.FileDeleted;\n  export import FileObject = FilesAPI.FileObject;\n  export import FileObjectsPage = FilesAPI.FileObjectsPage;\n  export import FileCreateParams = FilesAPI.FileCreateParams;\n  export import FileListParams = FilesAPI.FileListParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIResource } from \"../resource\";\nimport * as ImagesAPI from \"./images\";\nimport { type Uploadable, multipartFormRequestOptions } from \"../core\";\n\nexport class Images extends APIResource {\n  /**\n   * Creates a variation of a given image.\n   */\n  createVariation(\n    body: ImageCreateVariationParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<ImagesResponse> {\n    return this._client.post('/images/variations', multipartFormRequestOptions({ body, ...options }));\n  }\n\n  /**\n   * Creates an edited or extended image given an original image and a prompt.\n   */\n  edit(body: ImageEditParams, options?: Core.RequestOptions): Core.APIPromise<ImagesResponse> {\n    return this._client.post('/images/edits', multipartFormRequestOptions({ body, ...options }));\n  }\n\n  /**\n   * Creates an image given a prompt.\n   */\n  generate(body: ImageGenerateParams, options?: Core.RequestOptions): Core.APIPromise<ImagesResponse> {\n    return this._client.post('/images/generations', { body, ...options });\n  }\n}\n\n/**\n * Represents the url or the content of an image generated by the OpenAI API.\n */\nexport interface Image {\n  /**\n   * The base64-encoded JSON of the generated image, if `response_format` is\n   * `b64_json`.\n   */\n  b64_json?: string;\n\n  /**\n   * The prompt that was used to generate the image, if there was any revision to the\n   * prompt.\n   */\n  revised_prompt?: string;\n\n  /**\n   * The URL of the generated image, if `response_format` is `url` (default).\n   */\n  url?: string;\n}\n\nexport interface ImagesResponse {\n  created: number;\n\n  data: Array<Image>;\n}\n\nexport interface ImageCreateVariationParams {\n  /**\n   * The image to use as the basis for the variation(s). Must be a valid PNG file,\n   * less than 4MB, and square.\n   */\n  image: Uploadable;\n\n  /**\n   * The model to use for image generation. Only `dall-e-2` is supported at this\n   * time.\n   */\n  model?: (string & {}) | 'dall-e-2' | null;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only\n   * `n=1` is supported.\n   */\n  n?: number | null;\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\n   * `1024x1024`.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport interface ImageEditParams {\n  /**\n   * The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask\n   * is not provided, image must have transparency, which will be used as the mask.\n   */\n  image: Uploadable;\n\n  /**\n   * A text description of the desired image(s). The maximum length is 1000\n   * characters.\n   */\n  prompt: string;\n\n  /**\n   * An additional image whose fully transparent areas (e.g. where alpha is zero)\n   * indicate where `image` should be edited. Must be a valid PNG file, less than\n   * 4MB, and have the same dimensions as `image`.\n   */\n  mask?: Uploadable;\n\n  /**\n   * The model to use for image generation. Only `dall-e-2` is supported at this\n   * time.\n   */\n  model?: (string & {}) | 'dall-e-2' | null;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10.\n   */\n  n?: number | null;\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\n   * `1024x1024`.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport interface ImageGenerateParams {\n  /**\n   * A text description of the desired image(s). The maximum length is 1000\n   * characters for `dall-e-2` and 4000 characters for `dall-e-3`.\n   */\n  prompt: string;\n\n  /**\n   * The model to use for image generation.\n   */\n  model?: (string & {}) | 'dall-e-2' | 'dall-e-3' | null;\n\n  /**\n   * The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only\n   * `n=1` is supported.\n   */\n  n?: number | null;\n\n  /**\n   * The quality of the image that will be generated. `hd` creates images with finer\n   * details and greater consistency across the image. This param is only supported\n   * for `dall-e-3`.\n   */\n  quality?: 'standard' | 'hd';\n\n  /**\n   * The format in which the generated images are returned. Must be one of `url` or\n   * `b64_json`.\n   */\n  response_format?: 'url' | 'b64_json' | null;\n\n  /**\n   * The size of the generated images. Must be one of `256x256`, `512x512`, or\n   * `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or\n   * `1024x1792` for `dall-e-3` models.\n   */\n  size?: '256x256' | '512x512' | '1024x1024' | '1792x1024' | '1024x1792' | null;\n\n  /**\n   * The style of the generated images. Must be one of `vivid` or `natural`. Vivid\n   * causes the model to lean towards generating hyper-real and dramatic images.\n   * Natural causes the model to produce more natural, less hyper-real looking\n   * images. This param is only supported for `dall-e-3`.\n   */\n  style?: 'vivid' | 'natural' | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace Images {\n  export import Image = ImagesAPI.Image;\n  export import ImagesResponse = ImagesAPI.ImagesResponse;\n  export import ImageCreateVariationParams = ImagesAPI.ImageCreateVariationParams;\n  export import ImageEditParams = ImagesAPI.ImageEditParams;\n  export import ImageGenerateParams = ImagesAPI.ImageGenerateParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../core\";\nimport { APIResource } from \"../../resource\";\nimport { type Response } from \"../../_shims/index\";\nimport * as SpeechAPI from \"./speech\";\n\nexport class Speech extends APIResource {\n  /**\n   * Generates audio from the input text.\n   */\n  create(body: SpeechCreateParams, options?: Core.RequestOptions): Core.APIPromise<Response> {\n    return this._client.post('/audio/speech', { body, ...options, __binaryResponse: true });\n  }\n}\n\nexport interface SpeechCreateParams {\n  /**\n   * The text to generate audio for. The maximum length is 4096 characters.\n   */\n  input: string;\n\n  /**\n   * One of the available [TTS models](https://platform.openai.com/docs/models/tts):\n   * `tts-1` or `tts-1-hd`\n   */\n  model: (string & {}) | 'tts-1' | 'tts-1-hd';\n\n  /**\n   * The voice to use when generating the audio. Supported voices are `alloy`,\n   * `echo`, `fable`, `onyx`, `nova`, and `shimmer`. Previews of the voices are\n   * available in the\n   * [Text to speech guide](https://platform.openai.com/docs/guides/text-to-speech/voice-options).\n   */\n  voice: 'alloy' | 'echo' | 'fable' | 'onyx' | 'nova' | 'shimmer';\n\n  /**\n   * The format to audio in. Supported formats are `mp3`, `opus`, `aac`, and `flac`.\n   */\n  response_format?: 'mp3' | 'opus' | 'aac' | 'flac';\n\n  /**\n   * The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is\n   * the default.\n   */\n  speed?: number;\n}\n\nexport namespace Speech {\n  export import SpeechCreateParams = SpeechAPI.SpeechCreateParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../core\";\nimport { APIResource } from \"../../resource\";\nimport * as TranscriptionsAPI from \"./transcriptions\";\nimport { type Uploadable, multipartFormRequestOptions } from \"../../core\";\n\nexport class Transcriptions extends APIResource {\n  /**\n   * Transcribes audio into the input language.\n   */\n  create(body: TranscriptionCreateParams, options?: Core.RequestOptions): Core.APIPromise<Transcription> {\n    return this._client.post('/audio/transcriptions', multipartFormRequestOptions({ body, ...options }));\n  }\n}\n\nexport interface Transcription {\n  text: string;\n}\n\nexport interface TranscriptionCreateParams {\n  /**\n   * The audio file object (not file name) to transcribe, in one of these formats:\n   * flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uploadable;\n\n  /**\n   * ID of the model to use. Only `whisper-1` is currently available.\n   */\n  model: (string & {}) | 'whisper-1';\n\n  /**\n   * The language of the input audio. Supplying the input language in\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will\n   * improve accuracy and latency.\n   */\n  language?: string;\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The\n   * [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting)\n   * should match the audio language.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the transcript output, in one of these options: `json`, `text`,\n   * `srt`, `verbose_json`, or `vtt`.\n   */\n  response_format?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n}\n\nexport namespace Transcriptions {\n  export import Transcription = TranscriptionsAPI.Transcription;\n  export import TranscriptionCreateParams = TranscriptionsAPI.TranscriptionCreateParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../core\";\nimport { APIResource } from \"../../resource\";\nimport * as TranslationsAPI from \"./translations\";\nimport { type Uploadable, multipartFormRequestOptions } from \"../../core\";\n\nexport class Translations extends APIResource {\n  /**\n   * Translates audio into English.\n   */\n  create(body: TranslationCreateParams, options?: Core.RequestOptions): Core.APIPromise<Translation> {\n    return this._client.post('/audio/translations', multipartFormRequestOptions({ body, ...options }));\n  }\n}\n\nexport interface Translation {\n  text: string;\n}\n\nexport interface TranslationCreateParams {\n  /**\n   * The audio file object (not file name) translate, in one of these formats: flac,\n   * mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uploadable;\n\n  /**\n   * ID of the model to use. Only `whisper-1` is currently available.\n   */\n  model: (string & {}) | 'whisper-1';\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio\n   * segment. The\n   * [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting)\n   * should be in English.\n   */\n  prompt?: string;\n\n  /**\n   * The format of the transcript output, in one of these options: `json`, `text`,\n   * `srt`, `verbose_json`, or `vtt`.\n   */\n  response_format?: string;\n\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n   * output more random, while lower values like 0.2 will make it more focused and\n   * deterministic. If set to 0, the model will use\n   * [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n}\n\nexport namespace Translations {\n  export import Translation = TranslationsAPI.Translation;\n  export import TranslationCreateParams = TranslationsAPI.TranslationCreateParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport { APIResource } from \"../../resource\";\nimport * as SpeechAPI from \"./speech\";\nimport * as TranscriptionsAPI from \"./transcriptions\";\nimport * as TranslationsAPI from \"./translations\";\n\nexport class Audio extends APIResource {\n  transcriptions: TranscriptionsAPI.Transcriptions = new TranscriptionsAPI.Transcriptions(this._client);\n  translations: TranslationsAPI.Translations = new TranslationsAPI.Translations(this._client);\n  speech: SpeechAPI.Speech = new SpeechAPI.Speech(this._client);\n}\n\nexport namespace Audio {\n  export import Transcriptions = TranscriptionsAPI.Transcriptions;\n  export import Transcription = TranscriptionsAPI.Transcription;\n  export import TranscriptionCreateParams = TranscriptionsAPI.TranscriptionCreateParams;\n  export import Translations = TranslationsAPI.Translations;\n  export import Translation = TranslationsAPI.Translation;\n  export import TranslationCreateParams = TranslationsAPI.TranslationCreateParams;\n  export import Speech = SpeechAPI.Speech;\n  export import SpeechCreateParams = SpeechAPI.SpeechCreateParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIResource } from \"../resource\";\nimport * as ModerationsAPI from \"./moderations\";\n\nexport class Moderations extends APIResource {\n  /**\n   * Classifies if text violates OpenAI's Content Policy\n   */\n  create(\n    body: ModerationCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<ModerationCreateResponse> {\n    return this._client.post('/moderations', { body, ...options });\n  }\n}\n\nexport interface Moderation {\n  /**\n   * A list of the categories, and whether they are flagged or not.\n   */\n  categories: Moderation.Categories;\n\n  /**\n   * A list of the categories along with their scores as predicted by model.\n   */\n  category_scores: Moderation.CategoryScores;\n\n  /**\n   * Whether the content violates\n   * [OpenAI's usage policies](/policies/usage-policies).\n   */\n  flagged: boolean;\n}\n\nexport namespace Moderation {\n  /**\n   * A list of the categories, and whether they are flagged or not.\n   */\n  export interface Categories {\n    /**\n     * Content that expresses, incites, or promotes harassing language towards any\n     * target.\n     */\n    harassment: boolean;\n\n    /**\n     * Harassment content that also includes violence or serious harm towards any\n     * target.\n     */\n    'harassment/threatening': boolean;\n\n    /**\n     * Content that expresses, incites, or promotes hate based on race, gender,\n     * ethnicity, religion, nationality, sexual orientation, disability status, or\n     * caste. Hateful content aimed at non-protected groups (e.g., chess players) is\n     * harassment.\n     */\n    hate: boolean;\n\n    /**\n     * Hateful content that also includes violence or serious harm towards the targeted\n     * group based on race, gender, ethnicity, religion, nationality, sexual\n     * orientation, disability status, or caste.\n     */\n    'hate/threatening': boolean;\n\n    /**\n     * Content that promotes, encourages, or depicts acts of self-harm, such as\n     * suicide, cutting, and eating disorders.\n     */\n    'self-harm': boolean;\n\n    /**\n     * Content that encourages performing acts of self-harm, such as suicide, cutting,\n     * and eating disorders, or that gives instructions or advice on how to commit such\n     * acts.\n     */\n    'self-harm/instructions': boolean;\n\n    /**\n     * Content where the speaker expresses that they are engaging or intend to engage\n     * in acts of self-harm, such as suicide, cutting, and eating disorders.\n     */\n    'self-harm/intent': boolean;\n\n    /**\n     * Content meant to arouse sexual excitement, such as the description of sexual\n     * activity, or that promotes sexual services (excluding sex education and\n     * wellness).\n     */\n    sexual: boolean;\n\n    /**\n     * Sexual content that includes an individual who is under 18 years old.\n     */\n    'sexual/minors': boolean;\n\n    /**\n     * Content that depicts death, violence, or physical injury.\n     */\n    violence: boolean;\n\n    /**\n     * Content that depicts death, violence, or physical injury in graphic detail.\n     */\n    'violence/graphic': boolean;\n  }\n\n  /**\n   * A list of the categories along with their scores as predicted by model.\n   */\n  export interface CategoryScores {\n    /**\n     * The score for the category 'harassment'.\n     */\n    harassment: number;\n\n    /**\n     * The score for the category 'harassment/threatening'.\n     */\n    'harassment/threatening': number;\n\n    /**\n     * The score for the category 'hate'.\n     */\n    hate: number;\n\n    /**\n     * The score for the category 'hate/threatening'.\n     */\n    'hate/threatening': number;\n\n    /**\n     * The score for the category 'self-harm'.\n     */\n    'self-harm': number;\n\n    /**\n     * The score for the category 'self-harm/instructions'.\n     */\n    'self-harm/instructions': number;\n\n    /**\n     * The score for the category 'self-harm/intent'.\n     */\n    'self-harm/intent': number;\n\n    /**\n     * The score for the category 'sexual'.\n     */\n    sexual: number;\n\n    /**\n     * The score for the category 'sexual/minors'.\n     */\n    'sexual/minors': number;\n\n    /**\n     * The score for the category 'violence'.\n     */\n    violence: number;\n\n    /**\n     * The score for the category 'violence/graphic'.\n     */\n    'violence/graphic': number;\n  }\n}\n\n/**\n * Represents policy compliance report by OpenAI's content moderation model against\n * a given input.\n */\nexport interface ModerationCreateResponse {\n  /**\n   * The unique identifier for the moderation request.\n   */\n  id: string;\n\n  /**\n   * The model used to generate the moderation results.\n   */\n  model: string;\n\n  /**\n   * A list of moderation objects.\n   */\n  results: Array<Moderation>;\n}\n\nexport interface ModerationCreateParams {\n  /**\n   * The input text to classify\n   */\n  input: string | Array<string>;\n\n  /**\n   * Two content moderations models are available: `text-moderation-stable` and\n   * `text-moderation-latest`.\n   *\n   * The default is `text-moderation-latest` which will be automatically upgraded\n   * over time. This ensures you are always using our most accurate model. If you use\n   * `text-moderation-stable`, we will provide advanced notice before updating the\n   * model. Accuracy of `text-moderation-stable` may be slightly lower than for\n   * `text-moderation-latest`.\n   */\n  model?: (string & {}) | 'text-moderation-latest' | 'text-moderation-stable';\n}\n\nexport namespace Moderations {\n  export import Moderation = ModerationsAPI.Moderation;\n  export import ModerationCreateResponse = ModerationsAPI.ModerationCreateResponse;\n  export import ModerationCreateParams = ModerationsAPI.ModerationCreateParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIResource } from \"../resource\";\nimport * as ModelsAPI from \"./models\";\nimport { Page } from \"../pagination\";\n\nexport class Models extends APIResource {\n  /**\n   * Retrieves a model instance, providing basic information about the model such as\n   * the owner and permissioning.\n   */\n  retrieve(model: string, options?: Core.RequestOptions): Core.APIPromise<Model> {\n    return this._client.get(`/models/${model}`, options);\n  }\n\n  /**\n   * Lists the currently available models, and provides basic information about each\n   * one such as the owner and availability.\n   */\n  list(options?: Core.RequestOptions): Core.PagePromise<ModelsPage, Model> {\n    return this._client.getAPIList('/models', ModelsPage, options);\n  }\n\n  /**\n   * Delete a fine-tuned model. You must have the Owner role in your organization to\n   * delete a model.\n   */\n  del(model: string, options?: Core.RequestOptions): Core.APIPromise<ModelDeleted> {\n    return this._client.delete(`/models/${model}`, options);\n  }\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class ModelsPage extends Page<Model> {}\n\n/**\n * Describes an OpenAI model offering that can be used with the API.\n */\nexport interface Model {\n  /**\n   * The model identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) when the model was created.\n   */\n  created: number;\n\n  /**\n   * The object type, which is always \"model\".\n   */\n  object: 'model';\n\n  /**\n   * The organization that owns the model.\n   */\n  owned_by: string;\n}\n\nexport interface ModelDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: string;\n}\n\nexport namespace Models {\n  export import Model = ModelsAPI.Model;\n  export import ModelDeleted = ModelsAPI.ModelDeleted;\n  export import ModelsPage = ModelsAPI.ModelsPage;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../core\";\nimport { APIResource } from \"../../resource\";\nimport { isRequestOptions } from \"../../core\";\nimport * as JobsAPI from \"./jobs\";\nimport { CursorPage, type CursorPageParams } from \"../../pagination\";\n\nexport class Jobs extends APIResource {\n  /**\n   * Creates a fine-tuning job which begins the process of creating a new model from\n   * a given dataset.\n   *\n   * Response includes details of the enqueued job including job status and the name\n   * of the fine-tuned models once complete.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)\n   */\n  create(body: JobCreateParams, options?: Core.RequestOptions): Core.APIPromise<FineTuningJob> {\n    return this._client.post('/fine_tuning/jobs', { body, ...options });\n  }\n\n  /**\n   * Get info about a fine-tuning job.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)\n   */\n  retrieve(fineTuningJobId: string, options?: Core.RequestOptions): Core.APIPromise<FineTuningJob> {\n    return this._client.get(`/fine_tuning/jobs/${fineTuningJobId}`, options);\n  }\n\n  /**\n   * List your organization's fine-tuning jobs\n   */\n  list(\n    query?: JobListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<FineTuningJobsPage, FineTuningJob>;\n  list(options?: Core.RequestOptions): Core.PagePromise<FineTuningJobsPage, FineTuningJob>;\n  list(\n    query: JobListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<FineTuningJobsPage, FineTuningJob> {\n    if (isRequestOptions(query)) {\n      return this.list({}, query);\n    }\n    return this._client.getAPIList('/fine_tuning/jobs', FineTuningJobsPage, { query, ...options });\n  }\n\n  /**\n   * Immediately cancel a fine-tune job.\n   */\n  cancel(fineTuningJobId: string, options?: Core.RequestOptions): Core.APIPromise<FineTuningJob> {\n    return this._client.post(`/fine_tuning/jobs/${fineTuningJobId}/cancel`, options);\n  }\n\n  /**\n   * Get status updates for a fine-tuning job.\n   */\n  listEvents(\n    fineTuningJobId: string,\n    query?: JobListEventsParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<FineTuningJobEventsPage, FineTuningJobEvent>;\n  listEvents(\n    fineTuningJobId: string,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<FineTuningJobEventsPage, FineTuningJobEvent>;\n  listEvents(\n    fineTuningJobId: string,\n    query: JobListEventsParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<FineTuningJobEventsPage, FineTuningJobEvent> {\n    if (isRequestOptions(query)) {\n      return this.listEvents(fineTuningJobId, {}, query);\n    }\n    return this._client.getAPIList(`/fine_tuning/jobs/${fineTuningJobId}/events`, FineTuningJobEventsPage, {\n      query,\n      ...options,\n    });\n  }\n}\n\nexport class FineTuningJobsPage extends CursorPage<FineTuningJob> {}\n\nexport class FineTuningJobEventsPage extends CursorPage<FineTuningJobEvent> {}\n\n/**\n * The `fine_tuning.job` object represents a fine-tuning job that has been created\n * through the API.\n */\nexport interface FineTuningJob {\n  /**\n   * The object identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was created.\n   */\n  created_at: number;\n\n  /**\n   * For fine-tuning jobs that have `failed`, this will contain more information on\n   * the cause of the failure.\n   */\n  error: FineTuningJob.Error | null;\n\n  /**\n   * The name of the fine-tuned model that is being created. The value will be null\n   * if the fine-tuning job is still running.\n   */\n  fine_tuned_model: string | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was finished. The\n   * value will be null if the fine-tuning job is still running.\n   */\n  finished_at: number | null;\n\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning) for\n   * more details.\n   */\n  hyperparameters: FineTuningJob.Hyperparameters;\n\n  /**\n   * The base model that is being fine-tuned.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"fine_tuning.job\".\n   */\n  object: 'fine_tuning.job';\n\n  /**\n   * The organization that owns the fine-tuning job.\n   */\n  organization_id: string;\n\n  /**\n   * The compiled results file ID(s) for the fine-tuning job. You can retrieve the\n   * results with the\n   * [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).\n   */\n  result_files: Array<string>;\n\n  /**\n   * The current status of the fine-tuning job, which can be either\n   * `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.\n   */\n  status: 'validating_files' | 'queued' | 'running' | 'succeeded' | 'failed' | 'cancelled';\n\n  /**\n   * The total number of billable tokens processed by this fine-tuning job. The value\n   * will be null if the fine-tuning job is still running.\n   */\n  trained_tokens: number | null;\n\n  /**\n   * The file ID used for training. You can retrieve the training data with the\n   * [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).\n   */\n  training_file: string;\n\n  /**\n   * The file ID used for validation. You can retrieve the validation results with\n   * the\n   * [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).\n   */\n  validation_file: string | null;\n}\n\nexport namespace FineTuningJob {\n  /**\n   * For fine-tuning jobs that have `failed`, this will contain more information on\n   * the cause of the failure.\n   */\n  export interface Error {\n    /**\n     * A machine-readable error code.\n     */\n    code: string;\n\n    /**\n     * A human-readable error message.\n     */\n    message: string;\n\n    /**\n     * The parameter that was invalid, usually `training_file` or `validation_file`.\n     * This field will be null if the failure was not parameter-specific.\n     */\n    param: string | null;\n  }\n\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning) for\n   * more details.\n   */\n  export interface Hyperparameters {\n    /**\n     * The number of epochs to train the model for. An epoch refers to one full cycle\n     * through the training dataset. \"auto\" decides the optimal number of epochs based\n     * on the size of the dataset. If setting the number manually, we support any\n     * number between 1 and 50 epochs.\n     */\n    n_epochs: 'auto' | number;\n  }\n}\n\n/**\n * Fine-tuning job event object\n */\nexport interface FineTuningJobEvent {\n  id: string;\n\n  created_at: number;\n\n  level: 'info' | 'warn' | 'error';\n\n  message: string;\n\n  object: 'fine_tuning.job.event';\n}\n\nexport interface JobCreateParams {\n  /**\n   * The name of the model to fine-tune. You can select one of the\n   * [supported models](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned).\n   */\n  model: (string & {}) | 'babbage-002' | 'davinci-002' | 'gpt-3.5-turbo';\n\n  /**\n   * The ID of an uploaded file that contains training data.\n   *\n   * See [upload file](https://platform.openai.com/docs/api-reference/files/upload)\n   * for how to upload a file.\n   *\n   * Your dataset must be formatted as a JSONL file. Additionally, you must upload\n   * your file with the purpose `fine-tune`.\n   *\n   * See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning)\n   * for more details.\n   */\n  training_file: string;\n\n  /**\n   * The hyperparameters used for the fine-tuning job.\n   */\n  hyperparameters?: JobCreateParams.Hyperparameters;\n\n  /**\n   * A string of up to 18 characters that will be added to your fine-tuned model\n   * name.\n   *\n   * For example, a `suffix` of \"custom-model-name\" would produce a model name like\n   * `ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel`.\n   */\n  suffix?: string | null;\n\n  /**\n   * The ID of an uploaded file that contains validation data.\n   *\n   * If you provide this file, the data is used to generate validation metrics\n   * periodically during fine-tuning. These metrics can be viewed in the fine-tuning\n   * results file. The same data should not be present in both train and validation\n   * files.\n   *\n   * Your dataset must be formatted as a JSONL file. You must upload your file with\n   * the purpose `fine-tune`.\n   *\n   * See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning)\n   * for more details.\n   */\n  validation_file?: string | null;\n}\n\nexport namespace JobCreateParams {\n  /**\n   * The hyperparameters used for the fine-tuning job.\n   */\n  export interface Hyperparameters {\n    /**\n     * Number of examples in each batch. A larger batch size means that model\n     * parameters are updated less frequently, but with lower variance.\n     */\n    batch_size?: 'auto' | number;\n\n    /**\n     * Scaling factor for the learning rate. A smaller learning rate may be useful to\n     * avoid overfitting.\n     */\n    learning_rate_multiplier?: 'auto' | number;\n\n    /**\n     * The number of epochs to train the model for. An epoch refers to one full cycle\n     * through the training dataset.\n     */\n    n_epochs?: 'auto' | number;\n  }\n}\n\nexport interface JobListParams extends CursorPageParams {}\n\nexport interface JobListEventsParams extends CursorPageParams {}\n\nexport namespace Jobs {\n  export import FineTuningJob = JobsAPI.FineTuningJob;\n  export import FineTuningJobEvent = JobsAPI.FineTuningJobEvent;\n  export import FineTuningJobsPage = JobsAPI.FineTuningJobsPage;\n  export import FineTuningJobEventsPage = JobsAPI.FineTuningJobEventsPage;\n  export import JobCreateParams = JobsAPI.JobCreateParams;\n  export import JobListParams = JobsAPI.JobListParams;\n  export import JobListEventsParams = JobsAPI.JobListEventsParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport { APIResource } from \"../../resource\";\nimport * as JobsAPI from \"./jobs\";\n\nexport class FineTuning extends APIResource {\n  jobs: JobsAPI.Jobs = new JobsAPI.Jobs(this._client);\n}\n\nexport namespace FineTuning {\n  export import Jobs = JobsAPI.Jobs;\n  export import FineTuningJob = JobsAPI.FineTuningJob;\n  export import FineTuningJobEvent = JobsAPI.FineTuningJobEvent;\n  export import FineTuningJobsPage = JobsAPI.FineTuningJobsPage;\n  export import FineTuningJobEventsPage = JobsAPI.FineTuningJobEventsPage;\n  export import JobCreateParams = JobsAPI.JobCreateParams;\n  export import JobListParams = JobsAPI.JobListParams;\n  export import JobListEventsParams = JobsAPI.JobListEventsParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../core\";\nimport { APIResource } from \"../../../resource\";\nimport { isRequestOptions } from \"../../../core\";\nimport * as FilesAPI from \"./files\";\nimport { CursorPage, type CursorPageParams } from \"../../../pagination\";\n\nexport class Files extends APIResource {\n  /**\n   * Create an assistant file by attaching a\n   * [File](https://platform.openai.com/docs/api-reference/files) to an\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants).\n   */\n  create(\n    assistantId: string,\n    body: FileCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<AssistantFile> {\n    return this._client.post(`/assistants/${assistantId}/files`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Retrieves an AssistantFile.\n   */\n  retrieve(\n    assistantId: string,\n    fileId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<AssistantFile> {\n    return this._client.get(`/assistants/${assistantId}/files/${fileId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Returns a list of assistant files.\n   */\n  list(\n    assistantId: string,\n    query?: FileListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<AssistantFilesPage, AssistantFile>;\n  list(\n    assistantId: string,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<AssistantFilesPage, AssistantFile>;\n  list(\n    assistantId: string,\n    query: FileListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<AssistantFilesPage, AssistantFile> {\n    if (isRequestOptions(query)) {\n      return this.list(assistantId, {}, query);\n    }\n    return this._client.getAPIList(`/assistants/${assistantId}/files`, AssistantFilesPage, {\n      query,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Delete an assistant file.\n   */\n  del(\n    assistantId: string,\n    fileId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<FileDeleteResponse> {\n    return this._client.delete(`/assistants/${assistantId}/files/${fileId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n}\n\nexport class AssistantFilesPage extends CursorPage<AssistantFile> {}\n\n/**\n * A list of [Files](https://platform.openai.com/docs/api-reference/files) attached\n * to an `assistant`.\n */\nexport interface AssistantFile {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The assistant ID that the file is attached to.\n   */\n  assistant_id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the assistant file was created.\n   */\n  created_at: number;\n\n  /**\n   * The object type, which is always `assistant.file`.\n   */\n  object: 'assistant.file';\n}\n\n/**\n * Deletes the association between the assistant and the file, but does not delete\n * the [File](https://platform.openai.com/docs/api-reference/files) object itself.\n */\nexport interface FileDeleteResponse {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'assistant.file.deleted';\n}\n\nexport interface FileCreateParams {\n  /**\n   * A [File](https://platform.openai.com/docs/api-reference/files) ID (with\n   * `purpose=\"assistants\"`) that the assistant should use. Useful for tools like\n   * `retrieval` and `code_interpreter` that can access files.\n   */\n  file_id: string;\n}\n\nexport interface FileListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport namespace Files {\n  export import AssistantFile = FilesAPI.AssistantFile;\n  export import FileDeleteResponse = FilesAPI.FileDeleteResponse;\n  export import AssistantFilesPage = FilesAPI.AssistantFilesPage;\n  export import FileCreateParams = FilesAPI.FileCreateParams;\n  export import FileListParams = FilesAPI.FileListParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../core\";\nimport { APIResource } from \"../../../resource\";\nimport { isRequestOptions } from \"../../../core\";\nimport * as AssistantsAPI from \"./assistants\";\nimport * as Shared from \"../../shared\";\nimport * as FilesAPI from \"./files\";\nimport { CursorPage, type CursorPageParams } from \"../../../pagination\";\n\nexport class Assistants extends APIResource {\n  files: FilesAPI.Files = new FilesAPI.Files(this._client);\n\n  /**\n   * Create an assistant with a model and instructions.\n   */\n  create(body: AssistantCreateParams, options?: Core.RequestOptions): Core.APIPromise<Assistant> {\n    return this._client.post('/assistants', {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Retrieves an assistant.\n   */\n  retrieve(assistantId: string, options?: Core.RequestOptions): Core.APIPromise<Assistant> {\n    return this._client.get(`/assistants/${assistantId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Modifies an assistant.\n   */\n  update(\n    assistantId: string,\n    body: AssistantUpdateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<Assistant> {\n    return this._client.post(`/assistants/${assistantId}`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Returns a list of assistants.\n   */\n  list(\n    query?: AssistantListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<AssistantsPage, Assistant>;\n  list(options?: Core.RequestOptions): Core.PagePromise<AssistantsPage, Assistant>;\n  list(\n    query: AssistantListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<AssistantsPage, Assistant> {\n    if (isRequestOptions(query)) {\n      return this.list({}, query);\n    }\n    return this._client.getAPIList('/assistants', AssistantsPage, {\n      query,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Delete an assistant.\n   */\n  del(assistantId: string, options?: Core.RequestOptions): Core.APIPromise<AssistantDeleted> {\n    return this._client.delete(`/assistants/${assistantId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n}\n\nexport class AssistantsPage extends CursorPage<Assistant> {}\n\n/**\n * Represents an `assistant` that can call the model and use tools.\n */\nexport interface Assistant {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the assistant was created.\n   */\n  created_at: number;\n\n  /**\n   * The description of the assistant. The maximum length is 512 characters.\n   */\n  description: string | null;\n\n  /**\n   * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs\n   * attached to this assistant. There can be a maximum of 20 files attached to the\n   * assistant. Files are ordered by their creation date in ascending order.\n   */\n  file_ids: Array<string>;\n\n  /**\n   * The system instructions that the assistant uses. The maximum length is 32768\n   * characters.\n   */\n  instructions: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata: unknown | null;\n\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models/overview) for\n   * descriptions of them.\n   */\n  model: string;\n\n  /**\n   * The name of the assistant. The maximum length is 256 characters.\n   */\n  name: string | null;\n\n  /**\n   * The object type, which is always `assistant`.\n   */\n  object: 'assistant';\n\n  /**\n   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per\n   * assistant. Tools can be of types `code_interpreter`, `retrieval`, or `function`.\n   */\n  tools: Array<Assistant.CodeInterpreter | Assistant.Retrieval | Assistant.Function>;\n}\n\nexport namespace Assistant {\n  export interface CodeInterpreter {\n    /**\n     * The type of tool being defined: `code_interpreter`\n     */\n    type: 'code_interpreter';\n  }\n\n  export interface Retrieval {\n    /**\n     * The type of tool being defined: `retrieval`\n     */\n    type: 'retrieval';\n  }\n\n  export interface Function {\n    function: Shared.FunctionDefinition;\n\n    /**\n     * The type of tool being defined: `function`\n     */\n    type: 'function';\n  }\n}\n\nexport interface AssistantDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'assistant.deleted';\n}\n\nexport interface AssistantCreateParams {\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models/overview) for\n   * descriptions of them.\n   */\n  model: string;\n\n  /**\n   * The description of the assistant. The maximum length is 512 characters.\n   */\n  description?: string | null;\n\n  /**\n   * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs\n   * attached to this assistant. There can be a maximum of 20 files attached to the\n   * assistant. Files are ordered by their creation date in ascending order.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * The system instructions that the assistant uses. The maximum length is 32768\n   * characters.\n   */\n  instructions?: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n\n  /**\n   * The name of the assistant. The maximum length is 256 characters.\n   */\n  name?: string | null;\n\n  /**\n   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per\n   * assistant. Tools can be of types `code_interpreter`, `retrieval`, or `function`.\n   */\n  tools?: Array<\n    | AssistantCreateParams.AssistantToolsCode\n    | AssistantCreateParams.AssistantToolsRetrieval\n    | AssistantCreateParams.AssistantToolsFunction\n  >;\n}\n\nexport namespace AssistantCreateParams {\n  export interface AssistantToolsCode {\n    /**\n     * The type of tool being defined: `code_interpreter`\n     */\n    type: 'code_interpreter';\n  }\n\n  export interface AssistantToolsRetrieval {\n    /**\n     * The type of tool being defined: `retrieval`\n     */\n    type: 'retrieval';\n  }\n\n  export interface AssistantToolsFunction {\n    function: Shared.FunctionDefinition;\n\n    /**\n     * The type of tool being defined: `function`\n     */\n    type: 'function';\n  }\n}\n\nexport interface AssistantUpdateParams {\n  /**\n   * The description of the assistant. The maximum length is 512 characters.\n   */\n  description?: string | null;\n\n  /**\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs\n   * attached to this assistant. There can be a maximum of 20 files attached to the\n   * assistant. Files are ordered by their creation date in ascending order. If a\n   * file was previously attached to the list but does not show up in the list, it\n   * will be deleted from the assistant.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * The system instructions that the assistant uses. The maximum length is 32768\n   * characters.\n   */\n  instructions?: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n\n  /**\n   * ID of the model to use. You can use the\n   * [List models](https://platform.openai.com/docs/api-reference/models/list) API to\n   * see all of your available models, or see our\n   * [Model overview](https://platform.openai.com/docs/models/overview) for\n   * descriptions of them.\n   */\n  model?: string;\n\n  /**\n   * The name of the assistant. The maximum length is 256 characters.\n   */\n  name?: string | null;\n\n  /**\n   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per\n   * assistant. Tools can be of types `code_interpreter`, `retrieval`, or `function`.\n   */\n  tools?: Array<\n    | AssistantUpdateParams.AssistantToolsCode\n    | AssistantUpdateParams.AssistantToolsRetrieval\n    | AssistantUpdateParams.AssistantToolsFunction\n  >;\n}\n\nexport namespace AssistantUpdateParams {\n  export interface AssistantToolsCode {\n    /**\n     * The type of tool being defined: `code_interpreter`\n     */\n    type: 'code_interpreter';\n  }\n\n  export interface AssistantToolsRetrieval {\n    /**\n     * The type of tool being defined: `retrieval`\n     */\n    type: 'retrieval';\n  }\n\n  export interface AssistantToolsFunction {\n    function: Shared.FunctionDefinition;\n\n    /**\n     * The type of tool being defined: `function`\n     */\n    type: 'function';\n  }\n}\n\nexport interface AssistantListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport namespace Assistants {\n  export import Assistant = AssistantsAPI.Assistant;\n  export import AssistantDeleted = AssistantsAPI.AssistantDeleted;\n  export import AssistantsPage = AssistantsAPI.AssistantsPage;\n  export import AssistantCreateParams = AssistantsAPI.AssistantCreateParams;\n  export import AssistantUpdateParams = AssistantsAPI.AssistantUpdateParams;\n  export import AssistantListParams = AssistantsAPI.AssistantListParams;\n  export import Files = FilesAPI.Files;\n  export import AssistantFile = FilesAPI.AssistantFile;\n  export import FileDeleteResponse = FilesAPI.FileDeleteResponse;\n  export import AssistantFilesPage = FilesAPI.AssistantFilesPage;\n  export import FileCreateParams = FilesAPI.FileCreateParams;\n  export import FileListParams = FilesAPI.FileListParams;\n}\n","import { type ChatCompletionRunner } from './ChatCompletionRunner';\nimport { type ChatCompletionStreamingRunner } from './ChatCompletionStreamingRunner';\nimport { JSONSchema } from './jsonschema';\n\ntype PromiseOrValue<T> = T | Promise<T>;\n\nexport type RunnableFunctionWithParse<Args extends object> = {\n  /**\n   * @param args the return value from `parse`.\n   * @param runner the runner evaluating this callback.\n   * @returns a string to send back to OpenAI.\n   */\n  function: (\n    args: Args,\n    runner: ChatCompletionRunner | ChatCompletionStreamingRunner,\n  ) => PromiseOrValue<unknown>;\n  /**\n   * @param input the raw args from the OpenAI function call.\n   * @returns the parsed arguments to pass to `function`\n   */\n  parse: (input: string) => PromiseOrValue<Args>;\n  /**\n   * The parameters the function accepts, describes as a JSON Schema object.\n   */\n  parameters: JSONSchema;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the function.\n   */\n  description: string;\n  /**\n   * The name of the function to be called. Will default to function.name if omitted.\n   */\n  name?: string | undefined;\n};\n\nexport type RunnableFunctionWithoutParse = {\n  /**\n   * @param args the raw args from the OpenAI function call.\n   * @returns a string to send back to OpenAI\n   */\n  function: (\n    args: string,\n    runner: ChatCompletionRunner | ChatCompletionStreamingRunner,\n  ) => PromiseOrValue<unknown>;\n  /**\n   * The parameters the function accepts, describes as a JSON Schema object.\n   */\n  parameters: JSONSchema;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the function.\n   */\n  description: string;\n  /**\n   * The name of the function to be called. Will default to function.name if omitted.\n   */\n  name?: string | undefined;\n};\n\nexport type RunnableFunction<Args extends object | string> =\n  Args extends string ? RunnableFunctionWithoutParse\n  : Args extends object ? RunnableFunctionWithParse<Args>\n  : never;\n\nexport type RunnableToolFunction<Args extends object | string> =\n  Args extends string ? RunnableToolFunctionWithoutParse\n  : Args extends object ? RunnableToolFunctionWithParse<Args>\n  : never;\n\nexport type RunnableToolFunctionWithoutParse = {\n  type: 'function';\n  function: RunnableFunctionWithoutParse;\n};\nexport type RunnableToolFunctionWithParse<Args extends object> = {\n  type: 'function';\n  function: RunnableFunctionWithParse<Args>;\n};\n\nexport function isRunnableFunctionWithParse<Args extends object>(\n  fn: any,\n): fn is RunnableFunctionWithParse<Args> {\n  return typeof (fn as any).parse === 'function';\n}\n\nexport type BaseFunctionsArgs = readonly (object | string)[];\n\nexport type RunnableFunctions<FunctionsArgs extends BaseFunctionsArgs> =\n  [any[]] extends [FunctionsArgs] ? readonly RunnableFunction<any>[]\n  : {\n      [Index in keyof FunctionsArgs]: Index extends number ? RunnableFunction<FunctionsArgs[Index]>\n      : FunctionsArgs[Index];\n    };\n\nexport type RunnableTools<FunctionsArgs extends BaseFunctionsArgs> =\n  [any[]] extends [FunctionsArgs] ? readonly RunnableToolFunction<any>[]\n  : {\n      [Index in keyof FunctionsArgs]: Index extends number ? RunnableToolFunction<FunctionsArgs[Index]>\n      : FunctionsArgs[Index];\n    };\n\n/**\n * This is helper class for passing a `function` and `parse` where the `function`\n * argument type matches the `parse` return type.\n *\n * @deprecated - please use ParsingToolFunction instead.\n */\nexport class ParsingFunction<Args extends object> {\n  function: RunnableFunctionWithParse<Args>['function'];\n  parse: RunnableFunctionWithParse<Args>['parse'];\n  parameters: RunnableFunctionWithParse<Args>['parameters'];\n  description: RunnableFunctionWithParse<Args>['description'];\n  name?: RunnableFunctionWithParse<Args>['name'];\n\n  constructor(input: RunnableFunctionWithParse<Args>) {\n    this.function = input.function;\n    this.parse = input.parse;\n    this.parameters = input.parameters;\n    this.description = input.description;\n    this.name = input.name;\n  }\n}\n\n/**\n * This is helper class for passing a `function` and `parse` where the `function`\n * argument type matches the `parse` return type.\n */\nexport class ParsingToolFunction<Args extends object> {\n  type: 'function';\n  function: RunnableFunctionWithParse<Args>;\n\n  constructor(input: RunnableFunctionWithParse<Args>) {\n    this.type = 'function';\n    this.function = input;\n  }\n}\n","import {\n  type ChatCompletionAssistantMessageParam,\n  type ChatCompletionFunctionMessageParam,\n  type ChatCompletionMessageParam,\n  type ChatCompletionToolMessageParam,\n} from \"../resources\";\n\nexport const isAssistantMessage = (\n  message: ChatCompletionMessageParam | null | undefined,\n): message is ChatCompletionAssistantMessageParam => {\n  return message?.role === 'assistant';\n};\n\nexport const isFunctionMessage = (\n  message: ChatCompletionMessageParam | null | undefined,\n): message is ChatCompletionFunctionMessageParam => {\n  return message?.role === 'function';\n};\n\nexport const isToolMessage = (\n  message: ChatCompletionMessageParam | null | undefined,\n): message is ChatCompletionToolMessageParam => {\n  return message?.role === 'tool';\n};\n\nexport function isPresent<T>(obj: T | null | undefined): obj is T {\n  return obj != null;\n}\n","import * as Core from \"../core\";\nimport { type CompletionUsage } from \"../resources/completions\";\nimport {\n  type Completions,\n  type ChatCompletion,\n  type ChatCompletionMessage,\n  type ChatCompletionMessageParam,\n  type ChatCompletionCreateParams,\n  type ChatCompletionTool,\n} from \"../resources/chat/completions\";\nimport { APIUserAbortError, OpenAIError } from \"../error\";\nimport {\n  type RunnableFunction,\n  isRunnableFunctionWithParse,\n  type BaseFunctionsArgs,\n} from './RunnableFunction';\nimport { ChatCompletionFunctionRunnerParams, ChatCompletionToolRunnerParams } from './ChatCompletionRunner';\nimport {\n  ChatCompletionStreamingFunctionRunnerParams,\n  ChatCompletionStreamingToolRunnerParams,\n} from './ChatCompletionStreamingRunner';\nimport { isAssistantMessage, isFunctionMessage, isToolMessage } from './chatCompletionUtils';\n\nconst DEFAULT_MAX_CHAT_COMPLETIONS = 10;\nexport interface RunnerOptions extends Core.RequestOptions {\n  /** How many requests to make before canceling. Default 10. */\n  maxChatCompletions?: number;\n}\n\nexport abstract class AbstractChatCompletionRunner<\n  Events extends CustomEvents<any> = AbstractChatCompletionRunnerEvents,\n> {\n  controller: AbortController = new AbortController();\n\n  #connectedPromise: Promise<void>;\n  #resolveConnectedPromise: () => void = () => {};\n  #rejectConnectedPromise: (error: OpenAIError) => void = () => {};\n\n  #endPromise: Promise<void>;\n  #resolveEndPromise: () => void = () => {};\n  #rejectEndPromise: (error: OpenAIError) => void = () => {};\n\n  #listeners: { [Event in keyof Events]?: ListenersForEvent<Events, Event> } = {};\n\n  protected _chatCompletions: ChatCompletion[] = [];\n  messages: ChatCompletionMessageParam[] = [];\n\n  #ended = false;\n  #errored = false;\n  #aborted = false;\n  #catchingPromiseCreated = false;\n\n  constructor() {\n    this.#connectedPromise = new Promise<void>((resolve, reject) => {\n      this.#resolveConnectedPromise = resolve;\n      this.#rejectConnectedPromise = reject;\n    });\n\n    this.#endPromise = new Promise<void>((resolve, reject) => {\n      this.#resolveEndPromise = resolve;\n      this.#rejectEndPromise = reject;\n    });\n\n    // Don't let these promises cause unhandled rejection errors.\n    // we will manually cause an unhandled rejection error later\n    // if the user hasn't registered any error listener or called\n    // any promise-returning method.\n    this.#connectedPromise.catch(() => {});\n    this.#endPromise.catch(() => {});\n  }\n\n  protected _run(executor: () => Promise<any>) {\n    // Unfortunately if we call `executor()` immediately we get runtime errors about\n    // references to `this` before the `super()` constructor call returns.\n    setTimeout(() => {\n      executor().then(() => {\n        this._emitFinal();\n        this._emit('end');\n      }, this.#handleError);\n    }, 0);\n  }\n\n  protected _addChatCompletion(chatCompletion: ChatCompletion): ChatCompletion {\n    this._chatCompletions.push(chatCompletion);\n    this._emit('chatCompletion', chatCompletion);\n    const message = chatCompletion.choices[0]?.message;\n    if (message) this._addMessage(message as ChatCompletionMessageParam);\n    return chatCompletion;\n  }\n\n  protected _addMessage(message: ChatCompletionMessageParam, emit = true) {\n    if (!('content' in message)) message.content = null;\n\n    this.messages.push(message);\n\n    if (emit) {\n      this._emit('message', message);\n      if ((isFunctionMessage(message) || isToolMessage(message)) && message.content) {\n        // Note, this assumes that {role: 'tool', content: } is always the result of a call of tool of type=function.\n        this._emit('functionCallResult', message.content as string);\n      } else if (isAssistantMessage(message) && message.function_call) {\n        this._emit('functionCall', message.function_call);\n      } else if (isAssistantMessage(message) && message.tool_calls) {\n        for (const tool_call of message.tool_calls) {\n          if (tool_call.type === 'function') {\n            this._emit('functionCall', tool_call.function);\n          }\n        }\n      }\n    }\n  }\n\n  protected _connected() {\n    if (this.ended) return;\n    this.#resolveConnectedPromise();\n    this._emit('connect');\n  }\n\n  get ended(): boolean {\n    return this.#ended;\n  }\n\n  get errored(): boolean {\n    return this.#errored;\n  }\n\n  get aborted(): boolean {\n    return this.#aborted;\n  }\n\n  abort() {\n    this.controller.abort();\n  }\n\n  /**\n   * Adds the listener function to the end of the listeners array for the event.\n   * No checks are made to see if the listener has already been added. Multiple calls passing\n   * the same combination of event and listener will result in the listener being added, and\n   * called, multiple times.\n   * @returns this ChatCompletionStream, so that calls can be chained\n   */\n  on<Event extends keyof Events>(event: Event, listener: ListenerForEvent<Events, Event>): this {\n    const listeners: ListenersForEvent<Events, Event> =\n      this.#listeners[event] || (this.#listeners[event] = []);\n    listeners.push({ listener });\n    return this;\n  }\n\n  /**\n   * Removes the specified listener from the listener array for the event.\n   * off() will remove, at most, one instance of a listener from the listener array. If any single\n   * listener has been added multiple times to the listener array for the specified event, then\n   * off() must be called multiple times to remove each instance.\n   * @returns this ChatCompletionStream, so that calls can be chained\n   */\n  off<Event extends keyof Events>(event: Event, listener: ListenerForEvent<Events, Event>): this {\n    const listeners = this.#listeners[event];\n    if (!listeners) return this;\n    const index = listeners.findIndex((l) => l.listener === listener);\n    if (index >= 0) listeners.splice(index, 1);\n    return this;\n  }\n\n  /**\n   * Adds a one-time listener function for the event. The next time the event is triggered,\n   * this listener is removed and then invoked.\n   * @returns this ChatCompletionStream, so that calls can be chained\n   */\n  once<Event extends keyof Events>(event: Event, listener: ListenerForEvent<Events, Event>): this {\n    const listeners: ListenersForEvent<Events, Event> =\n      this.#listeners[event] || (this.#listeners[event] = []);\n    listeners.push({ listener, once: true });\n    return this;\n  }\n\n  /**\n   * This is similar to `.once()`, but returns a Promise that resolves the next time\n   * the event is triggered, instead of calling a listener callback.\n   * @returns a Promise that resolves the next time given event is triggered,\n   * or rejects if an error is emitted.  (If you request the 'error' event,\n   * returns a promise that resolves with the error).\n   *\n   * Example:\n   *\n   *   const message = await stream.emitted('message') // rejects if the stream errors\n   */\n  emitted<Event extends keyof Events>(\n    event: Event,\n  ): Promise<\n    EventParameters<Events, Event> extends [infer Param] ? Param\n    : EventParameters<Events, Event> extends [] ? void\n    : EventParameters<Events, Event>\n  > {\n    return new Promise((resolve, reject) => {\n      this.#catchingPromiseCreated = true;\n      if (event !== 'error') this.once('error', reject);\n      this.once(event, resolve as any);\n    });\n  }\n\n  async done(): Promise<void> {\n    this.#catchingPromiseCreated = true;\n    await this.#endPromise;\n  }\n\n  /**\n   * @returns a promise that resolves with the final ChatCompletion, or rejects\n   * if an error occurred or the stream ended prematurely without producing a ChatCompletion.\n   */\n  async finalChatCompletion(): Promise<ChatCompletion> {\n    await this.done();\n    const completion = this._chatCompletions[this._chatCompletions.length - 1];\n    if (!completion) throw new OpenAIError('stream ended without producing a ChatCompletion');\n    return completion;\n  }\n\n  #getFinalContent(): string | null {\n    return this.#getFinalMessage().content ?? null;\n  }\n\n  /**\n   * @returns a promise that resolves with the content of the final ChatCompletionMessage, or rejects\n   * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n   */\n  async finalContent(): Promise<string | null> {\n    await this.done();\n    return this.#getFinalContent();\n  }\n\n  #getFinalMessage(): ChatCompletionMessage {\n    let i = this.messages.length;\n    while (i-- > 0) {\n      const message = this.messages[i];\n      if (isAssistantMessage(message)) {\n        return { ...message, content: message.content ?? null };\n      }\n    }\n    throw new OpenAIError('stream ended without producing a ChatCompletionMessage with role=assistant');\n  }\n\n  /**\n   * @returns a promise that resolves with the the final assistant ChatCompletionMessage response,\n   * or rejects if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n   */\n  async finalMessage(): Promise<ChatCompletionMessage> {\n    await this.done();\n    return this.#getFinalMessage();\n  }\n\n  #getFinalFunctionCall(): ChatCompletionMessage.FunctionCall | undefined {\n    for (let i = this.messages.length - 1; i >= 0; i--) {\n      const message = this.messages[i];\n      if (isAssistantMessage(message) && message?.function_call) {\n        return message.function_call;\n      }\n      if (isAssistantMessage(message) && message?.tool_calls?.length) {\n        return message.tool_calls.at(-1)?.function;\n      }\n    }\n\n    return;\n  }\n\n  /**\n   * @returns a promise that resolves with the content of the final FunctionCall, or rejects\n   * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n   */\n  async finalFunctionCall(): Promise<ChatCompletionMessage.FunctionCall | undefined> {\n    await this.done();\n    return this.#getFinalFunctionCall();\n  }\n\n  #getFinalFunctionCallResult(): string | undefined {\n    for (let i = this.messages.length - 1; i >= 0; i--) {\n      const message = this.messages[i];\n      if (isFunctionMessage(message) && message.content != null) {\n        return message.content;\n      }\n      if (\n        isToolMessage(message) &&\n        message.content != null &&\n        this.messages.some(\n          (x) =>\n            x.role === 'assistant' &&\n            x.tool_calls?.some((y) => y.type === 'function' && y.id === message.tool_call_id),\n        )\n      ) {\n        return message.content;\n      }\n    }\n\n    return;\n  }\n\n  async finalFunctionCallResult(): Promise<string | undefined> {\n    await this.done();\n    return this.#getFinalFunctionCallResult();\n  }\n\n  #calculateTotalUsage(): CompletionUsage {\n    const total: CompletionUsage = {\n      completion_tokens: 0,\n      prompt_tokens: 0,\n      total_tokens: 0,\n    };\n    for (const { usage } of this._chatCompletions) {\n      if (usage) {\n        total.completion_tokens += usage.completion_tokens;\n        total.prompt_tokens += usage.prompt_tokens;\n        total.total_tokens += usage.total_tokens;\n      }\n    }\n    return total;\n  }\n\n  async totalUsage(): Promise<CompletionUsage> {\n    await this.done();\n    return this.#calculateTotalUsage();\n  }\n\n  allChatCompletions(): ChatCompletion[] {\n    return [...this._chatCompletions];\n  }\n\n  #handleError = (error: unknown) => {\n    this.#errored = true;\n    if (error instanceof Error && error.name === 'AbortError') {\n      error = new APIUserAbortError();\n    }\n    if (error instanceof APIUserAbortError) {\n      this.#aborted = true;\n      return this._emit('abort', error);\n    }\n    if (error instanceof OpenAIError) {\n      return this._emit('error', error);\n    }\n    if (error instanceof Error) {\n      const openAIError: OpenAIError = new OpenAIError(error.message);\n      // @ts-ignore\n      openAIError.cause = error;\n      return this._emit('error', openAIError);\n    }\n    return this._emit('error', new OpenAIError(String(error)));\n  };\n\n  protected _emit<Event extends keyof Events>(event: Event, ...args: EventParameters<Events, Event>) {\n    // make sure we don't emit any events after end\n    if (this.#ended) {\n      return;\n    }\n\n    if (event === 'end') {\n      this.#ended = true;\n      this.#resolveEndPromise();\n    }\n\n    const listeners: ListenersForEvent<Events, Event> | undefined = this.#listeners[event];\n    if (listeners) {\n      this.#listeners[event] = listeners.filter((l) => !l.once) as any;\n      listeners.forEach(({ listener }: any) => listener(...args));\n    }\n\n    if (event === 'abort') {\n      const error = args[0] as APIUserAbortError;\n      if (!this.#catchingPromiseCreated && !listeners?.length) {\n        Promise.reject(error);\n      }\n      this.#rejectConnectedPromise(error);\n      this.#rejectEndPromise(error);\n      this._emit('end');\n      return;\n    }\n\n    if (event === 'error') {\n      // NOTE: _emit('error', error) should only be called from #handleError().\n\n      const error = args[0] as OpenAIError;\n      if (!this.#catchingPromiseCreated && !listeners?.length) {\n        // Trigger an unhandled rejection if the user hasn't registered any error handlers.\n        // If you are seeing stack traces here, make sure to handle errors via either:\n        // - runner.on('error', () => ...)\n        // - await runner.done()\n        // - await runner.finalChatCompletion()\n        // - etc.\n        Promise.reject(error);\n      }\n      this.#rejectConnectedPromise(error);\n      this.#rejectEndPromise(error);\n      this._emit('end');\n    }\n  }\n\n  protected _emitFinal() {\n    const completion = this._chatCompletions[this._chatCompletions.length - 1];\n    if (completion) this._emit('finalChatCompletion', completion);\n    const finalMessage = this.#getFinalMessage();\n    if (finalMessage) this._emit('finalMessage', finalMessage);\n    const finalContent = this.#getFinalContent();\n    if (finalContent) this._emit('finalContent', finalContent);\n\n    const finalFunctionCall = this.#getFinalFunctionCall();\n    if (finalFunctionCall) this._emit('finalFunctionCall', finalFunctionCall);\n\n    const finalFunctionCallResult = this.#getFinalFunctionCallResult();\n    if (finalFunctionCallResult != null) this._emit('finalFunctionCallResult', finalFunctionCallResult);\n\n    if (this._chatCompletions.some((c) => c.usage)) {\n      this._emit('totalUsage', this.#calculateTotalUsage());\n    }\n  }\n\n  #validateParams(params: ChatCompletionCreateParams): void {\n    if (params.n != null && params.n > 1) {\n      throw new OpenAIError(\n        'ChatCompletion convenience helpers only support n=1 at this time. To use n>1, please use chat.completions.create() directly.',\n      );\n    }\n  }\n\n  protected async _createChatCompletion(\n    completions: Completions,\n    params: ChatCompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): Promise<ChatCompletion> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this.#validateParams(params);\n\n    const chatCompletion = await completions.create(\n      { ...params, stream: false },\n      { ...options, signal: this.controller.signal },\n    );\n    this._connected();\n    return this._addChatCompletion(chatCompletion);\n  }\n\n  protected async _runChatCompletion(\n    completions: Completions,\n    params: ChatCompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): Promise<ChatCompletion> {\n    for (const message of params.messages) {\n      this._addMessage(message, false);\n    }\n    return await this._createChatCompletion(completions, params, options);\n  }\n\n  protected async _runFunctions<FunctionsArgs extends BaseFunctionsArgs>(\n    completions: Completions,\n    params:\n      | ChatCompletionFunctionRunnerParams<FunctionsArgs>\n      | ChatCompletionStreamingFunctionRunnerParams<FunctionsArgs>,\n    options?: RunnerOptions,\n  ) {\n    const role = 'function' as const;\n    const { function_call = 'auto', stream, ...restParams } = params;\n    const singleFunctionToCall = typeof function_call !== 'string' && function_call?.name;\n    const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};\n\n    const functionsByName: Record<string, RunnableFunction<any>> = {};\n    for (const f of params.functions) {\n      functionsByName[f.name || f.function.name] = f;\n    }\n\n    const functions: ChatCompletionCreateParams.Function[] = params.functions.map(\n      (f): ChatCompletionCreateParams.Function => ({\n        name: f.name || f.function.name,\n        parameters: f.parameters as Record<string, unknown>,\n        description: f.description,\n      }),\n    );\n\n    for (const message of params.messages) {\n      this._addMessage(message, false);\n    }\n\n    for (let i = 0; i < maxChatCompletions; ++i) {\n      const chatCompletion: ChatCompletion = await this._createChatCompletion(\n        completions,\n        {\n          ...restParams,\n          function_call,\n          functions,\n          messages: [...this.messages],\n        },\n        options,\n      );\n      const message = chatCompletion.choices[0]?.message;\n      if (!message) {\n        throw new OpenAIError(`missing message in ChatCompletion response`);\n      }\n      if (!message.function_call) return;\n      const { name, arguments: args } = message.function_call;\n      const fn = functionsByName[name];\n      if (!fn) {\n        const content = `Invalid function_call: ${JSON.stringify(name)}. Available options are: ${functions\n          .map((f) => JSON.stringify(f.name))\n          .join(', ')}. Please try again`;\n\n        this._addMessage({ role, name, content });\n        continue;\n      } else if (singleFunctionToCall && singleFunctionToCall !== name) {\n        const content = `Invalid function_call: ${JSON.stringify(name)}. ${JSON.stringify(\n          singleFunctionToCall,\n        )} requested. Please try again`;\n\n        this._addMessage({ role, name, content });\n        continue;\n      }\n\n      let parsed;\n      try {\n        parsed = isRunnableFunctionWithParse(fn) ? await fn.parse(args) : args;\n      } catch (error) {\n        this._addMessage({\n          role,\n          name,\n          content: error instanceof Error ? error.message : String(error),\n        });\n        continue;\n      }\n\n      // @ts-expect-error it can't rule out `never` type.\n      const rawContent = await fn.function(parsed, this);\n      const content = this.#stringifyFunctionCallResult(rawContent);\n\n      this._addMessage({ role, name, content });\n\n      if (singleFunctionToCall) return;\n    }\n  }\n\n  protected async _runTools<FunctionsArgs extends BaseFunctionsArgs>(\n    completions: Completions,\n    params:\n      | ChatCompletionToolRunnerParams<FunctionsArgs>\n      | ChatCompletionStreamingToolRunnerParams<FunctionsArgs>,\n    options?: RunnerOptions,\n  ) {\n    const role = 'tool' as const;\n    const { tool_choice = 'auto', stream, ...restParams } = params;\n    const singleFunctionToCall = typeof tool_choice !== 'string' && tool_choice?.function?.name;\n    const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};\n\n    const functionsByName: Record<string, RunnableFunction<any>> = {};\n    for (const f of params.tools) {\n      if (f.type === 'function') {\n        functionsByName[f.function.name || f.function.function.name] = f.function;\n      }\n    }\n\n    const tools: ChatCompletionTool[] =\n      'tools' in params ?\n        params.tools.map((t) =>\n          t.type === 'function' ?\n            {\n              type: 'function',\n              function: {\n                name: t.function.name || t.function.function.name,\n                parameters: t.function.parameters as Record<string, unknown>,\n                description: t.function.description,\n              },\n            }\n          : (t as unknown as ChatCompletionTool),\n        )\n      : (undefined as any);\n\n    for (const message of params.messages) {\n      this._addMessage(message, false);\n    }\n\n    for (let i = 0; i < maxChatCompletions; ++i) {\n      const chatCompletion: ChatCompletion = await this._createChatCompletion(\n        completions,\n        {\n          ...restParams,\n          tool_choice,\n          tools,\n          messages: [...this.messages],\n        },\n        options,\n      );\n      const message = chatCompletion.choices[0]?.message;\n      if (!message) {\n        throw new OpenAIError(`missing message in ChatCompletion response`);\n      }\n      if (!message.tool_calls) {\n        return;\n      }\n\n      for (const tool_call of message.tool_calls) {\n        if (tool_call.type !== 'function') continue;\n        const tool_call_id = tool_call.id;\n        const { name, arguments: args } = tool_call.function;\n        const fn = functionsByName[name];\n\n        if (!fn) {\n          const content = `Invalid tool_call: ${JSON.stringify(name)}. Available options are: ${tools\n            .map((f) => JSON.stringify(f.function.name))\n            .join(', ')}. Please try again`;\n\n          this._addMessage({ role, tool_call_id, content });\n          continue;\n        } else if (singleFunctionToCall && singleFunctionToCall !== name) {\n          const content = `Invalid tool_call: ${JSON.stringify(name)}. ${JSON.stringify(\n            singleFunctionToCall,\n          )} requested. Please try again`;\n\n          this._addMessage({ role, tool_call_id, content });\n          continue;\n        }\n\n        let parsed;\n        try {\n          parsed = isRunnableFunctionWithParse(fn) ? await fn.parse(args) : args;\n        } catch (error) {\n          const content = error instanceof Error ? error.message : String(error);\n          this._addMessage({ role, tool_call_id, content });\n          continue;\n        }\n\n        // @ts-expect-error it can't rule out `never` type.\n        const rawContent = await fn.function(parsed, this);\n        const content = this.#stringifyFunctionCallResult(rawContent);\n        this._addMessage({ role, tool_call_id, content });\n\n        if (singleFunctionToCall) {\n          return;\n        }\n      }\n    }\n\n    return;\n  }\n\n  #stringifyFunctionCallResult(rawContent: unknown): string {\n    return (\n      typeof rawContent === 'string' ? rawContent\n      : rawContent === undefined ? 'undefined'\n      : JSON.stringify(rawContent)\n    );\n  }\n}\n\ntype CustomEvents<Event extends string> = {\n  [k in Event]: k extends keyof AbstractChatCompletionRunnerEvents ? AbstractChatCompletionRunnerEvents[k]\n  : (...args: any[]) => void;\n};\n\ntype ListenerForEvent<Events extends CustomEvents<any>, Event extends keyof Events> = Event extends (\n  keyof AbstractChatCompletionRunnerEvents\n) ?\n  AbstractChatCompletionRunnerEvents[Event]\n: Events[Event];\n\ntype ListenersForEvent<Events extends CustomEvents<any>, Event extends keyof Events> = Array<{\n  listener: ListenerForEvent<Events, Event>;\n  once?: boolean;\n}>;\ntype EventParameters<Events extends CustomEvents<any>, Event extends keyof Events> = Parameters<\n  ListenerForEvent<Events, Event>\n>;\n\nexport interface AbstractChatCompletionRunnerEvents {\n  connect: () => void;\n  functionCall: (functionCall: ChatCompletionMessage.FunctionCall) => void;\n  message: (message: ChatCompletionMessageParam) => void;\n  chatCompletion: (completion: ChatCompletion) => void;\n  finalContent: (contentSnapshot: string) => void;\n  finalMessage: (message: ChatCompletionMessageParam) => void;\n  finalChatCompletion: (completion: ChatCompletion) => void;\n  finalFunctionCall: (functionCall: ChatCompletionMessage.FunctionCall) => void;\n  functionCallResult: (content: string) => void;\n  finalFunctionCallResult: (content: string) => void;\n  error: (error: OpenAIError) => void;\n  abort: (error: APIUserAbortError) => void;\n  end: () => void;\n  totalUsage: (usage: CompletionUsage) => void;\n}\n","import {\n  type Completions,\n  type ChatCompletionMessageParam,\n  type ChatCompletionCreateParamsNonStreaming,\n} from \"../resources/chat/completions\";\nimport { type RunnableFunctions, type BaseFunctionsArgs, RunnableTools } from './RunnableFunction';\nimport {\n  AbstractChatCompletionRunner,\n  AbstractChatCompletionRunnerEvents,\n  RunnerOptions,\n} from './AbstractChatCompletionRunner';\nimport { isAssistantMessage } from './chatCompletionUtils';\n\nexport interface ChatCompletionRunnerEvents extends AbstractChatCompletionRunnerEvents {\n  content: (content: string) => void;\n}\n\nexport type ChatCompletionFunctionRunnerParams<FunctionsArgs extends BaseFunctionsArgs> = Omit<\n  ChatCompletionCreateParamsNonStreaming,\n  'functions'\n> & {\n  functions: RunnableFunctions<FunctionsArgs>;\n};\n\nexport type ChatCompletionToolRunnerParams<FunctionsArgs extends BaseFunctionsArgs> = Omit<\n  ChatCompletionCreateParamsNonStreaming,\n  'tools'\n> & {\n  tools: RunnableTools<FunctionsArgs>;\n};\n\nexport class ChatCompletionRunner extends AbstractChatCompletionRunner<ChatCompletionRunnerEvents> {\n  /** @deprecated - please use `runTools` instead. */\n  static runFunctions(\n    completions: Completions,\n    params: ChatCompletionFunctionRunnerParams<any[]>,\n    options?: RunnerOptions,\n  ): ChatCompletionRunner {\n    const runner = new ChatCompletionRunner();\n    const opts = {\n      ...options,\n      headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runFunctions' },\n    };\n    runner._run(() => runner._runFunctions(completions, params, opts));\n    return runner;\n  }\n\n  static runTools(\n    completions: Completions,\n    params: ChatCompletionToolRunnerParams<any[]>,\n    options?: RunnerOptions,\n  ): ChatCompletionRunner {\n    const runner = new ChatCompletionRunner();\n    const opts = {\n      ...options,\n      headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runTools' },\n    };\n    runner._run(() => runner._runTools(completions, params, opts));\n    return runner;\n  }\n\n  override _addMessage(message: ChatCompletionMessageParam) {\n    super._addMessage(message);\n    if (isAssistantMessage(message) && message.content) {\n      this._emit('content', message.content as string);\n    }\n  }\n}\n","import * as Core from \"../core\";\nimport { OpenAIError, APIUserAbortError } from \"../error\";\nimport {\n  Completions,\n  type ChatCompletion,\n  type ChatCompletionChunk,\n  type ChatCompletionCreateParams,\n  type ChatCompletionCreateParamsBase,\n} from \"../resources/chat/completions\";\nimport {\n  AbstractChatCompletionRunner,\n  type AbstractChatCompletionRunnerEvents,\n} from './AbstractChatCompletionRunner';\nimport { type ReadableStream } from \"../_shims/index\";\nimport { Stream } from \"../streaming\";\n\nexport interface ChatCompletionStreamEvents extends AbstractChatCompletionRunnerEvents {\n  content: (contentDelta: string, contentSnapshot: string) => void;\n  chunk: (chunk: ChatCompletionChunk, snapshot: ChatCompletionSnapshot) => void;\n}\n\nexport type ChatCompletionStreamParams = Omit<ChatCompletionCreateParamsBase, 'stream'> & {\n  stream?: true;\n};\n\nexport class ChatCompletionStream\n  extends AbstractChatCompletionRunner<ChatCompletionStreamEvents>\n  implements AsyncIterable<ChatCompletionChunk>\n{\n  #currentChatCompletionSnapshot: ChatCompletionSnapshot | undefined;\n\n  get currentChatCompletionSnapshot(): ChatCompletionSnapshot | undefined {\n    return this.#currentChatCompletionSnapshot;\n  }\n\n  /**\n   * Intended for use on the frontend, consuming a stream produced with\n   * `.toReadableStream()` on the backend.\n   *\n   * Note that messages sent to the model do not appear in `.on('message')`\n   * in this context.\n   */\n  static fromReadableStream(stream: ReadableStream): ChatCompletionStream {\n    const runner = new ChatCompletionStream();\n    runner._run(() => runner._fromReadableStream(stream));\n    return runner;\n  }\n\n  static createChatCompletion(\n    completions: Completions,\n    params: ChatCompletionStreamParams,\n    options?: Core.RequestOptions,\n  ): ChatCompletionStream {\n    const runner = new ChatCompletionStream();\n    runner._run(() =>\n      runner._runChatCompletion(\n        completions,\n        { ...params, stream: true },\n        { ...options, headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' } },\n      ),\n    );\n    return runner;\n  }\n\n  #beginRequest() {\n    if (this.ended) return;\n    this.#currentChatCompletionSnapshot = undefined;\n  }\n  #addChunk(chunk: ChatCompletionChunk) {\n    if (this.ended) return;\n    const completion = this.#accumulateChatCompletion(chunk);\n    this._emit('chunk', chunk, completion);\n    const delta = chunk.choices[0]?.delta?.content;\n    const snapshot = completion.choices[0]?.message;\n    if (delta != null && snapshot?.role === 'assistant' && snapshot?.content) {\n      this._emit('content', delta, snapshot.content);\n    }\n  }\n  #endRequest(): ChatCompletion {\n    if (this.ended) {\n      throw new OpenAIError(`stream has ended, this shouldn't happen`);\n    }\n    const snapshot = this.#currentChatCompletionSnapshot;\n    if (!snapshot) {\n      throw new OpenAIError(`request ended without sending any chunks`);\n    }\n    this.#currentChatCompletionSnapshot = undefined;\n    return finalizeChatCompletion(snapshot);\n  }\n\n  protected override async _createChatCompletion(\n    completions: Completions,\n    params: ChatCompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): Promise<ChatCompletion> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this.#beginRequest();\n    const stream = await completions.create(\n      { ...params, stream: true },\n      { ...options, signal: this.controller.signal },\n    );\n    this._connected();\n    for await (const chunk of stream) {\n      this.#addChunk(chunk);\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n    return this._addChatCompletion(this.#endRequest());\n  }\n\n  protected async _fromReadableStream(\n    readableStream: ReadableStream,\n    options?: Core.RequestOptions,\n  ): Promise<ChatCompletion> {\n    const signal = options?.signal;\n    if (signal) {\n      if (signal.aborted) this.controller.abort();\n      signal.addEventListener('abort', () => this.controller.abort());\n    }\n    this.#beginRequest();\n    this._connected();\n    const stream = Stream.fromReadableStream<ChatCompletionChunk>(readableStream, this.controller);\n    let chatId;\n    for await (const chunk of stream) {\n      if (chatId && chatId !== chunk.id) {\n        // A new request has been made.\n        this._addChatCompletion(this.#endRequest());\n      }\n\n      this.#addChunk(chunk);\n      chatId = chunk.id;\n    }\n    if (stream.controller.signal?.aborted) {\n      throw new APIUserAbortError();\n    }\n    return this._addChatCompletion(this.#endRequest());\n  }\n\n  #accumulateChatCompletion(chunk: ChatCompletionChunk): ChatCompletionSnapshot {\n    let snapshot = this.#currentChatCompletionSnapshot;\n    const { choices, ...rest } = chunk;\n    if (!snapshot) {\n      snapshot = this.#currentChatCompletionSnapshot = {\n        ...rest,\n        choices: [],\n      };\n    } else {\n      Object.assign(snapshot, rest);\n    }\n\n    for (const { delta, finish_reason, index, logprobs = null, ...other } of chunk.choices) {\n      let choice = snapshot.choices[index];\n      if (!choice) {\n        choice = snapshot.choices[index] = { finish_reason, index, message: {}, logprobs, ...other };\n      }\n\n      if (logprobs) {\n        if (!choice.logprobs) {\n          choice.logprobs = Object.assign({}, logprobs);\n        } else {\n          const { content, ...rest } = logprobs;\n          Object.assign(choice.logprobs, rest);\n          if (content) {\n            choice.logprobs.content ??= [];\n            choice.logprobs.content.push(...content);\n          }\n        }\n      }\n\n      if (finish_reason) choice.finish_reason = finish_reason;\n      Object.assign(choice, other);\n\n      if (!delta) continue; // Shouldn't happen; just in case.\n      const { content, function_call, role, tool_calls, ...rest } = delta;\n      Object.assign(choice.message, rest);\n\n      if (content) choice.message.content = (choice.message.content || '') + content;\n      if (role) choice.message.role = role;\n      if (function_call) {\n        if (!choice.message.function_call) {\n          choice.message.function_call = function_call;\n        } else {\n          if (function_call.name) choice.message.function_call.name = function_call.name;\n          if (function_call.arguments) {\n            choice.message.function_call.arguments ??= '';\n            choice.message.function_call.arguments += function_call.arguments;\n          }\n        }\n      }\n      if (tool_calls) {\n        if (!choice.message.tool_calls) choice.message.tool_calls = [];\n        for (const { index, id, type, function: fn, ...rest } of tool_calls) {\n          const tool_call = (choice.message.tool_calls[index] ??= {});\n          Object.assign(tool_call, rest);\n          if (id) tool_call.id = id;\n          if (type) tool_call.type = type;\n          if (fn) tool_call.function ??= { arguments: '' };\n          if (fn?.name) tool_call.function!.name = fn.name;\n          if (fn?.arguments) tool_call.function!.arguments += fn.arguments;\n        }\n      }\n    }\n    return snapshot;\n  }\n\n  [Symbol.asyncIterator](): AsyncIterator<ChatCompletionChunk> {\n    const pushQueue: ChatCompletionChunk[] = [];\n    const readQueue: ((chunk: ChatCompletionChunk | undefined) => void)[] = [];\n    let done = false;\n\n    this.on('chunk', (chunk) => {\n      const reader = readQueue.shift();\n      if (reader) {\n        reader(chunk);\n      } else {\n        pushQueue.push(chunk);\n      }\n    });\n\n    this.on('end', () => {\n      done = true;\n      for (const reader of readQueue) {\n        reader(undefined);\n      }\n      readQueue.length = 0;\n    });\n\n    return {\n      next: async (): Promise<IteratorResult<ChatCompletionChunk>> => {\n        if (!pushQueue.length) {\n          if (done) {\n            return { value: undefined, done: true };\n          }\n          return new Promise<ChatCompletionChunk | undefined>((resolve) => readQueue.push(resolve)).then(\n            (chunk) => (chunk ? { value: chunk, done: false } : { value: undefined, done: true }),\n          );\n        }\n        const chunk = pushQueue.shift()!;\n        return { value: chunk, done: false };\n      },\n    };\n  }\n\n  toReadableStream(): ReadableStream {\n    const stream = new Stream(this[Symbol.asyncIterator].bind(this), this.controller);\n    return stream.toReadableStream();\n  }\n}\n\nfunction finalizeChatCompletion(snapshot: ChatCompletionSnapshot): ChatCompletion {\n  const { id, choices, created, model, system_fingerprint, ...rest } = snapshot;\n  return {\n    ...rest,\n    id,\n    choices: choices.map(\n      ({ message, finish_reason, index, logprobs, ...choiceRest }): ChatCompletion.Choice => {\n        if (!finish_reason) throw new OpenAIError(`missing finish_reason for choice ${index}`);\n        const { content = null, function_call, tool_calls, ...messageRest } = message;\n        const role = message.role as 'assistant'; // this is what we expect; in theory it could be different which would make our types a slight lie but would be fine.\n        if (!role) throw new OpenAIError(`missing role for choice ${index}`);\n        if (function_call) {\n          const { arguments: args, name } = function_call;\n          if (args == null) throw new OpenAIError(`missing function_call.arguments for choice ${index}`);\n          if (!name) throw new OpenAIError(`missing function_call.name for choice ${index}`);\n          return {\n            ...choiceRest,\n            message: { content, function_call: { arguments: args, name }, role },\n            finish_reason,\n            index,\n            logprobs,\n          };\n        }\n        if (tool_calls) {\n          return {\n            ...choiceRest,\n            index,\n            finish_reason,\n            logprobs,\n            message: {\n              ...messageRest,\n              role,\n              content,\n              tool_calls: tool_calls.map((tool_call, i) => {\n                const { function: fn, type, id, ...toolRest } = tool_call;\n                const { arguments: args, name, ...fnRest } = fn || {};\n                if (id == null)\n                  throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].id\\n${str(snapshot)}`);\n                if (type == null)\n                  throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].type\\n${str(snapshot)}`);\n                if (name == null)\n                  throw new OpenAIError(\n                    `missing choices[${index}].tool_calls[${i}].function.name\\n${str(snapshot)}`,\n                  );\n                if (args == null)\n                  throw new OpenAIError(\n                    `missing choices[${index}].tool_calls[${i}].function.arguments\\n${str(snapshot)}`,\n                  );\n\n                return { ...toolRest, id, type, function: { ...fnRest, name, arguments: args } };\n              }),\n            },\n          };\n        }\n        return {\n          ...choiceRest,\n          message: { ...messageRest, content, role },\n          finish_reason,\n          index,\n          logprobs,\n        };\n      },\n    ),\n    created,\n    model,\n    object: 'chat.completion',\n    ...(system_fingerprint ? { system_fingerprint } : {}),\n  };\n}\n\nfunction str(x: unknown) {\n  return JSON.stringify(x);\n}\n\n/**\n * Represents a streamed chunk of a chat completion response returned by model,\n * based on the provided input.\n */\nexport interface ChatCompletionSnapshot {\n  /**\n   * A unique identifier for the chat completion.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletionSnapshot.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created.\n   */\n  created: number;\n\n  /**\n   * The model to generate the completion.\n   */\n  model: string;\n\n  // Note we do not include an \"object\" type on the snapshot,\n  // because the object is not a valid \"chat.completion\" until finalized.\n  // object: 'chat.completion';\n\n  /**\n   * This fingerprint represents the backend configuration that the model runs with.\n   *\n   * Can be used in conjunction with the `seed` request parameter to understand when\n   * backend changes have been made that might impact determinism.\n   */\n  system_fingerprint?: string;\n}\n\nexport namespace ChatCompletionSnapshot {\n  export interface Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    message: Choice.Message;\n\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, `content_filter` if\n     * content was omitted due to a flag from our content filters, or `function_call`\n     * if the model called a function.\n     */\n    finish_reason: ChatCompletion.Choice['finish_reason'] | null;\n\n    /**\n     * Log probability information for the choice.\n     */\n    logprobs: ChatCompletion.Choice.Logprobs | null;\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n  }\n\n  export namespace Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    export interface Message {\n      /**\n       * The contents of the chunk message.\n       */\n      content?: string | null;\n\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      function_call?: Message.FunctionCall;\n\n      tool_calls?: Array<Message.ToolCall>;\n\n      /**\n       * The role of the author of this message.\n       */\n      role?: 'system' | 'user' | 'assistant' | 'function' | 'tool';\n    }\n\n    export namespace Message {\n      export interface ToolCall {\n        /**\n         * The ID of the tool call.\n         */\n        id?: string;\n\n        function?: ToolCall.Function;\n\n        /**\n         * The type of the tool.\n         */\n        type?: 'function';\n      }\n\n      export namespace ToolCall {\n        export interface Function {\n          /**\n           * The arguments to call the function with, as generated by the model in JSON\n           * format. Note that the model does not always generate valid JSON, and may\n           * hallucinate parameters not defined by your function schema. Validate the\n           * arguments in your code before calling your function.\n           */\n          arguments?: string;\n\n          /**\n           * The name of the function to call.\n           */\n          name?: string;\n        }\n      }\n\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      export interface FunctionCall {\n        /**\n         * The arguments to call the function with, as generated by the model in JSON\n         * format. Note that the model does not always generate valid JSON, and may\n         * hallucinate parameters not defined by your function schema. Validate the\n         * arguments in your code before calling your function.\n         */\n        arguments?: string;\n\n        /**\n         * The name of the function to call.\n         */\n        name?: string;\n      }\n    }\n  }\n}\n","import {\n  Completions,\n  type ChatCompletionChunk,\n  type ChatCompletionCreateParamsStreaming,\n} from \"../resources/chat/completions\";\nimport { RunnerOptions, type AbstractChatCompletionRunnerEvents } from './AbstractChatCompletionRunner';\nimport { type ReadableStream } from \"../_shims/index\";\nimport { RunnableTools, type BaseFunctionsArgs, type RunnableFunctions } from './RunnableFunction';\nimport { ChatCompletionSnapshot, ChatCompletionStream } from './ChatCompletionStream';\n\nexport interface ChatCompletionStreamEvents extends AbstractChatCompletionRunnerEvents {\n  content: (contentDelta: string, contentSnapshot: string) => void;\n  chunk: (chunk: ChatCompletionChunk, snapshot: ChatCompletionSnapshot) => void;\n}\n\nexport type ChatCompletionStreamingFunctionRunnerParams<FunctionsArgs extends BaseFunctionsArgs> = Omit<\n  ChatCompletionCreateParamsStreaming,\n  'functions'\n> & {\n  functions: RunnableFunctions<FunctionsArgs>;\n};\n\nexport type ChatCompletionStreamingToolRunnerParams<FunctionsArgs extends BaseFunctionsArgs> = Omit<\n  ChatCompletionCreateParamsStreaming,\n  'tools'\n> & {\n  tools: RunnableTools<FunctionsArgs>;\n};\n\nexport class ChatCompletionStreamingRunner\n  extends ChatCompletionStream\n  implements AsyncIterable<ChatCompletionChunk>\n{\n  static override fromReadableStream(stream: ReadableStream): ChatCompletionStreamingRunner {\n    const runner = new ChatCompletionStreamingRunner();\n    runner._run(() => runner._fromReadableStream(stream));\n    return runner;\n  }\n\n  /** @deprecated - please use `runTools` instead. */\n  static runFunctions<T extends (string | object)[]>(\n    completions: Completions,\n    params: ChatCompletionStreamingFunctionRunnerParams<T>,\n    options?: RunnerOptions,\n  ): ChatCompletionStreamingRunner {\n    const runner = new ChatCompletionStreamingRunner();\n    const opts = {\n      ...options,\n      headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runFunctions' },\n    };\n    runner._run(() => runner._runFunctions(completions, params, opts));\n    return runner;\n  }\n\n  static runTools<T extends (string | object)[]>(\n    completions: Completions,\n    params: ChatCompletionStreamingToolRunnerParams<T>,\n    options?: RunnerOptions,\n  ): ChatCompletionStreamingRunner {\n    const runner = new ChatCompletionStreamingRunner();\n    const opts = {\n      ...options,\n      headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runTools' },\n    };\n    runner._run(() => runner._runTools(completions, params, opts));\n    return runner;\n  }\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../core\";\nimport { APIResource } from \"../../../resource\";\nimport { ChatCompletionRunner, ChatCompletionFunctionRunnerParams } from \"../../../lib/ChatCompletionRunner\";\nexport { ChatCompletionRunner, ChatCompletionFunctionRunnerParams } from \"../../../lib/ChatCompletionRunner\";\nimport {\n  ChatCompletionStreamingRunner,\n  ChatCompletionStreamingFunctionRunnerParams,\n} from \"../../../lib/ChatCompletionStreamingRunner\";\nexport {\n  ChatCompletionStreamingRunner,\n  ChatCompletionStreamingFunctionRunnerParams,\n} from \"../../../lib/ChatCompletionStreamingRunner\";\nimport { BaseFunctionsArgs } from \"../../../lib/RunnableFunction\";\nexport {\n  RunnableFunction,\n  RunnableFunctions,\n  RunnableFunctionWithParse,\n  RunnableFunctionWithoutParse,\n  ParsingFunction,\n  ParsingToolFunction,\n} from \"../../../lib/RunnableFunction\";\nimport { ChatCompletionToolRunnerParams } from \"../../../lib/ChatCompletionRunner\";\nexport { ChatCompletionToolRunnerParams } from \"../../../lib/ChatCompletionRunner\";\nimport { ChatCompletionStreamingToolRunnerParams } from \"../../../lib/ChatCompletionStreamingRunner\";\nexport { ChatCompletionStreamingToolRunnerParams } from \"../../../lib/ChatCompletionStreamingRunner\";\nimport { ChatCompletionStream, type ChatCompletionStreamParams } from \"../../../lib/ChatCompletionStream\";\nexport { ChatCompletionStream, type ChatCompletionStreamParams } from \"../../../lib/ChatCompletionStream\";\n\nexport class Completions extends APIResource {\n  /**\n   * @deprecated - use `runTools` instead.\n   */\n  runFunctions<FunctionsArgs extends BaseFunctionsArgs>(\n    body: ChatCompletionFunctionRunnerParams<FunctionsArgs>,\n    options?: Core.RequestOptions,\n  ): ChatCompletionRunner;\n  runFunctions<FunctionsArgs extends BaseFunctionsArgs>(\n    body: ChatCompletionStreamingFunctionRunnerParams<FunctionsArgs>,\n    options?: Core.RequestOptions,\n  ): ChatCompletionStreamingRunner;\n  runFunctions<FunctionsArgs extends BaseFunctionsArgs>(\n    body:\n      | ChatCompletionFunctionRunnerParams<FunctionsArgs>\n      | ChatCompletionStreamingFunctionRunnerParams<FunctionsArgs>,\n    options?: Core.RequestOptions,\n  ): ChatCompletionRunner | ChatCompletionStreamingRunner {\n    if (body.stream) {\n      return ChatCompletionStreamingRunner.runFunctions(\n        this._client.chat.completions,\n        body as ChatCompletionStreamingFunctionRunnerParams<FunctionsArgs>,\n        options,\n      );\n    }\n    return ChatCompletionRunner.runFunctions(\n      this._client.chat.completions,\n      body as ChatCompletionFunctionRunnerParams<FunctionsArgs>,\n      options,\n    );\n  }\n\n  /**\n   * A convenience helper for using tool calls with the /chat/completions endpoint\n   * which automatically calls the JavaScript functions you provide and sends their\n   * results back to the /chat/completions endpoint, looping as long as the model\n   * requests function calls.\n   *\n   * For more details and examples, see\n   * [the docs](https://github.com/openai/openai-node#automated-function-calls)\n   */\n  runTools<FunctionsArgs extends BaseFunctionsArgs>(\n    body: ChatCompletionToolRunnerParams<FunctionsArgs>,\n    options?: Core.RequestOptions,\n  ): ChatCompletionRunner;\n  runTools<FunctionsArgs extends BaseFunctionsArgs>(\n    body: ChatCompletionStreamingToolRunnerParams<FunctionsArgs>,\n    options?: Core.RequestOptions,\n  ): ChatCompletionStreamingRunner;\n  runTools<FunctionsArgs extends BaseFunctionsArgs>(\n    body:\n      | ChatCompletionToolRunnerParams<FunctionsArgs>\n      | ChatCompletionStreamingToolRunnerParams<FunctionsArgs>,\n    options?: Core.RequestOptions,\n  ): ChatCompletionRunner | ChatCompletionStreamingRunner {\n    if (body.stream) {\n      return ChatCompletionStreamingRunner.runTools(\n        this._client.chat.completions,\n        body as ChatCompletionStreamingToolRunnerParams<FunctionsArgs>,\n        options,\n      );\n    }\n    return ChatCompletionRunner.runTools(\n      this._client.chat.completions,\n      body as ChatCompletionToolRunnerParams<FunctionsArgs>,\n      options,\n    );\n  }\n\n  /**\n   * Creates a chat completion stream\n   */\n  stream(body: ChatCompletionStreamParams, options?: Core.RequestOptions): ChatCompletionStream {\n    return ChatCompletionStream.createChatCompletion(this._client.chat.completions, body, options);\n  }\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport { APIResource } from \"../../../resource\";\nimport * as CompletionsAPI from \"./completions\";\n\nexport class Chat extends APIResource {\n  completions: CompletionsAPI.Completions = new CompletionsAPI.Completions(this._client);\n}\n\nexport namespace Chat {\n  export import Completions = CompletionsAPI.Completions;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../../core\";\nimport { APIResource } from \"../../../../resource\";\nimport { isRequestOptions } from \"../../../../core\";\nimport * as FilesAPI from \"./files\";\nimport { CursorPage, type CursorPageParams } from \"../../../../pagination\";\n\nexport class Files extends APIResource {\n  /**\n   * Retrieves a message file.\n   */\n  retrieve(\n    threadId: string,\n    messageId: string,\n    fileId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<MessageFile> {\n    return this._client.get(`/threads/${threadId}/messages/${messageId}/files/${fileId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Returns a list of message files.\n   */\n  list(\n    threadId: string,\n    messageId: string,\n    query?: FileListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<MessageFilesPage, MessageFile>;\n  list(\n    threadId: string,\n    messageId: string,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<MessageFilesPage, MessageFile>;\n  list(\n    threadId: string,\n    messageId: string,\n    query: FileListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<MessageFilesPage, MessageFile> {\n    if (isRequestOptions(query)) {\n      return this.list(threadId, messageId, {}, query);\n    }\n    return this._client.getAPIList(`/threads/${threadId}/messages/${messageId}/files`, MessageFilesPage, {\n      query,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n}\n\nexport class MessageFilesPage extends CursorPage<MessageFile> {}\n\n/**\n * A list of files attached to a `message`.\n */\nexport interface MessageFile {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the message file was created.\n   */\n  created_at: number;\n\n  /**\n   * The ID of the [message](https://platform.openai.com/docs/api-reference/messages)\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\n   * attached to.\n   */\n  message_id: string;\n\n  /**\n   * The object type, which is always `thread.message.file`.\n   */\n  object: 'thread.message.file';\n}\n\nexport interface FileListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport namespace Files {\n  export import MessageFile = FilesAPI.MessageFile;\n  export import MessageFilesPage = FilesAPI.MessageFilesPage;\n  export import FileListParams = FilesAPI.FileListParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../../core\";\nimport { APIResource } from \"../../../../resource\";\nimport { isRequestOptions } from \"../../../../core\";\nimport * as MessagesAPI from \"./messages\";\nimport * as FilesAPI from \"./files\";\nimport { CursorPage, type CursorPageParams } from \"../../../../pagination\";\n\nexport class Messages extends APIResource {\n  files: FilesAPI.Files = new FilesAPI.Files(this._client);\n\n  /**\n   * Create a message.\n   */\n  create(\n    threadId: string,\n    body: MessageCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<ThreadMessage> {\n    return this._client.post(`/threads/${threadId}/messages`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Retrieve a message.\n   */\n  retrieve(\n    threadId: string,\n    messageId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<ThreadMessage> {\n    return this._client.get(`/threads/${threadId}/messages/${messageId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Modifies a message.\n   */\n  update(\n    threadId: string,\n    messageId: string,\n    body: MessageUpdateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<ThreadMessage> {\n    return this._client.post(`/threads/${threadId}/messages/${messageId}`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Returns a list of messages for a given thread.\n   */\n  list(\n    threadId: string,\n    query?: MessageListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<ThreadMessagesPage, ThreadMessage>;\n  list(threadId: string, options?: Core.RequestOptions): Core.PagePromise<ThreadMessagesPage, ThreadMessage>;\n  list(\n    threadId: string,\n    query: MessageListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<ThreadMessagesPage, ThreadMessage> {\n    if (isRequestOptions(query)) {\n      return this.list(threadId, {}, query);\n    }\n    return this._client.getAPIList(`/threads/${threadId}/messages`, ThreadMessagesPage, {\n      query,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n}\n\nexport class ThreadMessagesPage extends CursorPage<ThreadMessage> {}\n\n/**\n * References an image [File](https://platform.openai.com/docs/api-reference/files)\n * in the content of a message.\n */\nexport interface MessageContentImageFile {\n  image_file: MessageContentImageFile.ImageFile;\n\n  /**\n   * Always `image_file`.\n   */\n  type: 'image_file';\n}\n\nexport namespace MessageContentImageFile {\n  export interface ImageFile {\n    /**\n     * The [File](https://platform.openai.com/docs/api-reference/files) ID of the image\n     * in the message content.\n     */\n    file_id: string;\n  }\n}\n\n/**\n * The text content that is part of a message.\n */\nexport interface MessageContentText {\n  text: MessageContentText.Text;\n\n  /**\n   * Always `text`.\n   */\n  type: 'text';\n}\n\nexport namespace MessageContentText {\n  export interface Text {\n    annotations: Array<Text.FileCitation | Text.FilePath>;\n\n    /**\n     * The data that makes up the text.\n     */\n    value: string;\n  }\n\n  export namespace Text {\n    /**\n     * A citation within the message that points to a specific quote from a specific\n     * File associated with the assistant or the message. Generated when the assistant\n     * uses the \"retrieval\" tool to search files.\n     */\n    export interface FileCitation {\n      end_index: number;\n\n      file_citation: FileCitation.FileCitation;\n\n      start_index: number;\n\n      /**\n       * The text in the message content that needs to be replaced.\n       */\n      text: string;\n\n      /**\n       * Always `file_citation`.\n       */\n      type: 'file_citation';\n    }\n\n    export namespace FileCitation {\n      export interface FileCitation {\n        /**\n         * The ID of the specific File the citation is from.\n         */\n        file_id: string;\n\n        /**\n         * The specific quote in the file.\n         */\n        quote: string;\n      }\n    }\n\n    /**\n     * A URL for the file that's generated when the assistant used the\n     * `code_interpreter` tool to generate a file.\n     */\n    export interface FilePath {\n      end_index: number;\n\n      file_path: FilePath.FilePath;\n\n      start_index: number;\n\n      /**\n       * The text in the message content that needs to be replaced.\n       */\n      text: string;\n\n      /**\n       * Always `file_path`.\n       */\n      type: 'file_path';\n    }\n\n    export namespace FilePath {\n      export interface FilePath {\n        /**\n         * The ID of the file that was generated.\n         */\n        file_id: string;\n      }\n    }\n  }\n}\n\n/**\n * Represents a message within a\n * [thread](https://platform.openai.com/docs/api-reference/threads).\n */\nexport interface ThreadMessage {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * If applicable, the ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) that\n   * authored this message.\n   */\n  assistant_id: string | null;\n\n  /**\n   * The content of the message in array of text and/or images.\n   */\n  content: Array<MessageContentImageFile | MessageContentText>;\n\n  /**\n   * The Unix timestamp (in seconds) for when the message was created.\n   */\n  created_at: number;\n\n  /**\n   * A list of [file](https://platform.openai.com/docs/api-reference/files) IDs that\n   * the assistant should use. Useful for tools like retrieval and code_interpreter\n   * that can access files. A maximum of 10 files can be attached to a message.\n   */\n  file_ids: Array<string>;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata: unknown | null;\n\n  /**\n   * The object type, which is always `thread.message`.\n   */\n  object: 'thread.message';\n\n  /**\n   * The entity that produced the message. One of `user` or `assistant`.\n   */\n  role: 'user' | 'assistant';\n\n  /**\n   * If applicable, the ID of the\n   * [run](https://platform.openai.com/docs/api-reference/runs) associated with the\n   * authoring of this message.\n   */\n  run_id: string | null;\n\n  /**\n   * The [thread](https://platform.openai.com/docs/api-reference/threads) ID that\n   * this message belongs to.\n   */\n  thread_id: string;\n}\n\nexport interface ThreadMessageDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'thread.message.deleted';\n}\n\nexport interface MessageCreateParams {\n  /**\n   * The content of the message.\n   */\n  content: string;\n\n  /**\n   * The role of the entity that is creating the message. Currently only `user` is\n   * supported.\n   */\n  role: 'user';\n\n  /**\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n   * the message should use. There can be a maximum of 10 files attached to a\n   * message. Useful for tools like `retrieval` and `code_interpreter` that can\n   * access and use files.\n   */\n  file_ids?: Array<string>;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n}\n\nexport interface MessageUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n}\n\nexport interface MessageListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport namespace Messages {\n  export import MessageContentImageFile = MessagesAPI.MessageContentImageFile;\n  export import MessageContentText = MessagesAPI.MessageContentText;\n  export import ThreadMessage = MessagesAPI.ThreadMessage;\n  export import ThreadMessageDeleted = MessagesAPI.ThreadMessageDeleted;\n  export import ThreadMessagesPage = MessagesAPI.ThreadMessagesPage;\n  export import MessageCreateParams = MessagesAPI.MessageCreateParams;\n  export import MessageUpdateParams = MessagesAPI.MessageUpdateParams;\n  export import MessageListParams = MessagesAPI.MessageListParams;\n  export import Files = FilesAPI.Files;\n  export import MessageFile = FilesAPI.MessageFile;\n  export import MessageFilesPage = FilesAPI.MessageFilesPage;\n  export import FileListParams = FilesAPI.FileListParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../../core\";\nimport { APIResource } from \"../../../../resource\";\nimport { isRequestOptions } from \"../../../../core\";\nimport * as StepsAPI from \"./steps\";\nimport { CursorPage, type CursorPageParams } from \"../../../../pagination\";\n\nexport class Steps extends APIResource {\n  /**\n   * Retrieves a run step.\n   */\n  retrieve(\n    threadId: string,\n    runId: string,\n    stepId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<RunStep> {\n    return this._client.get(`/threads/${threadId}/runs/${runId}/steps/${stepId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Returns a list of run steps belonging to a run.\n   */\n  list(\n    threadId: string,\n    runId: string,\n    query?: StepListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<RunStepsPage, RunStep>;\n  list(\n    threadId: string,\n    runId: string,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<RunStepsPage, RunStep>;\n  list(\n    threadId: string,\n    runId: string,\n    query: StepListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<RunStepsPage, RunStep> {\n    if (isRequestOptions(query)) {\n      return this.list(threadId, runId, {}, query);\n    }\n    return this._client.getAPIList(`/threads/${threadId}/runs/${runId}/steps`, RunStepsPage, {\n      query,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n}\n\nexport class RunStepsPage extends CursorPage<RunStep> {}\n\n/**\n * Details of the Code Interpreter tool call the run step was involved in.\n */\nexport interface CodeToolCall {\n  /**\n   * The ID of the tool call.\n   */\n  id: string;\n\n  /**\n   * The Code Interpreter tool call definition.\n   */\n  code_interpreter: CodeToolCall.CodeInterpreter;\n\n  /**\n   * The type of tool call. This is always going to be `code_interpreter` for this\n   * type of tool call.\n   */\n  type: 'code_interpreter';\n}\n\nexport namespace CodeToolCall {\n  /**\n   * The Code Interpreter tool call definition.\n   */\n  export interface CodeInterpreter {\n    /**\n     * The input to the Code Interpreter tool call.\n     */\n    input: string;\n\n    /**\n     * The outputs from the Code Interpreter tool call. Code Interpreter can output one\n     * or more items, including text (`logs`) or images (`image`). Each of these are\n     * represented by a different object type.\n     */\n    outputs: Array<CodeInterpreter.Logs | CodeInterpreter.Image>;\n  }\n\n  export namespace CodeInterpreter {\n    /**\n     * Text output from the Code Interpreter tool call as part of a run step.\n     */\n    export interface Logs {\n      /**\n       * The text output from the Code Interpreter tool call.\n       */\n      logs: string;\n\n      /**\n       * Always `logs`.\n       */\n      type: 'logs';\n    }\n\n    export interface Image {\n      image: Image.Image;\n\n      /**\n       * Always `image`.\n       */\n      type: 'image';\n    }\n\n    export namespace Image {\n      export interface Image {\n        /**\n         * The [file](https://platform.openai.com/docs/api-reference/files) ID of the\n         * image.\n         */\n        file_id: string;\n      }\n    }\n  }\n}\n\nexport interface FunctionToolCall {\n  /**\n   * The ID of the tool call object.\n   */\n  id: string;\n\n  /**\n   * The definition of the function that was called.\n   */\n  function: FunctionToolCall.Function;\n\n  /**\n   * The type of tool call. This is always going to be `function` for this type of\n   * tool call.\n   */\n  type: 'function';\n}\n\nexport namespace FunctionToolCall {\n  /**\n   * The definition of the function that was called.\n   */\n  export interface Function {\n    /**\n     * The arguments passed to the function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function.\n     */\n    name: string;\n\n    /**\n     * The output of the function. This will be `null` if the outputs have not been\n     * [submitted](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)\n     * yet.\n     */\n    output: string | null;\n  }\n}\n\n/**\n * Details of the message creation by the run step.\n */\nexport interface MessageCreationStepDetails {\n  message_creation: MessageCreationStepDetails.MessageCreation;\n\n  /**\n   * Always `message_creation`.\n   */\n  type: 'message_creation';\n}\n\nexport namespace MessageCreationStepDetails {\n  export interface MessageCreation {\n    /**\n     * The ID of the message that was created by this run step.\n     */\n    message_id: string;\n  }\n}\n\nexport interface RetrievalToolCall {\n  /**\n   * The ID of the tool call object.\n   */\n  id: string;\n\n  /**\n   * For now, this is always going to be an empty object.\n   */\n  retrieval: unknown;\n\n  /**\n   * The type of tool call. This is always going to be `retrieval` for this type of\n   * tool call.\n   */\n  type: 'retrieval';\n}\n\n/**\n * Represents a step in execution of a run.\n */\nexport interface RunStep {\n  /**\n   * The identifier of the run step, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants)\n   * associated with the run step.\n   */\n  assistant_id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step was cancelled.\n   */\n  cancelled_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step completed.\n   */\n  completed_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step was created.\n   */\n  created_at: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step expired. A step is\n   * considered expired if the parent run is expired.\n   */\n  expired_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run step failed.\n   */\n  failed_at: number | null;\n\n  /**\n   * The last error associated with this run step. Will be `null` if there are no\n   * errors.\n   */\n  last_error: RunStep.LastError | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata: unknown | null;\n\n  /**\n   * The object type, which is always `thread.run.step`.\n   */\n  object: 'thread.run.step';\n\n  /**\n   * The ID of the [run](https://platform.openai.com/docs/api-reference/runs) that\n   * this run step is a part of.\n   */\n  run_id: string;\n\n  /**\n   * The status of the run step, which can be either `in_progress`, `cancelled`,\n   * `failed`, `completed`, or `expired`.\n   */\n  status: 'in_progress' | 'cancelled' | 'failed' | 'completed' | 'expired';\n\n  /**\n   * The details of the run step.\n   */\n  step_details: MessageCreationStepDetails | ToolCallsStepDetails;\n\n  /**\n   * The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)\n   * that was run.\n   */\n  thread_id: string;\n\n  /**\n   * The type of run step, which can be either `message_creation` or `tool_calls`.\n   */\n  type: 'message_creation' | 'tool_calls';\n\n  /**\n   * Usage statistics related to the run step. This value will be `null` while the\n   * run step's status is `in_progress`.\n   */\n  usage: RunStep.Usage | null;\n}\n\nexport namespace RunStep {\n  /**\n   * The last error associated with this run step. Will be `null` if there are no\n   * errors.\n   */\n  export interface LastError {\n    /**\n     * One of `server_error` or `rate_limit_exceeded`.\n     */\n    code: 'server_error' | 'rate_limit_exceeded';\n\n    /**\n     * A human-readable description of the error.\n     */\n    message: string;\n  }\n\n  /**\n   * Usage statistics related to the run step. This value will be `null` while the\n   * run step's status is `in_progress`.\n   */\n  export interface Usage {\n    /**\n     * Number of completion tokens used over the course of the run step.\n     */\n    completion_tokens: number;\n\n    /**\n     * Number of prompt tokens used over the course of the run step.\n     */\n    prompt_tokens: number;\n\n    /**\n     * Total number of tokens used (prompt + completion).\n     */\n    total_tokens: number;\n  }\n}\n\n/**\n * Details of the tool call.\n */\nexport interface ToolCallsStepDetails {\n  /**\n   * An array of tool calls the run step was involved in. These can be associated\n   * with one of three types of tools: `code_interpreter`, `retrieval`, or\n   * `function`.\n   */\n  tool_calls: Array<CodeToolCall | RetrievalToolCall | FunctionToolCall>;\n\n  /**\n   * Always `tool_calls`.\n   */\n  type: 'tool_calls';\n}\n\nexport interface StepListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport namespace Steps {\n  export import CodeToolCall = StepsAPI.CodeToolCall;\n  export import FunctionToolCall = StepsAPI.FunctionToolCall;\n  export import MessageCreationStepDetails = StepsAPI.MessageCreationStepDetails;\n  export import RetrievalToolCall = StepsAPI.RetrievalToolCall;\n  export import RunStep = StepsAPI.RunStep;\n  export import ToolCallsStepDetails = StepsAPI.ToolCallsStepDetails;\n  export import RunStepsPage = StepsAPI.RunStepsPage;\n  export import StepListParams = StepsAPI.StepListParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../../core\";\nimport { APIResource } from \"../../../../resource\";\nimport { isRequestOptions } from \"../../../../core\";\nimport * as RunsAPI from \"./runs\";\nimport * as Shared from \"../../../shared\";\nimport * as StepsAPI from \"./steps\";\nimport { CursorPage, type CursorPageParams } from \"../../../../pagination\";\n\nexport class Runs extends APIResource {\n  steps: StepsAPI.Steps = new StepsAPI.Steps(this._client);\n\n  /**\n   * Create a run.\n   */\n  create(threadId: string, body: RunCreateParams, options?: Core.RequestOptions): Core.APIPromise<Run> {\n    return this._client.post(`/threads/${threadId}/runs`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Retrieves a run.\n   */\n  retrieve(threadId: string, runId: string, options?: Core.RequestOptions): Core.APIPromise<Run> {\n    return this._client.get(`/threads/${threadId}/runs/${runId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Modifies a run.\n   */\n  update(\n    threadId: string,\n    runId: string,\n    body: RunUpdateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<Run> {\n    return this._client.post(`/threads/${threadId}/runs/${runId}`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Returns a list of runs belonging to a thread.\n   */\n  list(\n    threadId: string,\n    query?: RunListParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<RunsPage, Run>;\n  list(threadId: string, options?: Core.RequestOptions): Core.PagePromise<RunsPage, Run>;\n  list(\n    threadId: string,\n    query: RunListParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<RunsPage, Run> {\n    if (isRequestOptions(query)) {\n      return this.list(threadId, {}, query);\n    }\n    return this._client.getAPIList(`/threads/${threadId}/runs`, RunsPage, {\n      query,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Cancels a run that is `in_progress`.\n   */\n  cancel(threadId: string, runId: string, options?: Core.RequestOptions): Core.APIPromise<Run> {\n    return this._client.post(`/threads/${threadId}/runs/${runId}/cancel`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * When a run has the `status: \"requires_action\"` and `required_action.type` is\n   * `submit_tool_outputs`, this endpoint can be used to submit the outputs from the\n   * tool calls once they're all completed. All outputs must be submitted in a single\n   * request.\n   */\n  submitToolOutputs(\n    threadId: string,\n    runId: string,\n    body: RunSubmitToolOutputsParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<Run> {\n    return this._client.post(`/threads/${threadId}/runs/${runId}/submit_tool_outputs`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n}\n\nexport class RunsPage extends CursorPage<Run> {}\n\n/**\n * Tool call objects\n */\nexport interface RequiredActionFunctionToolCall {\n  /**\n   * The ID of the tool call. This ID must be referenced when you submit the tool\n   * outputs in using the\n   * [Submit tool outputs to run](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)\n   * endpoint.\n   */\n  id: string;\n\n  /**\n   * The function definition.\n   */\n  function: RequiredActionFunctionToolCall.Function;\n\n  /**\n   * The type of tool call the output is required for. For now, this is always\n   * `function`.\n   */\n  type: 'function';\n}\n\nexport namespace RequiredActionFunctionToolCall {\n  /**\n   * The function definition.\n   */\n  export interface Function {\n    /**\n     * The arguments that the model expects you to pass to the function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function.\n     */\n    name: string;\n  }\n}\n\n/**\n * Represents an execution run on a\n * [thread](https://platform.openai.com/docs/api-reference/threads).\n */\nexport interface Run {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * execution of this run.\n   */\n  assistant_id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was cancelled.\n   */\n  cancelled_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was completed.\n   */\n  completed_at: number | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was created.\n   */\n  created_at: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run will expire.\n   */\n  expires_at: number;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run failed.\n   */\n  failed_at: number | null;\n\n  /**\n   * The list of [File](https://platform.openai.com/docs/api-reference/files) IDs the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * this run.\n   */\n  file_ids: Array<string>;\n\n  /**\n   * The instructions that the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * this run.\n   */\n  instructions: string;\n\n  /**\n   * The last error associated with this run. Will be `null` if there are no errors.\n   */\n  last_error: Run.LastError | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata: unknown | null;\n\n  /**\n   * The model that the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * this run.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `thread.run`.\n   */\n  object: 'thread.run';\n\n  /**\n   * Details on the action required to continue the run. Will be `null` if no action\n   * is required.\n   */\n  required_action: Run.RequiredAction | null;\n\n  /**\n   * The Unix timestamp (in seconds) for when the run was started.\n   */\n  started_at: number | null;\n\n  /**\n   * The status of the run, which can be either `queued`, `in_progress`,\n   * `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`, or\n   * `expired`.\n   */\n  status:\n    | 'queued'\n    | 'in_progress'\n    | 'requires_action'\n    | 'cancelling'\n    | 'cancelled'\n    | 'failed'\n    | 'completed'\n    | 'expired';\n\n  /**\n   * The ID of the [thread](https://platform.openai.com/docs/api-reference/threads)\n   * that was executed on as a part of this run.\n   */\n  thread_id: string;\n\n  /**\n   * The list of tools that the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) used for\n   * this run.\n   */\n  tools: Array<Run.AssistantToolsCode | Run.AssistantToolsRetrieval | Run.AssistantToolsFunction>;\n\n  /**\n   * Usage statistics related to the run. This value will be `null` if the run is not\n   * in a terminal state (i.e. `in_progress`, `queued`, etc.).\n   */\n  usage: Run.Usage | null;\n}\n\nexport namespace Run {\n  /**\n   * The last error associated with this run. Will be `null` if there are no errors.\n   */\n  export interface LastError {\n    /**\n     * One of `server_error` or `rate_limit_exceeded`.\n     */\n    code: 'server_error' | 'rate_limit_exceeded';\n\n    /**\n     * A human-readable description of the error.\n     */\n    message: string;\n  }\n\n  /**\n   * Details on the action required to continue the run. Will be `null` if no action\n   * is required.\n   */\n  export interface RequiredAction {\n    /**\n     * Details on the tool outputs needed for this run to continue.\n     */\n    submit_tool_outputs: RequiredAction.SubmitToolOutputs;\n\n    /**\n     * For now, this is always `submit_tool_outputs`.\n     */\n    type: 'submit_tool_outputs';\n  }\n\n  export namespace RequiredAction {\n    /**\n     * Details on the tool outputs needed for this run to continue.\n     */\n    export interface SubmitToolOutputs {\n      /**\n       * A list of the relevant tool calls.\n       */\n      tool_calls: Array<RunsAPI.RequiredActionFunctionToolCall>;\n    }\n  }\n\n  export interface AssistantToolsCode {\n    /**\n     * The type of tool being defined: `code_interpreter`\n     */\n    type: 'code_interpreter';\n  }\n\n  export interface AssistantToolsRetrieval {\n    /**\n     * The type of tool being defined: `retrieval`\n     */\n    type: 'retrieval';\n  }\n\n  export interface AssistantToolsFunction {\n    function: Shared.FunctionDefinition;\n\n    /**\n     * The type of tool being defined: `function`\n     */\n    type: 'function';\n  }\n\n  /**\n   * Usage statistics related to the run. This value will be `null` if the run is not\n   * in a terminal state (i.e. `in_progress`, `queued`, etc.).\n   */\n  export interface Usage {\n    /**\n     * Number of completion tokens used over the course of the run.\n     */\n    completion_tokens: number;\n\n    /**\n     * Number of prompt tokens used over the course of the run.\n     */\n    prompt_tokens: number;\n\n    /**\n     * Total number of tokens used (prompt + completion).\n     */\n    total_tokens: number;\n  }\n}\n\nexport interface RunCreateParams {\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to\n   * execute this run.\n   */\n  assistant_id: string;\n\n  /**\n   * Appends additional instructions at the end of the instructions for the run. This\n   * is useful for modifying the behavior on a per-run basis without overriding other\n   * instructions.\n   */\n  additional_instructions?: string | null;\n\n  /**\n   * Overrides the\n   * [instructions](https://platform.openai.com/docs/api-reference/assistants/createAssistant)\n   * of the assistant. This is useful for modifying the behavior on a per-run basis.\n   */\n  instructions?: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n\n  /**\n   * The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to\n   * be used to execute this run. If a value is provided here, it will override the\n   * model associated with the assistant. If not, the model associated with the\n   * assistant will be used.\n   */\n  model?: string | null;\n\n  /**\n   * Override the tools the assistant can use for this run. This is useful for\n   * modifying the behavior on a per-run basis.\n   */\n  tools?: Array<\n    | RunCreateParams.AssistantToolsCode\n    | RunCreateParams.AssistantToolsRetrieval\n    | RunCreateParams.AssistantToolsFunction\n  > | null;\n}\n\nexport namespace RunCreateParams {\n  export interface AssistantToolsCode {\n    /**\n     * The type of tool being defined: `code_interpreter`\n     */\n    type: 'code_interpreter';\n  }\n\n  export interface AssistantToolsRetrieval {\n    /**\n     * The type of tool being defined: `retrieval`\n     */\n    type: 'retrieval';\n  }\n\n  export interface AssistantToolsFunction {\n    function: Shared.FunctionDefinition;\n\n    /**\n     * The type of tool being defined: `function`\n     */\n    type: 'function';\n  }\n}\n\nexport interface RunUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n}\n\nexport interface RunListParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport interface RunSubmitToolOutputsParams {\n  /**\n   * A list of tools for which the outputs are being submitted.\n   */\n  tool_outputs: Array<RunSubmitToolOutputsParams.ToolOutput>;\n}\n\nexport namespace RunSubmitToolOutputsParams {\n  export interface ToolOutput {\n    /**\n     * The output of the tool call to be submitted to continue the run.\n     */\n    output?: string;\n\n    /**\n     * The ID of the tool call in the `required_action` object within the run object\n     * the output is being submitted for.\n     */\n    tool_call_id?: string;\n  }\n}\n\nexport namespace Runs {\n  export import RequiredActionFunctionToolCall = RunsAPI.RequiredActionFunctionToolCall;\n  export import Run = RunsAPI.Run;\n  export import RunsPage = RunsAPI.RunsPage;\n  export import RunCreateParams = RunsAPI.RunCreateParams;\n  export import RunUpdateParams = RunsAPI.RunUpdateParams;\n  export import RunListParams = RunsAPI.RunListParams;\n  export import RunSubmitToolOutputsParams = RunsAPI.RunSubmitToolOutputsParams;\n  export import Steps = StepsAPI.Steps;\n  export import CodeToolCall = StepsAPI.CodeToolCall;\n  export import FunctionToolCall = StepsAPI.FunctionToolCall;\n  export import MessageCreationStepDetails = StepsAPI.MessageCreationStepDetails;\n  export import RetrievalToolCall = StepsAPI.RetrievalToolCall;\n  export import RunStep = StepsAPI.RunStep;\n  export import ToolCallsStepDetails = StepsAPI.ToolCallsStepDetails;\n  export import RunStepsPage = StepsAPI.RunStepsPage;\n  export import StepListParams = StepsAPI.StepListParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../../../core\";\nimport { APIResource } from \"../../../resource\";\nimport { isRequestOptions } from \"../../../core\";\nimport * as ThreadsAPI from \"./threads\";\nimport * as Shared from \"../../shared\";\nimport * as MessagesAPI from \"./messages/messages\";\nimport * as RunsAPI from \"./runs/runs\";\n\nexport class Threads extends APIResource {\n  runs: RunsAPI.Runs = new RunsAPI.Runs(this._client);\n  messages: MessagesAPI.Messages = new MessagesAPI.Messages(this._client);\n\n  /**\n   * Create a thread.\n   */\n  create(body?: ThreadCreateParams, options?: Core.RequestOptions): Core.APIPromise<Thread>;\n  create(options?: Core.RequestOptions): Core.APIPromise<Thread>;\n  create(\n    body: ThreadCreateParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<Thread> {\n    if (isRequestOptions(body)) {\n      return this.create({}, body);\n    }\n    return this._client.post('/threads', {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Retrieves a thread.\n   */\n  retrieve(threadId: string, options?: Core.RequestOptions): Core.APIPromise<Thread> {\n    return this._client.get(`/threads/${threadId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Modifies a thread.\n   */\n  update(threadId: string, body: ThreadUpdateParams, options?: Core.RequestOptions): Core.APIPromise<Thread> {\n    return this._client.post(`/threads/${threadId}`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Delete a thread.\n   */\n  del(threadId: string, options?: Core.RequestOptions): Core.APIPromise<ThreadDeleted> {\n    return this._client.delete(`/threads/${threadId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n\n  /**\n   * Create a thread and run it in one request.\n   */\n  createAndRun(body: ThreadCreateAndRunParams, options?: Core.RequestOptions): Core.APIPromise<RunsAPI.Run> {\n    return this._client.post('/threads/runs', {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v1', ...options?.headers },\n    });\n  }\n}\n\n/**\n * Represents a thread that contains\n * [messages](https://platform.openai.com/docs/api-reference/messages).\n */\nexport interface Thread {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the thread was created.\n   */\n  created_at: number;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata: unknown | null;\n\n  /**\n   * The object type, which is always `thread`.\n   */\n  object: 'thread';\n}\n\nexport interface ThreadDeleted {\n  id: string;\n\n  deleted: boolean;\n\n  object: 'thread.deleted';\n}\n\nexport interface ThreadCreateParams {\n  /**\n   * A list of [messages](https://platform.openai.com/docs/api-reference/messages) to\n   * start the thread with.\n   */\n  messages?: Array<ThreadCreateParams.Message>;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n}\n\nexport namespace ThreadCreateParams {\n  export interface Message {\n    /**\n     * The content of the message.\n     */\n    content: string;\n\n    /**\n     * The role of the entity that is creating the message. Currently only `user` is\n     * supported.\n     */\n    role: 'user';\n\n    /**\n     * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n     * the message should use. There can be a maximum of 10 files attached to a\n     * message. Useful for tools like `retrieval` and `code_interpreter` that can\n     * access and use files.\n     */\n    file_ids?: Array<string>;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format. Keys\n     * can be a maximum of 64 characters long and values can be a maxium of 512\n     * characters long.\n     */\n    metadata?: unknown | null;\n  }\n}\n\nexport interface ThreadUpdateParams {\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n}\n\nexport interface ThreadCreateAndRunParams {\n  /**\n   * The ID of the\n   * [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to\n   * execute this run.\n   */\n  assistant_id: string;\n\n  /**\n   * Override the default system message of the assistant. This is useful for\n   * modifying the behavior on a per-run basis.\n   */\n  instructions?: string | null;\n\n  /**\n   * Set of 16 key-value pairs that can be attached to an object. This can be useful\n   * for storing additional information about the object in a structured format. Keys\n   * can be a maximum of 64 characters long and values can be a maxium of 512\n   * characters long.\n   */\n  metadata?: unknown | null;\n\n  /**\n   * The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to\n   * be used to execute this run. If a value is provided here, it will override the\n   * model associated with the assistant. If not, the model associated with the\n   * assistant will be used.\n   */\n  model?: string | null;\n\n  /**\n   * If no thread is provided, an empty thread will be created.\n   */\n  thread?: ThreadCreateAndRunParams.Thread;\n\n  /**\n   * Override the tools the assistant can use for this run. This is useful for\n   * modifying the behavior on a per-run basis.\n   */\n  tools?: Array<\n    | ThreadCreateAndRunParams.AssistantToolsCode\n    | ThreadCreateAndRunParams.AssistantToolsRetrieval\n    | ThreadCreateAndRunParams.AssistantToolsFunction\n  > | null;\n}\n\nexport namespace ThreadCreateAndRunParams {\n  /**\n   * If no thread is provided, an empty thread will be created.\n   */\n  export interface Thread {\n    /**\n     * A list of [messages](https://platform.openai.com/docs/api-reference/messages) to\n     * start the thread with.\n     */\n    messages?: Array<Thread.Message>;\n\n    /**\n     * Set of 16 key-value pairs that can be attached to an object. This can be useful\n     * for storing additional information about the object in a structured format. Keys\n     * can be a maximum of 64 characters long and values can be a maxium of 512\n     * characters long.\n     */\n    metadata?: unknown | null;\n  }\n\n  export namespace Thread {\n    export interface Message {\n      /**\n       * The content of the message.\n       */\n      content: string;\n\n      /**\n       * The role of the entity that is creating the message. Currently only `user` is\n       * supported.\n       */\n      role: 'user';\n\n      /**\n       * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n       * the message should use. There can be a maximum of 10 files attached to a\n       * message. Useful for tools like `retrieval` and `code_interpreter` that can\n       * access and use files.\n       */\n      file_ids?: Array<string>;\n\n      /**\n       * Set of 16 key-value pairs that can be attached to an object. This can be useful\n       * for storing additional information about the object in a structured format. Keys\n       * can be a maximum of 64 characters long and values can be a maxium of 512\n       * characters long.\n       */\n      metadata?: unknown | null;\n    }\n  }\n\n  export interface AssistantToolsCode {\n    /**\n     * The type of tool being defined: `code_interpreter`\n     */\n    type: 'code_interpreter';\n  }\n\n  export interface AssistantToolsRetrieval {\n    /**\n     * The type of tool being defined: `retrieval`\n     */\n    type: 'retrieval';\n  }\n\n  export interface AssistantToolsFunction {\n    function: Shared.FunctionDefinition;\n\n    /**\n     * The type of tool being defined: `function`\n     */\n    type: 'function';\n  }\n}\n\nexport namespace Threads {\n  export import Thread = ThreadsAPI.Thread;\n  export import ThreadDeleted = ThreadsAPI.ThreadDeleted;\n  export import ThreadCreateParams = ThreadsAPI.ThreadCreateParams;\n  export import ThreadUpdateParams = ThreadsAPI.ThreadUpdateParams;\n  export import ThreadCreateAndRunParams = ThreadsAPI.ThreadCreateAndRunParams;\n  export import Runs = RunsAPI.Runs;\n  export import RequiredActionFunctionToolCall = RunsAPI.RequiredActionFunctionToolCall;\n  export import Run = RunsAPI.Run;\n  export import RunsPage = RunsAPI.RunsPage;\n  export import RunCreateParams = RunsAPI.RunCreateParams;\n  export import RunUpdateParams = RunsAPI.RunUpdateParams;\n  export import RunListParams = RunsAPI.RunListParams;\n  export import RunSubmitToolOutputsParams = RunsAPI.RunSubmitToolOutputsParams;\n  export import Messages = MessagesAPI.Messages;\n  export import MessageContentImageFile = MessagesAPI.MessageContentImageFile;\n  export import MessageContentText = MessagesAPI.MessageContentText;\n  export import ThreadMessage = MessagesAPI.ThreadMessage;\n  export import ThreadMessageDeleted = MessagesAPI.ThreadMessageDeleted;\n  export import ThreadMessagesPage = MessagesAPI.ThreadMessagesPage;\n  export import MessageCreateParams = MessagesAPI.MessageCreateParams;\n  export import MessageUpdateParams = MessagesAPI.MessageUpdateParams;\n  export import MessageListParams = MessagesAPI.MessageListParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport { APIResource } from \"../../resource\";\nimport * as AssistantsAPI from \"./assistants/assistants\";\nimport * as ChatAPI from \"./chat/chat\";\nimport * as ThreadsAPI from \"./threads/threads\";\n\nexport class Beta extends APIResource {\n  chat: ChatAPI.Chat = new ChatAPI.Chat(this._client);\n  assistants: AssistantsAPI.Assistants = new AssistantsAPI.Assistants(this._client);\n  threads: ThreadsAPI.Threads = new ThreadsAPI.Threads(this._client);\n}\n\nexport namespace Beta {\n  export import Chat = ChatAPI.Chat;\n  export import Assistants = AssistantsAPI.Assistants;\n  export import Assistant = AssistantsAPI.Assistant;\n  export import AssistantDeleted = AssistantsAPI.AssistantDeleted;\n  export import AssistantsPage = AssistantsAPI.AssistantsPage;\n  export import AssistantCreateParams = AssistantsAPI.AssistantCreateParams;\n  export import AssistantUpdateParams = AssistantsAPI.AssistantUpdateParams;\n  export import AssistantListParams = AssistantsAPI.AssistantListParams;\n  export import Threads = ThreadsAPI.Threads;\n  export import Thread = ThreadsAPI.Thread;\n  export import ThreadDeleted = ThreadsAPI.ThreadDeleted;\n  export import ThreadCreateParams = ThreadsAPI.ThreadCreateParams;\n  export import ThreadUpdateParams = ThreadsAPI.ThreadUpdateParams;\n  export import ThreadCreateAndRunParams = ThreadsAPI.ThreadCreateAndRunParams;\n}\n","// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from './core';\nimport * as Pagination from './pagination';\nimport * as Errors from './error';\nimport { type Agent } from './_shims/index';\nimport * as Uploads from './uploads';\nimport * as API from \"./resources/index\";\n\nexport interface ClientOptions {\n  /**\n   * Defaults to process.env['OPENAI_API_KEY'].\n   */\n  apiKey?: string | undefined;\n\n  /**\n   * Defaults to process.env['OPENAI_ORG_ID'].\n   */\n  organization?: string | null | undefined;\n\n  /**\n   * Override the default base URL for the API, e.g., \"https://api.example.com/v2/\"\n   *\n   * Defaults to process.env['OPENAI_BASE_URL'].\n   */\n  baseURL?: string | null | undefined;\n\n  /**\n   * The maximum amount of time (in milliseconds) that the client should wait for a response\n   * from the server before timing out a single request.\n   *\n   * Note that request timeouts are retried by default, so in a worst-case scenario you may wait\n   * much longer than this timeout before the promise succeeds or fails.\n   */\n  timeout?: number;\n\n  /**\n   * An HTTP agent used to manage HTTP(S) connections.\n   *\n   * If not provided, an agent will be constructed by default in the Node.js environment,\n   * otherwise no agent is used.\n   */\n  httpAgent?: Agent;\n\n  /**\n   * Specify a custom `fetch` function implementation.\n   *\n   * If not provided, we use `node-fetch` on Node.js and otherwise expect that `fetch` is\n   * defined globally.\n   */\n  fetch?: Core.Fetch | undefined;\n\n  /**\n   * The maximum number of times that the client will retry a request in case of a\n   * temporary failure, like a network error or a 5XX error from the server.\n   *\n   * @default 2\n   */\n  maxRetries?: number;\n\n  /**\n   * Default headers to include with every request to the API.\n   *\n   * These can be removed in individual requests by explicitly setting the\n   * header to `undefined` or `null` in request options.\n   */\n  defaultHeaders?: Core.Headers;\n\n  /**\n   * Default query parameters to include with every request to the API.\n   *\n   * These can be removed in individual requests by explicitly setting the\n   * param to `undefined` in request options.\n   */\n  defaultQuery?: Core.DefaultQuery;\n\n  /**\n   * By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n   * Only set this option to `true` if you understand the risks and have appropriate mitigations in place.\n   */\n  dangerouslyAllowBrowser?: boolean;\n}\n\n/** API Client for interfacing with the OpenAI API. */\nexport class OpenAI extends Core.APIClient {\n  apiKey: string;\n  organization: string | null;\n\n  private _options: ClientOptions;\n\n  /**\n   * API Client for interfacing with the OpenAI API.\n   *\n   * @param {string | undefined} [opts.apiKey=process.env['OPENAI_API_KEY'] ?? undefined]\n   * @param {string | null | undefined} [opts.organization=process.env['OPENAI_ORG_ID'] ?? null]\n   * @param {string} [opts.baseURL=process.env['OPENAI_BASE_URL'] ?? https://api.openai.com/v1] - Override the default base URL for the API.\n   * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.\n   * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.\n   * @param {Core.Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.\n   * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.\n   * @param {Core.Headers} opts.defaultHeaders - Default headers to include with every request to the API.\n   * @param {Core.DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.\n   * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n   */\n  constructor({\n    baseURL = Core.readEnv('OPENAI_BASE_URL'),\n    apiKey = Core.readEnv('OPENAI_API_KEY'),\n    organization = Core.readEnv('OPENAI_ORG_ID') ?? null,\n    ...opts\n  }: ClientOptions = {}) {\n    if (apiKey === undefined) {\n      throw new Errors.OpenAIError(\n        \"The OPENAI_API_KEY environment variable is missing or empty; either provide it, or instantiate the OpenAI client with an apiKey option, like new OpenAI({ apiKey: 'My API Key' }).\",\n      );\n    }\n\n    const options: ClientOptions = {\n      apiKey,\n      organization,\n      ...opts,\n      baseURL: baseURL || `https://api.openai.com/v1`,\n    };\n\n    if (!options.dangerouslyAllowBrowser && Core.isRunningInBrowser()) {\n      throw new Errors.OpenAIError(\n        \"It looks like you're running in a browser-like environment.\\n\\nThis is disabled by default, as it risks exposing your secret API credentials to attackers.\\nIf you understand the risks and have appropriate mitigations in place,\\nyou can set the `dangerouslyAllowBrowser` option to `true`, e.g.,\\n\\nnew OpenAI({ apiKey, dangerouslyAllowBrowser: true });\\n\\nhttps://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\\n\",\n      );\n    }\n\n    super({\n      baseURL: options.baseURL!,\n      timeout: options.timeout ?? 600000 /* 10 minutes */,\n      httpAgent: options.httpAgent,\n      maxRetries: options.maxRetries,\n      fetch: options.fetch,\n    });\n    this._options = options;\n\n    this.apiKey = apiKey;\n    this.organization = organization;\n  }\n\n  completions: API.Completions = new API.Completions(this);\n  chat: API.Chat = new API.Chat(this);\n  embeddings: API.Embeddings = new API.Embeddings(this);\n  files: API.Files = new API.Files(this);\n  images: API.Images = new API.Images(this);\n  audio: API.Audio = new API.Audio(this);\n  moderations: API.Moderations = new API.Moderations(this);\n  models: API.Models = new API.Models(this);\n  fineTuning: API.FineTuning = new API.FineTuning(this);\n  beta: API.Beta = new API.Beta(this);\n\n  protected override defaultQuery(): Core.DefaultQuery | undefined {\n    return this._options.defaultQuery;\n  }\n\n  protected override defaultHeaders(opts: Core.FinalRequestOptions): Core.Headers {\n    return {\n      ...super.defaultHeaders(opts),\n      'OpenAI-Organization': this.organization,\n      ...this._options.defaultHeaders,\n    };\n  }\n\n  protected override authHeaders(opts: Core.FinalRequestOptions): Core.Headers {\n    return { Authorization: `Bearer ${this.apiKey}` };\n  }\n\n  static OpenAI = this;\n\n  static OpenAIError = Errors.OpenAIError;\n  static APIError = Errors.APIError;\n  static APIConnectionError = Errors.APIConnectionError;\n  static APIConnectionTimeoutError = Errors.APIConnectionTimeoutError;\n  static APIUserAbortError = Errors.APIUserAbortError;\n  static NotFoundError = Errors.NotFoundError;\n  static ConflictError = Errors.ConflictError;\n  static RateLimitError = Errors.RateLimitError;\n  static BadRequestError = Errors.BadRequestError;\n  static AuthenticationError = Errors.AuthenticationError;\n  static InternalServerError = Errors.InternalServerError;\n  static PermissionDeniedError = Errors.PermissionDeniedError;\n  static UnprocessableEntityError = Errors.UnprocessableEntityError;\n}\n\nexport const {\n  OpenAIError,\n  APIError,\n  APIConnectionError,\n  APIConnectionTimeoutError,\n  APIUserAbortError,\n  NotFoundError,\n  ConflictError,\n  RateLimitError,\n  BadRequestError,\n  AuthenticationError,\n  InternalServerError,\n  PermissionDeniedError,\n  UnprocessableEntityError,\n} = Errors;\n\nexport import toFile = Uploads.toFile;\nexport import fileFromPath = Uploads.fileFromPath;\n\nexport namespace OpenAI {\n  // Helper functions\n  export import toFile = Uploads.toFile;\n  export import fileFromPath = Uploads.fileFromPath;\n\n  export import RequestOptions = Core.RequestOptions;\n\n  export import Page = Pagination.Page;\n  export import PageResponse = Pagination.PageResponse;\n\n  export import CursorPage = Pagination.CursorPage;\n  export import CursorPageParams = Pagination.CursorPageParams;\n  export import CursorPageResponse = Pagination.CursorPageResponse;\n\n  export import Completions = API.Completions;\n  export import Completion = API.Completion;\n  export import CompletionChoice = API.CompletionChoice;\n  export import CompletionUsage = API.CompletionUsage;\n  export import CompletionCreateParams = API.CompletionCreateParams;\n  export import CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n\n  export import Chat = API.Chat;\n  export import ChatCompletion = API.ChatCompletion;\n  export import ChatCompletionAssistantMessageParam = API.ChatCompletionAssistantMessageParam;\n  export import ChatCompletionChunk = API.ChatCompletionChunk;\n  export import ChatCompletionContentPart = API.ChatCompletionContentPart;\n  export import ChatCompletionContentPartImage = API.ChatCompletionContentPartImage;\n  export import ChatCompletionContentPartText = API.ChatCompletionContentPartText;\n  export import ChatCompletionFunctionCallOption = API.ChatCompletionFunctionCallOption;\n  export import ChatCompletionFunctionMessageParam = API.ChatCompletionFunctionMessageParam;\n  export import ChatCompletionMessage = API.ChatCompletionMessage;\n  export import ChatCompletionMessageParam = API.ChatCompletionMessageParam;\n  export import ChatCompletionMessageToolCall = API.ChatCompletionMessageToolCall;\n  export import ChatCompletionNamedToolChoice = API.ChatCompletionNamedToolChoice;\n  export import ChatCompletionRole = API.ChatCompletionRole;\n  export import ChatCompletionSystemMessageParam = API.ChatCompletionSystemMessageParam;\n  export import ChatCompletionTokenLogprob = API.ChatCompletionTokenLogprob;\n  export import ChatCompletionTool = API.ChatCompletionTool;\n  export import ChatCompletionToolChoiceOption = API.ChatCompletionToolChoiceOption;\n  export import ChatCompletionToolMessageParam = API.ChatCompletionToolMessageParam;\n  export import ChatCompletionUserMessageParam = API.ChatCompletionUserMessageParam;\n  export import ChatCompletionCreateParams = API.ChatCompletionCreateParams;\n  export import ChatCompletionCreateParamsNonStreaming = API.ChatCompletionCreateParamsNonStreaming;\n  export import ChatCompletionCreateParamsStreaming = API.ChatCompletionCreateParamsStreaming;\n\n  export import Embeddings = API.Embeddings;\n  export import CreateEmbeddingResponse = API.CreateEmbeddingResponse;\n  export import Embedding = API.Embedding;\n  export import EmbeddingCreateParams = API.EmbeddingCreateParams;\n\n  export import Files = API.Files;\n  export import FileContent = API.FileContent;\n  export import FileDeleted = API.FileDeleted;\n  export import FileObject = API.FileObject;\n  export import FileObjectsPage = API.FileObjectsPage;\n  export import FileCreateParams = API.FileCreateParams;\n  export import FileListParams = API.FileListParams;\n\n  export import Images = API.Images;\n  export import Image = API.Image;\n  export import ImagesResponse = API.ImagesResponse;\n  export import ImageCreateVariationParams = API.ImageCreateVariationParams;\n  export import ImageEditParams = API.ImageEditParams;\n  export import ImageGenerateParams = API.ImageGenerateParams;\n\n  export import Audio = API.Audio;\n\n  export import Moderations = API.Moderations;\n  export import Moderation = API.Moderation;\n  export import ModerationCreateResponse = API.ModerationCreateResponse;\n  export import ModerationCreateParams = API.ModerationCreateParams;\n\n  export import Models = API.Models;\n  export import Model = API.Model;\n  export import ModelDeleted = API.ModelDeleted;\n  export import ModelsPage = API.ModelsPage;\n\n  export import FineTuning = API.FineTuning;\n\n  export import Beta = API.Beta;\n\n  export import FunctionDefinition = API.FunctionDefinition;\n  export import FunctionParameters = API.FunctionParameters;\n}\n\nexport default OpenAI;\n"],"names":["VERSION","kind","fetch","FormData","File","ReadableStream","getMultipartRequestOptions","getDefaultAgent","fileFromPath","isFsReadStream","auto","Request","undefined","Response","Headers","Blob","MultipartBody","constructor","body","Symbol","toStringTag","shims","options","arguments","length","Error","concat","manuallyImported","recommendation","_fetch","_Request","_Response","_Headers","error","message","async","form","opts","url","value","OpenAIError","APIError","status","headers","super","makeMessage","this","data","code","param","type","msg","JSON","stringify","generate","errorResponse","APIConnectionError","cause","castToError","BadRequestError","AuthenticationError","PermissionDeniedError","NotFoundError","ConflictError","UnprocessableEntityError","RateLimitError","InternalServerError","APIUserAbortError","_ref","APIConnectionTimeoutError","Stream","iterator","controller","fromSSEResponse","response","consumed","decoder","SSEDecoder","done","sse","abort","lineDecoder","LineDecoder","iter","readableStreamAsyncIterable","chunk","line","decode","flush","iterMessages","startsWith","event","parse","e","console","raw","name","fromReadableStream","readableStream","iterLines","asyncIterator","tee","left","right","teeIterator","queue","next","result","push","shift","toReadableStream","self","encoder","TextEncoder","start","pull","ctrl","close","bytes","encode","enqueue","err","cancel","_iter$return","_iter","return","call","chunks","endsWith","substring","join","fieldname","_","str","delimiter","index","indexOf","partition","buffer","trailingCR","text","decodeText","slice","trailingNewline","NEWLINE_CHARS","has","lines","split","NEWLINE_REGEXP","pop","Buffer","toString","Uint8Array","from","TextDecoder","_this$textDecoder","ArrayBuffer","textDecoder","stream","reader","getReader","read","releaseLock","cancelPromise","Set","isResponseLike","blob","isBlobLike","size","arrayBuffer","isUploadable","lastModified","isFileLike","toFile","_getName","_URL$pathname$split$p","URL","pathname","bits","parts","isView","_value$constructor","isAsyncIterableIterator","props","Object","getOwnPropertyNames","map","p","propsForError","getBytes","_getStringFromMaybeBu","getStringFromMaybeBuffer","filename","path","getName","_bits$","x","String","isMultipartBody","multipartFormRequestOptions","createForm","Promise","all","entries","key","addFormValue","TypeError","append","file","Array","isArray","entry","_ref2","prop","defaultParseResponse","debug","__streamClass","__binaryResponse","contentType","get","includes","json","APIPromise","responsePromise","parseResponse","resolve","_thenUnwrap","transform","asResponse","then","withResponse","parsedPromise","onfulfilled","onrejected","catch","finally","onfinally","APIClient","baseURL","maxRetries","timeout","httpAgent","overridenFetch","validatePositiveInteger","authHeaders","defaultHeaders","Accept","getUserAgent","getPlatformHeaders","validateHeaders","customHeaders","defaultIdempotencyKey","uuid4","methodRequest","post","patch","put","delete","method","request","getAPIList","Page","requestAPIList","calculateContentLength","byteLength","buildRequest","_options$timeout","_options$httpAgent","_httpAgent$options","_httpAgent$options$ti","_options$signal","query","contentLength","buildURL","minAgentTimeout","idempotencyHeader","idempotencyKey","req","buildHeaders","agent","signal","_ref3","reqHeaders","applyHeadersMut","shimsKind","prepareOptions","prepareRequest","_ref4","parseHeaders","fromEntries","header","makeStatusError","remainingRetries","makeRequest","optionsInput","retriesRemaining","_options$signal2","_options$maxRetries","aborted","AbortController","fetchWithTimeout","_options$signal3","retryRequest","responseHeaders","createResponseHeaders","ok","shouldRetry","retryMessage","errText","errJSON","safeJSON","errMessage","PagePromise","isAbsoluteURL","defaultQuery","isEmptyObj","search","stringifyQuery","filter","_ref5","_ref6","encodeURIComponent","init","ms","addEventListener","setTimeout","getRequestClient","clearTimeout","shouldRetryHeader","timeoutMillis","retryAfterMillisHeader","timeoutMs","parseFloat","Number","isNaN","retryAfterHeader","timeoutSeconds","Date","now","_options$maxRetries2","calculateDefaultRetryTimeoutMillis","sleep","numRetries","Math","min","pow","random","AbstractPage","client","_AbstractPage_client","set","__classPrivateFieldSet","hasNextPage","getPaginatedItems","nextPageInfo","getNextPage","nextInfo","nextOptions","params","searchParams","__classPrivateFieldGet","iterPages","page","WeakMap","item","Proxy","target","toLowerCase","requestOptionsKeys","isRequestOptions","obj","keys","every","k","hasOwn","getPlatformProperties","Deno","build","normalizePlatform","os","normalizeArch","arch","version","EdgeRuntime","process","prototype","platform","browserInfo","navigator","browserPatterns","pattern","match","exec","userAgent","major","minor","browser","getBrowserInfo","_platformHeaders","_platformHeaders2","startsWithSchemeRegexp","RegExp","test","n","isInteger","readEnv","env","_process$env$env$trim","_process$env","_Deno$env","_Deno$env$get","trim","_k","hasOwnProperty","targetHeaders","newHeaders","lowerKey","val","action","_len","args","_key","log","replace","c","r","object","_this$data","nextPageParams","CursorPage","_this$data2","info","_data","id","after","APIResource","_client","Completions","create","_body$stream","Chat","completions","CompletionsAPI","Embeddings","Files","retrieve","fileId","list","FileObjectsPage","del","content","retrieveContent","waitForProcessing","pollInterval","maxWait","TERMINAL_STATES","FilesAPI","Images","createVariation","edit","Speech","Transcriptions","Translations","Audio","transcriptions","TranscriptionsAPI","translations","TranslationsAPI","speech","SpeechAPI","Moderations","Models","model","ModelsPage","ModelsAPI","Jobs","fineTuningJobId","FineTuningJobsPage","listEvents","FineTuningJobEventsPage","JobsAPI","FineTuning","jobs","assistantId","AssistantFilesPage","Assistants","files","update","AssistantsPage","isRunnableFunctionWithParse","fn","AssistantsAPI","isAssistantMessage","role","isFunctionMessage","isToolMessage","DEFAULT_MAX_CHAT_COMPLETIONS","AbstractChatCompletionRunner","_AbstractChatCompletionRunner_connectedPromise","_AbstractChatCompletionRunner_resolveConnectedPromise","_AbstractChatCompletionRunner_rejectConnectedPromise","_AbstractChatCompletionRunner_endPromise","_AbstractChatCompletionRunner_resolveEndPromise","_AbstractChatCompletionRunner_rejectEndPromise","_AbstractChatCompletionRunner_listeners","_chatCompletions","messages","_AbstractChatCompletionRunner_ended","_AbstractChatCompletionRunner_errored","_AbstractChatCompletionRunner_aborted","_AbstractChatCompletionRunner_catchingPromiseCreated","_AbstractChatCompletionRunner_handleError","_emit","openAIError","reject","_run","executor","_emitFinal","_addChatCompletion","chatCompletion","_chatCompletion$choic","choices","_addMessage","emit","function_call","tool_calls","tool_call","function","_connected","ended","errored","on","listener","off","listeners","findIndex","l","splice","once","emitted","finalChatCompletion","completion","finalContent","_AbstractChatCompletionRunner_instances","_AbstractChatCompletionRunner_getFinalContent","finalMessage","_AbstractChatCompletionRunner_getFinalMessage","finalFunctionCall","_AbstractChatCompletionRunner_getFinalFunctionCall","finalFunctionCallResult","_AbstractChatCompletionRunner_getFinalFunctionCallResult","totalUsage","_AbstractChatCompletionRunner_calculateTotalUsage","allChatCompletions","forEach","some","usage","_createChatCompletion","_AbstractChatCompletionRunner_validateParams","_runChatCompletion","_runFunctions","restParams","singleFunctionToCall","maxChatCompletions","functionsByName","f","functions","parameters","description","i","_chatCompletion$choic2","parsed","rawContent","_AbstractChatCompletionRunner_stringifyFunctionCallResult","_runTools","_tool_choice$function","tool_choice","tools","t","_chatCompletion$choic3","tool_call_id","_classPrivateFieldGe","_message$content","_message$tool_calls","_message$tool_calls$a","at","_x$tool_calls","y","total","completion_tokens","prompt_tokens","total_tokens","ChatCompletionRunner","runFunctions","runner","runTools","ChatCompletionStream","_ChatCompletionStream_currentChatCompletionSnapshot","currentChatCompletionSnapshot","_fromReadableStream","createChatCompletion","_stream$controller$si","_ChatCompletionStream_instances","_ChatCompletionStream_beginRequest","_ChatCompletionStream_addChunk","_ChatCompletionStream_endRequest","_stream$controller$si2","chatId","WeakSet","_chunk$choices$","_completion$choices$","_ChatCompletionStream_accumulateChatCompletion","delta","snapshot","created","system_fingerprint","rest","finish_reason","logprobs","choiceRest","messageRest","toolRest","fnRest","finalizeChatCompletion","assign","other","choice","_a$content","_a","_b$arguments","_b","_c$index","_tool_call$function","_c","pushQueue","readQueue","bind","ChatCompletionStreamingRunner","chat","threadId","messageId","MessageFilesPage","Messages","ThreadMessagesPage","MessagesAPI","Steps","runId","stepId","RunStepsPage","StepsAPI","Runs","steps","RunsPage","submitToolOutputs","RunsAPI","Threads","runs","createAndRun","Beta","ChatAPI","assistants","threads","ThreadsAPI","OpenAI","Core","_Core$readEnv","apiKey","organization","Errors","dangerouslyAllowBrowser","window","document","API","embeddings","images","audio","moderations","models","fineTuning","beta","_options","Authorization","Uploads","Pagination"],"sourceRoot":""}